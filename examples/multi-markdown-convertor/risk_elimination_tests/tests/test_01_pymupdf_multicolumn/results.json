{
  "test_id": "01",
  "test_name": "PyMuPDF Multi-column PDF Block Order",
  "pdf_path": "/Users/yaniv/Documents/code/ostruct/examples/multi-markdown-convertor/risk_elimination_tests/test-inputs/NIPS-2017-attention-is-all-you-need-Paper.pdf",
  "total_blocks": 197,
  "correctly_sequenced_blocks": 196,
  "sequence_accuracy_ratio": 0.9949238578680203,
  "blocks_data": [
    {
      "page": 1,
      "block_index": 0,
      "bbox": [
        211.48800659179688,
        96.52021026611328,
        399.89349365234375,
        118.91744995117188
      ],
      "text_preview": "Attention Is All You Need",
      "original_order": 0,
      "sorted_order": 0
    },
    {
      "page": 1,
      "block_index": 1,
      "bbox": [
        116.68099975585938,
        184.13389587402344,
        216.05787658691406,
        219.1035614013672
      ],
      "text_preview": "Ashish Vaswani\u2217 Google Brain avaswani@google.com",
      "original_order": 1,
      "sorted_order": 1
    },
    {
      "page": 1,
      "block_index": 2,
      "bbox": [
        230.69200134277344,
        184.13389587402344,
        309.1475830078125,
        219.1035614013672
      ],
      "text_preview": "Noam Shazeer\u2217 Google Brain noam@google.com",
      "original_order": 2,
      "sorted_order": 2
    },
    {
      "page": 1,
      "block_index": 3,
      "bbox": [
        323.7870178222656,
        184.13389587402344,
        407.4730529785156,
        219.1035614013672
      ],
      "text_preview": "Niki Parmar\u2217 Google Research nikip@google.com",
      "original_order": 3,
      "sorted_order": 3
    },
    {
      "page": 1,
      "block_index": 4,
      "bbox": [
        422.11199951171875,
        184.13389587402344,
        497.210693359375,
        219.1035614013672
      ],
      "text_preview": "Jakob Uszkoreit\u2217 Google Research usz@google.com",
      "original_order": 4,
      "sorted_order": 4
    },
    {
      "page": 1,
      "block_index": 5,
      "bbox": [
        126.88200378417969,
        234.13194274902344,
        210.5677947998047,
        269.1005859375
      ],
      "text_preview": "Llion Jones\u2217 Google Research llion@google.com",
      "original_order": 5,
      "sorted_order": 5
    },
    {
      "page": 1,
      "block_index": 6,
      "bbox": [
        235.406982421875,
        234.13194274902344,
        340.01446533203125,
        269.1005859375
      ],
      "text_preview": "Aidan N. Gomez\u2217 \u2020 University of Toronto aidan@cs.toronto.edu",
      "original_order": 6,
      "sorted_order": 6
    },
    {
      "page": 1,
      "block_index": 7,
      "bbox": [
        364.8489990234375,
        234.13194274902344,
        485.1377258300781,
        269.1005859375
      ],
      "text_preview": "\u0141ukasz Kaiser\u2217 Google Brain lukaszkaiser@google.com",
      "original_order": 7,
      "sorted_order": 7
    },
    {
      "page": 1,
      "block_index": 8,
      "bbox": [
        238.02200317382812,
        284.12896728515625,
        374.0018005371094,
        308.1896057128906
      ],
      "text_preview": "Illia Polosukhin\u2217 \u2021 illia.polosukhin@gmail.com",
      "original_order": 8,
      "sorted_order": 8
    },
    {
      "page": 1,
      "block_index": 9,
      "bbox": [
        283.75799560546875,
        334.30706787109375,
        328.2432861328125,
        349.86077880859375
      ],
      "text_preview": "Abstract",
      "original_order": 9,
      "sorted_order": 9
    },
    {
      "page": 1,
      "block_index": 10,
      "bbox": [
        143.86599731445312,
        358.56256103515625,
        469.78717041015625,
        501.47650146484375
      ],
      "text_preview": "The dominant sequence transduction models are based on complex recurrent or convolutional neural net",
      "original_order": 10,
      "sorted_order": 10
    },
    {
      "page": 1,
      "block_index": 11,
      "bbox": [
        108.0,
        518.3610229492188,
        190.8136749267578,
        533.9147338867188
      ],
      "text_preview": "1 Introduction",
      "original_order": 11,
      "sorted_order": 11
    },
    {
      "page": 1,
      "block_index": 12,
      "bbox": [
        108.0,
        543.1595458984375,
        504.1676330566406,
        598.8004760742188
      ],
      "text_preview": "Recurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks in pa",
      "original_order": 12,
      "sorted_order": 12
    },
    {
      "page": 1,
      "block_index": 13,
      "bbox": [
        108.0,
        603.3826293945312,
        504.31378173828125,
        716.008544921875
      ],
      "text_preview": "\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and",
      "original_order": 13,
      "sorted_order": 13
    },
    {
      "page": 1,
      "block_index": 14,
      "bbox": [
        108.0,
        731.592041015625,
        460.1375732421875,
        742.3965454101562
      ],
      "text_preview": "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.",
      "original_order": 14,
      "sorted_order": 14
    },
    {
      "page": 2,
      "block_index": 0,
      "bbox": [
        108.0,
        72.757568359375,
        504.35089111328125,
        161.12547302246094
      ],
      "text_preview": "Recurrent models typically factor computation along the symbol positions of the input and output seq",
      "original_order": 0,
      "sorted_order": 0
    },
    {
      "page": 2,
      "block_index": 1,
      "bbox": [
        107.64099884033203,
        165.50958251953125,
        505.65545654296875,
        210.2415008544922
      ],
      "text_preview": "Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion",
      "original_order": 1,
      "sorted_order": 1
    },
    {
      "page": 2,
      "block_index": 2,
      "bbox": [
        107.69100189208984,
        214.62554931640625,
        505.73870849609375,
        259.35748291015625
      ],
      "text_preview": "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relyi",
      "original_order": 2,
      "sorted_order": 2
    },
    {
      "page": 2,
      "block_index": 3,
      "bbox": [
        108.0,
        273.3809814453125,
        188.82911682128906,
        288.9346923828125
      ],
      "text_preview": "2 Background",
      "original_order": 3,
      "sorted_order": 3
    },
    {
      "page": 2,
      "block_index": 4,
      "bbox": [
        107.69100189208984,
        298.67156982421875,
        505.2414245605469,
        397.9494934082031
      ],
      "text_preview": "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20",
      "original_order": 4,
      "sorted_order": 4
    },
    {
      "page": 2,
      "block_index": 5,
      "bbox": [
        108.0,
        402.33355712890625,
        505.2498779296875,
        447.06549072265625
      ],
      "text_preview": "Self-attention, sometimes called intra-attention is an attention mechanism relating different positi",
      "original_order": 5,
      "sorted_order": 5
    },
    {
      "page": 2,
      "block_index": 6,
      "bbox": [
        108.0,
        451.4495544433594,
        505.6537780761719,
        485.2724914550781
      ],
      "text_preview": "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned",
      "original_order": 6,
      "sorted_order": 6
    },
    {
      "page": 2,
      "block_index": 7,
      "bbox": [
        107.69100189208984,
        489.6555480957031,
        505.656982421875,
        534.3875122070312
      ],
      "text_preview": "To the best of our knowledge, however, the Transformer is the \ufb01rst transduction model relying entire",
      "original_order": 7,
      "sorted_order": 7
    },
    {
      "page": 2,
      "block_index": 8,
      "bbox": [
        108.0,
        548.4110107421875,
        226.09344482421875,
        563.9647216796875
      ],
      "text_preview": "3 Model Architecture",
      "original_order": 8,
      "sorted_order": 8
    },
    {
      "page": 2,
      "block_index": 9,
      "bbox": [
        108.0,
        573.7025756835938,
        505.7431640625,
        629.343505859375
      ],
      "text_preview": "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29]. H",
      "original_order": 9,
      "sorted_order": 9
    },
    {
      "page": 2,
      "block_index": 10,
      "bbox": [
        107.69100189208984,
        633.7275390625,
        505.2474670410156,
        667.5504760742188
      ],
      "text_preview": "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully",
      "original_order": 10,
      "sorted_order": 10
    },
    {
      "page": 2,
      "block_index": 11,
      "bbox": [
        108.0,
        679.5518798828125,
        253.005615234375,
        692.51318359375
      ],
      "text_preview": "3.1 Encoder and Decoder Stacks",
      "original_order": 11,
      "sorted_order": 11
    },
    {
      "page": 2,
      "block_index": 12,
      "bbox": [
        108.0,
        699.5269165039062,
        505.6571960449219,
        722.7994995117188
      ],
      "text_preview": "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers",
      "original_order": 12,
      "sorted_order": 12
    },
    {
      "page": 2,
      "block_index": 13,
      "bbox": [
        303.5090026855469,
        740.6825561523438,
        308.49029541015625,
        752.6875
      ],
      "text_preview": "2",
      "original_order": 13,
      "sorted_order": 13
    },
    {
      "page": 3,
      "block_index": 0,
      "bbox": [
        196.5590057373047,
        72.00198364257812,
        415.43902587890625,
        394.4179992675781
      ],
      "text_preview": "<image: DeviceRGB, width: 1520, height: 2239, bpc: 8>",
      "original_order": 0,
      "sorted_order": 0
    },
    {
      "page": 3,
      "block_index": 1,
      "bbox": [
        210.01100158691406,
        403.0955505371094,
        401.99029541015625,
        415.1004943847656
      ],
      "text_preview": "Figure 1: The Transformer - model architecture.",
      "original_order": 1,
      "sorted_order": 1
    },
    {
      "page": 3,
      "block_index": 2,
      "bbox": [
        107.64099884033203,
        437.7255554199219,
        504.16607666015625,
        494.5526428222656
      ],
      "text_preview": "wise fully connected feed-forward network. We employ a residual connection [10] around each of the t",
      "original_order": 2,
      "sorted_order": 2
    },
    {
      "page": 3,
      "block_index": 3,
      "bbox": [
        108.0,
        507.02490234375,
        504.0016174316406,
        584.843505859375
      ],
      "text_preview": "Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two s",
      "original_order": 3,
      "sorted_order": 3
    },
    {
      "page": 3,
      "block_index": 4,
      "bbox": [
        108.0,
        599.845947265625,
        170.81419372558594,
        612.8072509765625
      ],
      "text_preview": "3.2 Attention",
      "original_order": 4,
      "sorted_order": 4
    },
    {
      "page": 3,
      "block_index": 5,
      "bbox": [
        107.64099884033203,
        621.3435668945312,
        505.2488708496094,
        666.0755004882812
      ],
      "text_preview": "An attention function can be described as mapping a query and a set of key-value pairs to an output,",
      "original_order": 5,
      "sorted_order": 5
    },
    {
      "page": 3,
      "block_index": 6,
      "bbox": [
        108.0,
        679.73388671875,
        263.8847961425781,
        692.6951904296875
      ],
      "text_preview": "3.2.1 Scaled Dot-Product Attention",
      "original_order": 6,
      "sorted_order": 6
    },
    {
      "page": 3,
      "block_index": 7,
      "bbox": [
        107.53199768066406,
        699.8855590820312,
        504.0005187988281,
        723.2374877929688
      ],
      "text_preview": "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of qu",
      "original_order": 7,
      "sorted_order": 7
    },
    {
      "page": 3,
      "block_index": 8,
      "bbox": [
        303.5090026855469,
        740.6825561523438,
        308.49029541015625,
        752.6875
      ],
      "text_preview": "3",
      "original_order": 8,
      "sorted_order": 8
    },
    {
      "page": 4,
      "block_index": 0,
      "bbox": [
        147.7830047607422,
        69.549560546875,
        266.2183837890625,
        81.55449676513672
      ],
      "text_preview": "Scaled Dot-Product Attention",
      "original_order": 0,
      "sorted_order": 0
    },
    {
      "page": 4,
      "block_index": 1,
      "bbox": [
        363.58599853515625,
        69.549560546875,
        450.20074462890625,
        81.55449676513672
      ],
      "text_preview": "Multi-Head Attention",
      "original_order": 2,
      "sorted_order": 1
    },
    {
      "page": 4,
      "block_index": 2,
      "bbox": [
        346.7720031738281,
        82.68698120117188,
        467.0119934082031,
        267.29498291015625
      ],
      "text_preview": "<image: DeviceRGB, width: 835, height: 1282, bpc: 8>",
      "original_order": 3,
      "sorted_order": 2
    },
    {
      "page": 4,
      "block_index": 3,
      "bbox": [
        174.9600067138672,
        94.02497863769531,
        239.04000854492188,
        221.32098388671875
      ],
      "text_preview": "<image: DeviceRGB, width: 445, height: 884, bpc: 8>",
      "original_order": 1,
      "sorted_order": 3
    },
    {
      "page": 4,
      "block_index": 4,
      "bbox": [
        108.0,
        272.8135681152344,
        503.9972229003906,
        295.7275085449219
      ],
      "text_preview": "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several atte",
      "original_order": 4,
      "sorted_order": 4
    },
    {
      "page": 4,
      "block_index": 5,
      "bbox": [
        107.7509994506836,
        312.77099609375,
        503.9972839355469,
        341.9435119628906
      ],
      "text_preview": "query with all keys, divide each by \u221adk, and apply a softmax function to obtain the weights on the v",
      "original_order": 5,
      "sorted_order": 5
    },
    {
      "page": 4,
      "block_index": 6,
      "bbox": [
        108.0,
        346.3275451660156,
        504.170166015625,
        380.1505126953125
      ],
      "text_preview": "In practice, we compute the attention function on a set of queries simultaneously, packed together i",
      "original_order": 6,
      "sorted_order": 6
    },
    {
      "page": 4,
      "block_index": 7,
      "bbox": [
        219.97000122070312,
        399.1636657714844,
        377.35931396484375,
        417.23968505859375
      ],
      "text_preview": "Attention(Q, K, V ) = softmax(QKT",
      "original_order": 7,
      "sorted_order": 7
    },
    {
      "page": 4,
      "block_index": 8,
      "bbox": [
        358.0769958496094,
        405.5435485839844,
        504.0003662109375,
        425.4314270019531
      ],
      "text_preview": "\u221adk )V (1)",
      "original_order": 8,
      "sorted_order": 8
    },
    {
      "page": 4,
      "block_index": 9,
      "bbox": [
        107.69100189208984,
        436.91754150390625,
        505.6527099609375,
        505.7294921875
      ],
      "text_preview": "The two most commonly used attention functions are additive attention [2], and dot-product (multi- p",
      "original_order": 9,
      "sorted_order": 9
    },
    {
      "page": 4,
      "block_index": 10,
      "bbox": [
        107.53199768066406,
        510.112548828125,
        504.0,
        560.1085205078125
      ],
      "text_preview": "While for small values of dk the two mechanisms perform similarly, additive attention outperforms do",
      "original_order": 10,
      "sorted_order": 10
    },
    {
      "page": 4,
      "block_index": 11,
      "bbox": [
        108.0,
        571.6359252929688,
        230.58978271484375,
        584.5972290039062
      ],
      "text_preview": "3.2.2 Multi-Head Attention",
      "original_order": 11,
      "sorted_order": 11
    },
    {
      "page": 4,
      "block_index": 12,
      "bbox": [
        107.64099884033203,
        592.092529296875,
        505.2415771484375,
        658.6425170898438
      ],
      "text_preview": "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,",
      "original_order": 12,
      "sorted_order": 12
    },
    {
      "page": 4,
      "block_index": 13,
      "bbox": [
        108.0,
        663.0265502929688,
        503.996826171875,
        685.9404907226562
      ],
      "text_preview": "Multi-head attention allows the model to jointly attend to information from different representation",
      "original_order": 13,
      "sorted_order": 13
    },
    {
      "page": 4,
      "block_index": 14,
      "bbox": [
        107.7760009765625,
        699.3176879882812,
        503.9986572265625,
        746.5225830078125
      ],
      "text_preview": "4To illustrate why the dot products get large, assume that the components of q and k are independent",
      "original_order": 14,
      "sorted_order": 14
    },
    {
      "page": 4,
      "block_index": 15,
      "bbox": [
        303.5090026855469,
        740.6825561523438,
        308.49029541015625,
        752.6875
      ],
      "text_preview": "4",
      "original_order": 15,
      "sorted_order": 15
    },
    {
      "page": 5,
      "block_index": 0,
      "bbox": [
        186.94000244140625,
        90.46661376953125,
        410.915283203125,
        103.0484390258789
      ],
      "text_preview": "MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O",
      "original_order": 0,
      "sorted_order": 0
    },
    {
      "page": 5,
      "block_index": 1,
      "bbox": [
        224.4970245361328,
        106.6036376953125,
        425.051513671875,
        121.88865661621094
      ],
      "text_preview": "where headi = Attention(QW Q i , KW K i , V W V i )",
      "original_order": 1,
      "sorted_order": 1
    },
    {
      "page": 5,
      "block_index": 2,
      "bbox": [
        107.53199768066406,
        146.74163818359375,
        502.823486328125,
        178.5550079345703
      ],
      "text_preview": "Where the projections are parameter matrices W Q i \u2208 Rdmodel\u00d7dk, W K i \u2208 Rdmodel\u00d7dk, W V i \u2208 Rdmodel",
      "original_order": 2,
      "sorted_order": 2
    },
    {
      "page": 5,
      "block_index": 3,
      "bbox": [
        108.0,
        176.174560546875,
        504.0021057128906,
        209.9974822998047
      ],
      "text_preview": "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv =",
      "original_order": 3,
      "sorted_order": 3
    },
    {
      "page": 5,
      "block_index": 4,
      "bbox": [
        108.0,
        220.7408905029297,
        302.8584899902344,
        233.70223999023438
      ],
      "text_preview": "3.2.3 Applications of Attention in our Model",
      "original_order": 4,
      "sorted_order": 4
    },
    {
      "page": 5,
      "block_index": 5,
      "bbox": [
        107.69100189208984,
        239.72955322265625,
        372.6064453125,
        251.73448181152344
      ],
      "text_preview": "The Transformer uses multi-head attention in three different ways:",
      "original_order": 5,
      "sorted_order": 5
    },
    {
      "page": 5,
      "block_index": 6,
      "bbox": [
        133.9029998779297,
        260.08953857421875,
        505.2421569824219,
        315.7304992675781
      ],
      "text_preview": "\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the m",
      "original_order": 6,
      "sorted_order": 6
    },
    {
      "page": 5,
      "block_index": 7,
      "bbox": [
        133.9029998779297,
        318.612548828125,
        504.00323486328125,
        363.3445129394531
      ],
      "text_preview": "\u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values and",
      "original_order": 7,
      "sorted_order": 7
    },
    {
      "page": 5,
      "block_index": 8,
      "bbox": [
        133.9029998779297,
        366.2265625,
        504.00311279296875,
        421.8684997558594
      ],
      "text_preview": "\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to al",
      "original_order": 8,
      "sorted_order": 8
    },
    {
      "page": 5,
      "block_index": 9,
      "bbox": [
        108.0,
        433.9569091796875,
        292.8659973144531,
        446.9182434082031
      ],
      "text_preview": "3.3 Position-wise Feed-Forward Networks",
      "original_order": 9,
      "sorted_order": 9
    },
    {
      "page": 5,
      "block_index": 10,
      "bbox": [
        108.0,
        454.2905578613281,
        504.3429260253906,
        488.1134948730469
      ],
      "text_preview": "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully",
      "original_order": 10,
      "sorted_order": 10
    },
    {
      "page": 5,
      "block_index": 11,
      "bbox": [
        226.9010009765625,
        503.40655517578125,
        504.0003662109375,
        516.5966186523438
      ],
      "text_preview": "FFN(x) = max(0, xW1 + b1)W2 + b2 (2)",
      "original_order": 11,
      "sorted_order": 11
    },
    {
      "page": 5,
      "block_index": 12,
      "bbox": [
        107.53199768066406,
        523.758544921875,
        505.74517822265625,
        569.6766357421875
      ],
      "text_preview": "While the linear transformations are the same across different positions, they use different paramet",
      "original_order": 12,
      "sorted_order": 12
    },
    {
      "page": 5,
      "block_index": 13,
      "bbox": [
        107.99999237060547,
        580.5789184570312,
        240.0242919921875,
        593.5402221679688
      ],
      "text_preview": "3.4 Embeddings and Softmax",
      "original_order": 13,
      "sorted_order": 13
    },
    {
      "page": 5,
      "block_index": 14,
      "bbox": [
        108.0,
        600.9125366210938,
        505.74383544921875,
        663.319091796875
      ],
      "text_preview": "Similarly to other sequence transduction models, we use learned embeddings to convert the input toke",
      "original_order": 14,
      "sorted_order": 14
    },
    {
      "page": 5,
      "block_index": 15,
      "bbox": [
        108.0,
        668.6429443359375,
        215.19754028320312,
        681.604248046875
      ],
      "text_preview": "3.5 Positional Encoding",
      "original_order": 15,
      "sorted_order": 15
    },
    {
      "page": 5,
      "block_index": 16,
      "bbox": [
        108.0,
        688.9765625,
        504.0016784667969,
        722.7994995117188
      ],
      "text_preview": "Since our model contains no recurrence and no convolution, in order for the model to make use of the",
      "original_order": 16,
      "sorted_order": 16
    },
    {
      "page": 5,
      "block_index": 17,
      "bbox": [
        303.5090026855469,
        740.6825561523438,
        308.49029541015625,
        752.6875
      ],
      "text_preview": "5",
      "original_order": 17,
      "sorted_order": 17
    },
    {
      "page": 6,
      "block_index": 0,
      "bbox": [
        107.69100189208984,
        69.549560546875,
        504.0035705566406,
        103.37248992919922
      ],
      "text_preview": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for",
      "original_order": 0,
      "sorted_order": 0
    },
    {
      "page": 6,
      "block_index": 1,
      "bbox": [
        124.5469970703125,
        114.861572265625,
        487.4545593261719,
        137.77549743652344
      ],
      "text_preview": "Layer Type Complexity per Layer Sequential Maximum Path Length Operations",
      "original_order": 1,
      "sorted_order": 1
    },
    {
      "page": 6,
      "block_index": 2,
      "bbox": [
        124.5469970703125,
        136.2769775390625,
        464.8059997558594,
        190.8511505126953
      ],
      "text_preview": "Self-Attention O(n2 \u00b7 d) O(1) O(1) Recurrent O(n \u00b7 d2) O(n) O(n) Convolutional O(k \u00b7 n \u00b7 d2) O(1) O(",
      "original_order": 2,
      "sorted_order": 2
    },
    {
      "page": 6,
      "block_index": 3,
      "bbox": [
        108.0,
        214.08355712890625,
        505.2479248046875,
        247.90647888183594
      ],
      "text_preview": "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel a",
      "original_order": 3,
      "sorted_order": 3
    },
    {
      "page": 6,
      "block_index": 4,
      "bbox": [
        108.0,
        252.29052734375,
        389.86175537109375,
        264.29547119140625
      ],
      "text_preview": "In this work, we use sine and cosine functions of different frequencies:",
      "original_order": 4,
      "sorted_order": 4
    },
    {
      "page": 6,
      "block_index": 5,
      "bbox": [
        235.50900268554688,
        287.7705993652344,
        386.2974548339844,
        301.40362548828125
      ],
      "text_preview": "PE(pos,2i) = sin(pos/100002i/dmodel)",
      "original_order": 5,
      "sorted_order": 5
    },
    {
      "page": 6,
      "block_index": 6,
      "bbox": [
        225.6939697265625,
        304.6416015625,
        386.2974853515625,
        318.275634765625
      ],
      "text_preview": "PE(pos,2i+1) = cos(pos/100002i/dmodel)",
      "original_order": 6,
      "sorted_order": 6
    },
    {
      "page": 6,
      "block_index": 7,
      "bbox": [
        107.64099884033203,
        329.4905700683594,
        504.34283447265625,
        385.5694274902344
      ],
      "text_preview": "where pos is the position and i is the dimension. That is, each dimension of the positional encoding",
      "original_order": 7,
      "sorted_order": 7
    },
    {
      "page": 6,
      "block_index": 8,
      "bbox": [
        107.53199768066406,
        389.51556396484375,
        504.00238037109375,
        434.24749755859375
      ],
      "text_preview": "We also experimented with using learned positional embeddings [8] instead, and found that the two ve",
      "original_order": 8,
      "sorted_order": 8
    },
    {
      "page": 6,
      "block_index": 9,
      "bbox": [
        108.0,
        449.8800048828125,
        225.04141235351562,
        465.4337158203125
      ],
      "text_preview": "4 Why Self-Attention",
      "original_order": 9,
      "sorted_order": 9
    },
    {
      "page": 6,
      "block_index": 10,
      "bbox": [
        106.83399963378906,
        476.174560546875,
        505.6539001464844,
        531.8154907226562
      ],
      "text_preview": "In this section we compare various aspects of self-attention layers to the recurrent and convolu- ti",
      "original_order": 10,
      "sorted_order": 10
    },
    {
      "page": 6,
      "block_index": 11,
      "bbox": [
        108.0,
        536.1995849609375,
        504.0002136230469,
        559.1134643554688
      ],
      "text_preview": "One is the total computational complexity per layer. Another is the amount of computation that can b",
      "original_order": 11,
      "sorted_order": 11
    },
    {
      "page": 6,
      "block_index": 12,
      "bbox": [
        107.69100189208984,
        563.49658203125,
        504.0014343261719,
        640.9564819335938
      ],
      "text_preview": "The third is the path length between long-range dependencies in the network. Learning long-range dep",
      "original_order": 12,
      "sorted_order": 12
    },
    {
      "page": 6,
      "block_index": 13,
      "bbox": [
        107.64099884033203,
        645.3395385742188,
        504.3455810546875,
        722.7994995117188
      ],
      "text_preview": "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequent",
      "original_order": 13,
      "sorted_order": 13
    },
    {
      "page": 6,
      "block_index": 14,
      "bbox": [
        303.5090026855469,
        740.6825561523438,
        308.49029541015625,
        752.6875
      ],
      "text_preview": "6",
      "original_order": 14,
      "sorted_order": 14
    },
    {
      "page": 7,
      "block_index": 0,
      "bbox": [
        108.0,
        72.757568359375,
        504.0014343261719,
        95.67150115966797
      ],
      "text_preview": "the input sequence centered around the respective output position. This would increase the maximum p",
      "original_order": 0,
      "sorted_order": 0
    },
    {
      "page": 7,
      "block_index": 1,
      "bbox": [
        107.64099884033203,
        100.0545654296875,
        505.2477111816406,
        188.4235076904297
      ],
      "text_preview": "A single convolutional layer with kernel width k < n does not connect all pairs of input and output",
      "original_order": 1,
      "sorted_order": 1
    },
    {
      "page": 7,
      "block_index": 2,
      "bbox": [
        107.64099884033203,
        192.80657958984375,
        504.0037536621094,
        237.5394744873047
      ],
      "text_preview": "As side bene\ufb01t, self-attention could yield more interpretable models. We inspect attention distribut",
      "original_order": 2,
      "sorted_order": 2
    },
    {
      "page": 7,
      "block_index": 3,
      "bbox": [
        108.0,
        252.6400146484375,
        170.22682189941406,
        268.1937255859375
      ],
      "text_preview": "5 Training",
      "original_order": 3,
      "sorted_order": 3
    },
    {
      "page": 7,
      "block_index": 4,
      "bbox": [
        107.69100189208984,
        278.6155700683594,
        337.4783935546875,
        290.6205139160156
      ],
      "text_preview": "This section describes the training regime for our models.",
      "original_order": 4,
      "sorted_order": 4
    },
    {
      "page": 7,
      "block_index": 5,
      "bbox": [
        108.0,
        303.69891357421875,
        249.52862548828125,
        316.6602478027344
      ],
      "text_preview": "5.1 Training Data and Batching",
      "original_order": 5,
      "sorted_order": 5
    },
    {
      "page": 7,
      "block_index": 6,
      "bbox": [
        107.53199768066406,
        324.4255676269531,
        505.654541015625,
        401.885498046875
      ],
      "text_preview": "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence",
      "original_order": 6,
      "sorted_order": 6
    },
    {
      "page": 7,
      "block_index": 7,
      "bbox": [
        108.0,
        414.9638977050781,
        233.04055786132812,
        427.92523193359375
      ],
      "text_preview": "5.2 Hardware and Schedule",
      "original_order": 7,
      "sorted_order": 7
    },
    {
      "page": 7,
      "block_index": 8,
      "bbox": [
        107.53199768066406,
        435.6905517578125,
        504.0014953613281,
        491.3324890136719
      ],
      "text_preview": "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperpar",
      "original_order": 8,
      "sorted_order": 8
    },
    {
      "page": 7,
      "block_index": 9,
      "bbox": [
        108.0,
        504.410888671875,
        174.1317596435547,
        517.3721923828125
      ],
      "text_preview": "5.3 Optimizer",
      "original_order": 9,
      "sorted_order": 9
    },
    {
      "page": 7,
      "block_index": 10,
      "bbox": [
        107.53199768066406,
        525.1375732421875,
        504.0029296875,
        548.051513671875
      ],
      "text_preview": "We used the Adam optimizer [17] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. We varied the learning rate o",
      "original_order": 10,
      "sorted_order": 10
    },
    {
      "page": 7,
      "block_index": 11,
      "bbox": [
        162.89199829101562,
        564.146484375,
        504.0003356933594,
        586.3161010742188
      ],
      "text_preview": "lrate = d\u22120.5 model \u00b7 min(step_num\u22120.5, step_num \u00b7 warmup_steps\u22121.5) (3)",
      "original_order": 11,
      "sorted_order": 11
    },
    {
      "page": 7,
      "block_index": 12,
      "bbox": [
        107.69100189208984,
        586.341552734375,
        505.243896484375,
        620.1644897460938
      ],
      "text_preview": "This corresponds to increasing the learning rate linearly for the \ufb01rst warmup_steps training steps,",
      "original_order": 12,
      "sorted_order": 12
    },
    {
      "page": 7,
      "block_index": 13,
      "bbox": [
        107.99999237060547,
        633.242919921875,
        193.50897216796875,
        646.2042236328125
      ],
      "text_preview": "5.4 Regularization",
      "original_order": 13,
      "sorted_order": 13
    },
    {
      "page": 7,
      "block_index": 14,
      "bbox": [
        107.53199005126953,
        653.9705810546875,
        331.9892578125,
        665.9755249023438
      ],
      "text_preview": "We employ three types of regularization during training:",
      "original_order": 14,
      "sorted_order": 14
    },
    {
      "page": 7,
      "block_index": 15,
      "bbox": [
        107.99999237060547,
        677.7089233398438,
        504.00250244140625,
        723.9846801757812
      ],
      "text_preview": "Residual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the su",
      "original_order": 15,
      "sorted_order": 15
    },
    {
      "page": 7,
      "block_index": 16,
      "bbox": [
        303.5090026855469,
        740.6825561523438,
        308.49029541015625,
        752.6875
      ],
      "text_preview": "7",
      "original_order": 16,
      "sorted_order": 16
    },
    {
      "page": 8,
      "block_index": 0,
      "bbox": [
        107.69100189208984,
        69.549560546875,
        504.00311279296875,
        92.46349334716797
      ],
      "text_preview": "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the En",
      "original_order": 0,
      "sorted_order": 0
    },
    {
      "page": 8,
      "block_index": 1,
      "bbox": [
        136.67100524902344,
        96.13653564453125,
        475.3241882324219,
        116.43047332763672
      ],
      "text_preview": "Model BLEU Training Cost (FLOPs)",
      "original_order": 1,
      "sorted_order": 1
    },
    {
      "page": 8,
      "block_index": 2,
      "bbox": [
        288.7200012207031,
        111.8475341796875,
        468.8238525390625,
        123.85247039794922
      ],
      "text_preview": "EN-DE EN-FR EN-DE EN-FR",
      "original_order": 2,
      "sorted_order": 2
    },
    {
      "page": 8,
      "block_index": 3,
      "bbox": [
        136.67100524902344,
        123.154541015625,
        473.0951232910156,
        153.3070831298828
      ],
      "text_preview": "ByteNet [15] 23.75 Deep-Att + PosUnk [32] 39.2 1.0 \u00b7 1020",
      "original_order": 3,
      "sorted_order": 3
    },
    {
      "page": 8,
      "block_index": 4,
      "bbox": [
        136.67098999023438,
        145.91949462890625,
        473.09515380859375,
        164.6890411376953
      ],
      "text_preview": "GNMT + RL [31] 24.6 39.92 2.3 \u00b7 1019 1.4 \u00b7 1020",
      "original_order": 4,
      "sorted_order": 4
    },
    {
      "page": 8,
      "block_index": 5,
      "bbox": [
        136.6710205078125,
        157.301513671875,
        473.0951843261719,
        176.07106018066406
      ],
      "text_preview": "ConvS2S [8] 25.16 40.46 9.6 \u00b7 1018 1.5 \u00b7 1020",
      "original_order": 5,
      "sorted_order": 5
    },
    {
      "page": 8,
      "block_index": 6,
      "bbox": [
        136.67105102539062,
        168.68450927734375,
        473.09521484375,
        187.4540557861328
      ],
      "text_preview": "MoE [26] 26.03 40.56 2.0 \u00b7 1019 1.2 \u00b7 1020",
      "original_order": 6,
      "sorted_order": 6
    },
    {
      "page": 8,
      "block_index": 7,
      "bbox": [
        136.67100524902344,
        181.321533203125,
        473.0951232910156,
        200.09107971191406
      ],
      "text_preview": "Deep-Att + PosUnk Ensemble [32] 40.4 8.0 \u00b7 1020",
      "original_order": 7,
      "sorted_order": 7
    },
    {
      "page": 8,
      "block_index": 8,
      "bbox": [
        136.67098999023438,
        192.70452880859375,
        473.09515380859375,
        211.4740753173828
      ],
      "text_preview": "GNMT + RL Ensemble [31] 26.30 41.16 1.8 \u00b7 1020 1.1 \u00b7 1021",
      "original_order": 8,
      "sorted_order": 8
    },
    {
      "page": 8,
      "block_index": 9,
      "bbox": [
        136.6710205078125,
        203.7278289794922,
        473.0951843261719,
        222.8560333251953
      ],
      "text_preview": "ConvS2S Ensemble [8] 26.36 41.29 7.7 \u00b7 1019 1.2 \u00b7 1021",
      "original_order": 9,
      "sorted_order": 9
    },
    {
      "page": 8,
      "block_index": 10,
      "bbox": [
        136.67100524902344,
        217.2225341796875,
        450.7341003417969,
        235.79281616210938
      ],
      "text_preview": "Transformer (base model) 27.3 38.1 3.3 \u00b7 1018",
      "original_order": 10,
      "sorted_order": 10
    },
    {
      "page": 8,
      "block_index": 11,
      "bbox": [
        136.67098999023438,
        228.24583435058594,
        447.9541320800781,
        247.37403869628906
      ],
      "text_preview": "Transformer (big) 28.4 41.0 2.3 \u00b7 1019",
      "original_order": 11,
      "sorted_order": 11
    },
    {
      "page": 8,
      "block_index": 12,
      "bbox": [
        108.0,
        270.65692138671875,
        504.0008544921875,
        293.92950439453125
      ],
      "text_preview": "Label Smoothing During training, we employed label smoothing of value \u03f5ls = 0.1 [30]. This hurts per",
      "original_order": 12,
      "sorted_order": 12
    },
    {
      "page": 8,
      "block_index": 13,
      "bbox": [
        108.0,
        310.02301025390625,
        163.12542724609375,
        325.57672119140625
      ],
      "text_preview": "6 Results",
      "original_order": 13,
      "sorted_order": 13
    },
    {
      "page": 8,
      "block_index": 14,
      "bbox": [
        108.0,
        336.23492431640625,
        219.07301330566406,
        349.1962585449219
      ],
      "text_preview": "6.1 Machine Translation",
      "original_order": 14,
      "sorted_order": 14
    },
    {
      "page": 8,
      "block_index": 15,
      "bbox": [
        108.0,
        357.35955810546875,
        504.6652526855469,
        423.90948486328125
      ],
      "text_preview": "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in",
      "original_order": 15,
      "sorted_order": 15
    },
    {
      "page": 8,
      "block_index": 16,
      "bbox": [
        108.0,
        428.2935485839844,
        505.2453308105469,
        474.211669921875
      ],
      "text_preview": "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, out",
      "original_order": 16,
      "sorted_order": 16
    },
    {
      "page": 8,
      "block_index": 17,
      "bbox": [
        107.64099884033203,
        477.4095458984375,
        504.0042724609375,
        533.0504760742188
      ],
      "text_preview": "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were",
      "original_order": 17,
      "sorted_order": 17
    },
    {
      "page": 8,
      "block_index": 18,
      "bbox": [
        107.69100189208984,
        537.4345703125,
        503.99761962890625,
        582.16650390625
      ],
      "text_preview": "Table 2 summarizes our results and compares our translation quality and training costs to other mode",
      "original_order": 18,
      "sorted_order": 18
    },
    {
      "page": 8,
      "block_index": 19,
      "bbox": [
        108.0,
        596.2379150390625,
        203.9398193359375,
        609.19921875
      ],
      "text_preview": "6.2 Model Variations",
      "original_order": 19,
      "sorted_order": 19
    },
    {
      "page": 8,
      "block_index": 20,
      "bbox": [
        107.69100189208984,
        617.361572265625,
        504.0009460449219,
        662.094482421875
      ],
      "text_preview": "To evaluate the importance of different components of the Transformer, we varied our base model in d",
      "original_order": 20,
      "sorted_order": 20
    },
    {
      "page": 8,
      "block_index": 21,
      "bbox": [
        108.0,
        666.4775390625,
        505.24139404296875,
        700.3004760742188
      ],
      "text_preview": "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimension",
      "original_order": 21,
      "sorted_order": 21
    },
    {
      "page": 8,
      "block_index": 22,
      "bbox": [
        120.65299987792969,
        710.6676635742188,
        453.9782409667969,
        722.51953125
      ],
      "text_preview": "5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.",
      "original_order": 22,
      "sorted_order": 22
    },
    {
      "page": 8,
      "block_index": 23,
      "bbox": [
        303.5090026855469,
        740.6825561523438,
        308.49029541015625,
        752.6875
      ],
      "text_preview": "8",
      "original_order": 23,
      "sorted_order": 23
    },
    {
      "page": 9,
      "block_index": 0,
      "bbox": [
        107.69100189208984,
        69.549560546875,
        504.0033264160156,
        114.28148651123047
      ],
      "text_preview": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the b",
      "original_order": 0,
      "sorted_order": 0
    },
    {
      "page": 9,
      "block_index": 1,
      "bbox": [
        146.1269989013672,
        129.83258056640625,
        502.7594299316406,
        147.94664001464844
      ],
      "text_preview": "N dmodel dff h dk dv Pdrop \u03f5ls train PPL BLEU params",
      "original_order": 1,
      "sorted_order": 1
    },
    {
      "page": 9,
      "block_index": 2,
      "bbox": [
        370.3070068359375,
        141.21453857421875,
        498.9620666503906,
        159.9840850830078
      ],
      "text_preview": "steps (dev) (dev) \u00d7106",
      "original_order": 2,
      "sorted_order": 2
    },
    {
      "page": 9,
      "block_index": 3,
      "bbox": [
        116.46800231933594,
        153.8525390625,
        493.35015869140625,
        165.8574676513672
      ],
      "text_preview": "base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65",
      "original_order": 3,
      "sorted_order": 3
    },
    {
      "page": 9,
      "block_index": 4,
      "bbox": [
        236.61700439453125,
        166.48956298828125,
        457.74383544921875,
        178.49449157714844
      ],
      "text_preview": "1 512 512 5.29 24.9",
      "original_order": 5,
      "sorted_order": 4
    },
    {
      "page": 9,
      "block_index": 5,
      "bbox": [
        118.40599822998047,
        182.85357666015625,
        132.2340850830078,
        194.85850524902344
      ],
      "text_preview": "(A)",
      "original_order": 4,
      "sorted_order": 5
    },
    {
      "page": 9,
      "block_index": 6,
      "bbox": [
        236.61700439453125,
        177.3995361328125,
        457.74383544921875,
        189.4044647216797
      ],
      "text_preview": "4 128 128 5.00 25.5",
      "original_order": 6,
      "sorted_order": 6
    },
    {
      "page": 9,
      "block_index": 7,
      "bbox": [
        234.12600708007812,
        188.30853271484375,
        457.74383544921875,
        200.31346130371094
      ],
      "text_preview": "16 32 32 4.91 25.8",
      "original_order": 7,
      "sorted_order": 7
    },
    {
      "page": 9,
      "block_index": 8,
      "bbox": [
        234.12600708007812,
        199.217529296875,
        457.74383544921875,
        211.2224578857422
      ],
      "text_preview": "32 16 16 5.01 25.4",
      "original_order": 8,
      "sorted_order": 8
    },
    {
      "page": 9,
      "block_index": 9,
      "bbox": [
        118.68000030517578,
        211.85455322265625,
        493.35015869140625,
        229.3144989013672
      ],
      "text_preview": "(B) 16 5.16 25.1 58",
      "original_order": 9,
      "sorted_order": 9
    },
    {
      "page": 9,
      "block_index": 10,
      "bbox": [
        258.5350036621094,
        222.7635498046875,
        493.35015869140625,
        234.7684783935547
      ],
      "text_preview": "32 5.01 25.4 60",
      "original_order": 10,
      "sorted_order": 10
    },
    {
      "page": 9,
      "block_index": 11,
      "bbox": [
        148.1820068359375,
        235.40155029296875,
        493.35015869140625,
        247.40647888183594
      ],
      "text_preview": "2 6.11 23.7 36",
      "original_order": 12,
      "sorted_order": 11
    },
    {
      "page": 9,
      "block_index": 12,
      "bbox": [
        148.1820068359375,
        246.310546875,
        493.35015869140625,
        258.31549072265625
      ],
      "text_preview": "4 5.19 25.3 50",
      "original_order": 13,
      "sorted_order": 12
    },
    {
      "page": 9,
      "block_index": 13,
      "bbox": [
        148.1820068359375,
        257.21954345703125,
        493.35015869140625,
        269.2244873046875
      ],
      "text_preview": "8 4.88 25.5 80",
      "original_order": 14,
      "sorted_order": 13
    },
    {
      "page": 9,
      "block_index": 14,
      "bbox": [
        118.68000030517578,
        268.1285400390625,
        131.96014404296875,
        280.13348388671875
      ],
      "text_preview": "(C)",
      "original_order": 11,
      "sorted_order": 14
    },
    {
      "page": 9,
      "block_index": 15,
      "bbox": [
        171.25999450683594,
        268.1285400390625,
        493.35015869140625,
        280.13348388671875
      ],
      "text_preview": "256 32 32 5.75 24.5 28",
      "original_order": 15,
      "sorted_order": 15
    },
    {
      "page": 9,
      "block_index": 16,
      "bbox": [
        168.7689971923828,
        279.0375671386719,
        495.8407897949219,
        291.0425109863281
      ],
      "text_preview": "1024 128 128 4.66 26.0 168",
      "original_order": 16,
      "sorted_order": 16
    },
    {
      "page": 9,
      "block_index": 17,
      "bbox": [
        202.24600219726562,
        289.9465637207031,
        493.35015869140625,
        301.9515075683594
      ],
      "text_preview": "1024 5.12 25.4 53",
      "original_order": 17,
      "sorted_order": 17
    },
    {
      "page": 9,
      "block_index": 18,
      "bbox": [
        202.24600219726562,
        300.8555603027344,
        493.35015869140625,
        312.8605041503906
      ],
      "text_preview": "4096 4.75 26.2 90",
      "original_order": 18,
      "sorted_order": 18
    },
    {
      "page": 9,
      "block_index": 19,
      "bbox": [
        315.1130065917969,
        313.4935607910156,
        457.74383544921875,
        325.4985046386719
      ],
      "text_preview": "0.0 5.77 24.6",
      "original_order": 20,
      "sorted_order": 19
    },
    {
      "page": 9,
      "block_index": 20,
      "bbox": [
        315.1130065917969,
        324.4025573730469,
        457.74383544921875,
        336.4075012207031
      ],
      "text_preview": "0.2 4.95 25.5",
      "original_order": 21,
      "sorted_order": 20
    },
    {
      "page": 9,
      "block_index": 21,
      "bbox": [
        118.40599822998047,
        329.8575439453125,
        132.2340850830078,
        341.86248779296875
      ],
      "text_preview": "(D)",
      "original_order": 19,
      "sorted_order": 21
    },
    {
      "page": 9,
      "block_index": 22,
      "bbox": [
        344.7919921875,
        335.3115539550781,
        457.74383544921875,
        347.3164978027344
      ],
      "text_preview": "0.0 4.67 25.3",
      "original_order": 22,
      "sorted_order": 22
    },
    {
      "page": 9,
      "block_index": 23,
      "bbox": [
        344.7919921875,
        346.2205505371094,
        457.74383544921875,
        358.2254943847656
      ],
      "text_preview": "0.2 5.47 25.7",
      "original_order": 23,
      "sorted_order": 23
    },
    {
      "page": 9,
      "block_index": 24,
      "bbox": [
        118.95899963378906,
        358.8585510253906,
        457.74383544921875,
        370.8634948730469
      ],
      "text_preview": "(E) positional embedding instead of sinusoids 4.92 25.7",
      "original_order": 24,
      "sorted_order": 24
    },
    {
      "page": 9,
      "block_index": 25,
      "bbox": [
        118.9540023803711,
        371.13690185546875,
        495.8448791503906,
        384.0982360839844
      ],
      "text_preview": "big 6 1024 4096 16 0.3 300K 4.33 26.4 213",
      "original_order": 25,
      "sorted_order": 25
    },
    {
      "page": 9,
      "block_index": 26,
      "bbox": [
        108.0,
        413.4345703125,
        505.2413024902344,
        479.9844970703125
      ],
      "text_preview": "In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This su",
      "original_order": 26,
      "sorted_order": 26
    },
    {
      "page": 9,
      "block_index": 27,
      "bbox": [
        108.0,
        497.92401123046875,
        183.06671142578125,
        513.4777221679688
      ],
      "text_preview": "7 Conclusion",
      "original_order": 27,
      "sorted_order": 27
    },
    {
      "page": 9,
      "block_index": 28,
      "bbox": [
        108.0,
        525.6025390625,
        503.99725341796875,
        559.4254760742188
      ],
      "text_preview": "In this work, we presented the Transformer, the \ufb01rst sequence transduction model based entirely on a",
      "original_order": 28,
      "sorted_order": 28
    },
    {
      "page": 9,
      "block_index": 29,
      "bbox": [
        108.0,
        563.80859375,
        503.99761962890625,
        608.54150390625
      ],
      "text_preview": "For translation tasks, the Transformer can be trained signi\ufb01cantly faster than architectures based o",
      "original_order": 29,
      "sorted_order": 29
    },
    {
      "page": 9,
      "block_index": 30,
      "bbox": [
        107.53199768066406,
        612.924560546875,
        505.6082458496094,
        657.6575317382812
      ],
      "text_preview": "We are excited about the future of attention-based models and plan to apply them to other tasks. We",
      "original_order": 30,
      "sorted_order": 30
    },
    {
      "page": 9,
      "block_index": 31,
      "bbox": [
        107.69100189208984,
        662.04052734375,
        505.0701599121094,
        685.7415771484375
      ],
      "text_preview": "The code we used to train and evaluate our models is available at https://github.com/ tensorflow/ten",
      "original_order": 31,
      "sorted_order": 31
    },
    {
      "page": 9,
      "block_index": 32,
      "bbox": [
        107.99999237060547,
        699.5269165039062,
        503.9998779296875,
        722.7994995117188
      ],
      "text_preview": "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments,",
      "original_order": 32,
      "sorted_order": 32
    },
    {
      "page": 9,
      "block_index": 33,
      "bbox": [
        303.5090026855469,
        740.6825561523438,
        308.49029541015625,
        752.6875
      ],
      "text_preview": "9",
      "original_order": 33,
      "sorted_order": 33
    },
    {
      "page": 10,
      "block_index": 0,
      "bbox": [
        108.0,
        70.48602294921875,
        163.54383850097656,
        86.03973388671875
      ],
      "text_preview": "References",
      "original_order": 0,
      "sorted_order": 0
    },
    {
      "page": 10,
      "block_index": 1,
      "bbox": [
        112.98100280761719,
        90.57952880859375,
        504.0028381347656,
        113.49346160888672
      ],
      "text_preview": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv",
      "original_order": 1,
      "sorted_order": 1
    },
    {
      "page": 10,
      "block_index": 2,
      "bbox": [
        112.98098754882812,
        120.925537109375,
        504.3450927734375,
        143.83946228027344
      ],
      "text_preview": "[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learni",
      "original_order": 2,
      "sorted_order": 2
    },
    {
      "page": 10,
      "block_index": 3,
      "bbox": [
        112.98098754882812,
        151.27154541015625,
        504.0043640136719,
        174.1854705810547
      ],
      "text_preview": "[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machin",
      "original_order": 3,
      "sorted_order": 3
    },
    {
      "page": 10,
      "block_index": 4,
      "bbox": [
        112.98097229003906,
        181.6185302734375,
        504.0025634765625,
        204.53245544433594
      ],
      "text_preview": "[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading",
      "original_order": 4,
      "sorted_order": 4
    },
    {
      "page": 10,
      "block_index": 5,
      "bbox": [
        112.98098754882812,
        211.96453857421875,
        505.2427062988281,
        245.78746032714844
      ],
      "text_preview": "[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua",
      "original_order": 5,
      "sorted_order": 5
    },
    {
      "page": 10,
      "block_index": 6,
      "bbox": [
        112.98099517822266,
        253.21954345703125,
        504.0006408691406,
        276.13348388671875
      ],
      "text_preview": "[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint",
      "original_order": 6,
      "sorted_order": 6
    },
    {
      "page": 10,
      "block_index": 7,
      "bbox": [
        112.98099517822266,
        283.5655517578125,
        504.003662109375,
        306.4804992675781
      ],
      "text_preview": "[7] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated",
      "original_order": 7,
      "sorted_order": 7
    },
    {
      "page": 10,
      "block_index": 8,
      "bbox": [
        112.98098754882812,
        313.9125671386719,
        505.6533508300781,
        336.8265075683594
      ],
      "text_preview": "[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu- tional",
      "original_order": 8,
      "sorted_order": 8
    },
    {
      "page": 10,
      "block_index": 9,
      "bbox": [
        112.98101806640625,
        344.258544921875,
        503.9955749511719,
        367.1724853515625
      ],
      "text_preview": "[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850",
      "original_order": 9,
      "sorted_order": 9
    },
    {
      "page": 10,
      "block_index": 10,
      "bbox": [
        107.99999237060547,
        374.60455322265625,
        505.6490783691406,
        408.427490234375
      ],
      "text_preview": "[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recog",
      "original_order": 10,
      "sorted_order": 10
    },
    {
      "page": 10,
      "block_index": 11,
      "bbox": [
        108.00000762939453,
        415.86053466796875,
        504.0020751953125,
        438.7745056152344
      ],
      "text_preview": "[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient \ufb02ow in recurre",
      "original_order": 11,
      "sorted_order": 11
    },
    {
      "page": 10,
      "block_index": 12,
      "bbox": [
        107.99999237060547,
        446.20654296875,
        505.2454528808594,
        469.1205139160156
      ],
      "text_preview": "[12] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131",
      "original_order": 12,
      "sorted_order": 12
    },
    {
      "page": 10,
      "block_index": 13,
      "bbox": [
        107.99999237060547,
        476.55255126953125,
        503.9997253417969,
        499.46649169921875
      ],
      "text_preview": "[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the lim",
      "original_order": 13,
      "sorted_order": 13
    },
    {
      "page": 10,
      "block_index": 14,
      "bbox": [
        108.0,
        506.8985595703125,
        504.0038757324219,
        529.8125
      ],
      "text_preview": "[14] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on",
      "original_order": 14,
      "sorted_order": 14
    },
    {
      "page": 10,
      "block_index": 15,
      "bbox": [
        107.99996948242188,
        537.2455444335938,
        505.6535339355469,
        571.0684814453125
      ],
      "text_preview": "[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray",
      "original_order": 15,
      "sorted_order": 15
    },
    {
      "page": 10,
      "block_index": 16,
      "bbox": [
        107.99999237060547,
        578.5005493164062,
        505.7476501464844,
        601.4144897460938
      ],
      "text_preview": "[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In In",
      "original_order": 16,
      "sorted_order": 16
    },
    {
      "page": 10,
      "block_index": 17,
      "bbox": [
        108.0,
        608.8465576171875,
        505.745361328125,
        620.8515014648438
      ],
      "text_preview": "[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.",
      "original_order": 17,
      "sorted_order": 17
    },
    {
      "page": 10,
      "block_index": 18,
      "bbox": [
        108.0,
        628.2835693359375,
        504.000732421875,
        651.197509765625
      ],
      "text_preview": "[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arX",
      "original_order": 18,
      "sorted_order": 18
    },
    {
      "page": 10,
      "block_index": 19,
      "bbox": [
        107.99998474121094,
        658.6305541992188,
        504.0027770996094,
        692.4534912109375
      ],
      "text_preview": "[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua",
      "original_order": 19,
      "sorted_order": 19
    },
    {
      "page": 10,
      "block_index": 20,
      "bbox": [
        107.99998474121094,
        699.8855590820312,
        503.9985046386719,
        722.7994995117188
      ],
      "text_preview": "[20] Samy Bengio \u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural Informati",
      "original_order": 20,
      "sorted_order": 20
    },
    {
      "page": 10,
      "block_index": 21,
      "bbox": [
        301.01898193359375,
        740.6825561523438,
        310.9815673828125,
        752.6875
      ],
      "text_preview": "10",
      "original_order": 21,
      "sorted_order": 21
    },
    {
      "page": 11,
      "block_index": 0,
      "bbox": [
        108.0,
        72.757568359375,
        505.6534729003906,
        95.67150115966797
      ],
      "text_preview": "[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention- base",
      "original_order": 0,
      "sorted_order": 0
    },
    {
      "page": 11,
      "block_index": 1,
      "bbox": [
        107.99996948242188,
        102.5455322265625,
        504.0035095214844,
        125.45946502685547
      ],
      "text_preview": "[22] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention mode",
      "original_order": 1,
      "sorted_order": 1
    },
    {
      "page": 11,
      "block_index": 2,
      "bbox": [
        107.99996948242188,
        132.33355712890625,
        503.99749755859375,
        155.2474822998047
      ],
      "text_preview": "[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summa",
      "original_order": 2,
      "sorted_order": 2
    },
    {
      "page": 11,
      "block_index": 3,
      "bbox": [
        107.99996948242188,
        162.12255859375,
        504.0006408691406,
        185.03648376464844
      ],
      "text_preview": "[24] O\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint",
      "original_order": 3,
      "sorted_order": 3
    },
    {
      "page": 11,
      "block_index": 4,
      "bbox": [
        108.0,
        191.91058349609375,
        504.0021667480469,
        214.8245086669922
      ],
      "text_preview": "[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with",
      "original_order": 4,
      "sorted_order": 4
    },
    {
      "page": 11,
      "block_index": 5,
      "bbox": [
        107.99996948242188,
        221.69854736328125,
        505.24853515625,
        255.52146911621094
      ],
      "text_preview": "[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and J",
      "original_order": 5,
      "sorted_order": 5
    },
    {
      "page": 11,
      "block_index": 6,
      "bbox": [
        108.0,
        262.39556884765625,
        505.6509704589844,
        296.2195129394531
      ],
      "text_preview": "[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- n",
      "original_order": 6,
      "sorted_order": 6
    },
    {
      "page": 11,
      "block_index": 7,
      "bbox": [
        108.00000762939453,
        303.09356689453125,
        505.24591064453125,
        347.82550048828125
      ],
      "text_preview": "[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. I",
      "original_order": 7,
      "sorted_order": 7
    },
    {
      "page": 11,
      "block_index": 8,
      "bbox": [
        107.99999237060547,
        354.6995544433594,
        504.00286865234375,
        377.6134948730469
      ],
      "text_preview": "[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networ",
      "original_order": 8,
      "sorted_order": 8
    },
    {
      "page": 11,
      "block_index": 9,
      "bbox": [
        107.99996948242188,
        384.4885559082031,
        505.740478515625,
        407.4024963378906
      ],
      "text_preview": "[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethin",
      "original_order": 9,
      "sorted_order": 9
    },
    {
      "page": 11,
      "block_index": 10,
      "bbox": [
        107.99996948242188,
        414.27655029296875,
        504.00335693359375,
        459.0085144042969
      ],
      "text_preview": "[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim",
      "original_order": 10,
      "sorted_order": 10
    },
    {
      "page": 11,
      "block_index": 11,
      "bbox": [
        107.99998474121094,
        465.882568359375,
        504.0027770996094,
        488.7965087890625
      ],
      "text_preview": "[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward",
      "original_order": 11,
      "sorted_order": 11
    },
    {
      "page": 11,
      "block_index": 12,
      "bbox": [
        301.01898193359375,
        740.6825561523438,
        310.9815673828125,
        752.6875
      ],
      "text_preview": "11",
      "original_order": 12,
      "sorted_order": 12
    }
  ],
  "success": true,
  "error": null
}