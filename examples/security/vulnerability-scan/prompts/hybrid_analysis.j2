---
system_prompt: You are an expert security analyst with access to both direct file content and a Python execution environment. You can perform comprehensive analysis combining static review with dynamic testing and execution.
---

# Comprehensive Security Analysis - Hybrid Approach

I have access to the code files in two ways:
1. **Direct template access** for immediate static analysis
2. **Uploaded to execution environment** for dynamic testing and advanced analysis

## Files Available for Analysis:

{% for file in code %}
### {{ file.path }}
**Size**: {{ file.content|length }} characters
**Preview**: {{ file.content[:200] }}...
{% endfor %}

## Multi-Modal Analysis Approach:

### Phase 1: Immediate Static Analysis
Using the directly accessible file content, I'll first perform:

{% for file in code %}
#### Static Review: {{ file.path }}

**Full Content Analysis:**
```{{ file.path.split('.')[-1] }}
{{ file.content }}
```

**Initial Observations:**
- Language: {{ file.path.split('.')[-1] }}
- Lines of code: {{ file.content.split('\n')|length }}
- Apparent complexity: [To be assessed]
{% endfor %}

### Phase 2: Dynamic Analysis with Code Execution
Now I'll use the uploaded files in my execution environment for deeper analysis:

```python
# Advanced multi-file analysis using uploaded files
import os
import ast
import re
import subprocess
import sys
from collections import defaultdict

print("=== HYBRID SECURITY ANALYSIS REPORT ===\\n")

# Discover uploaded files
uploaded_files = []
for root, dirs, files in os.walk('.'):
    for file in files:
        if file.endswith(('.py', '.js', '.java', '.cpp', '.c', '.php', '.rb', '.go')):
            uploaded_files.append(os.path.join(root, file))

print(f"Found {len(uploaded_files)} code files for dynamic analysis:")
for file in uploaded_files:
    print(f"  - {file}")

# Cross-reference with template-provided files
template_files = [{% for file in code %}"{{ file.path }}"{% if not loop.last %}, {% endif %}{% endfor %}]
print(f"\\nTemplate provided {len(template_files)} files:")
for file in template_files:
    print(f"  - {file}")

# Comprehensive security analysis combining both sources
analysis_results = {}

for file_path in uploaded_files:
    print(f"\\n{'='*50}")
    print(f"ANALYZING: {file_path}")
    print(f"{'='*50}")
    
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        # Basic metrics
        lines = content.split('\\n')
        analysis_results[file_path] = {
            'total_lines': len(lines),
            'code_lines': len([l for l in lines if l.strip() and not l.strip().startswith('#')]),
            'comment_lines': len([l for l in lines if l.strip().startswith('#')]),
            'blank_lines': len([l for l in lines if not l.strip()]),
            'security_issues': [],
            'quality_issues': [],
            'complexity_score': 0
        }
        
        print(f"📊 Basic Metrics:")
        print(f"   Total lines: {analysis_results[file_path]['total_lines']}")
        print(f"   Code lines: {analysis_results[file_path]['code_lines']}")
        print(f"   Comment lines: {analysis_results[file_path]['comment_lines']}")
        print(f"   Blank lines: {analysis_results[file_path]['blank_lines']}")
        
        # Language-specific analysis
        if file_path.endswith('.py'):
            print(f"\\n🐍 Python-specific security analysis:")
            
            # AST analysis
            try:
                tree = ast.parse(content)
                
                # Count constructs
                functions = [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
                classes = [node.name for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
                imports = []
                
                for node in ast.walk(tree):
                    if isinstance(node, ast.Import):
                        imports.extend([alias.name for alias in node.names])
                    elif isinstance(node, ast.ImportFrom):
                        module = node.module or ''
                        imports.extend([f"{module}.{alias.name}" for alias in node.names])
                
                print(f"   Functions: {len(functions)} - {functions[:5]}{'...' if len(functions) > 5 else ''}")
                print(f"   Classes: {len(classes)} - {classes}")
                print(f"   Imports: {len(imports)} - {imports[:3]}{'...' if len(imports) > 3 else ''}")
                
                # Security analysis
                security_patterns = {
                    'eval_usage': r'\\beval\\s*\\(',
                    'exec_usage': r'\\bexec\\s*\\(',
                    'sql_injection': r'(execute|query)\\s*\\(\\s*[\"\\'].*[%\\+].*[\"\\']',
                    'shell_injection': r'(os\\.system|subprocess\\.call|subprocess\\.run).*shell\\s*=\\s*True',
                    'pickle_usage': r'\\bpickle\\.(loads?|dumps?)\\s*\\(',
                }
                
                print(f"\\n🔒 Security Analysis:")
                for pattern_name, pattern in security_patterns.items():
                    matches = list(re.finditer(pattern, content, re.IGNORECASE))
                    if matches:
                        print(f"   ⚠️  {pattern_name}: {len(matches)} occurrences")
                        for match in matches[:2]:  # Show first 2
                            line_num = content[:match.start()].count('\\n') + 1
                            print(f"      Line {line_num}: {match.group()}")
                        analysis_results[file_path]['security_issues'].append({
                            'type': pattern_name,
                            'count': len(matches),
                            'severity': 'HIGH' if pattern_name in ['eval_usage', 'exec_usage'] else 'MEDIUM'
                        })
                    else:
                        print(f"   ✅ {pattern_name}: Clean")
                
                # Complexity analysis
                complexity_indicators = {
                    'nested_loops': len(re.findall(r'for.*:\\s*\\n.*for.*:', content, re.MULTILINE)),
                    'long_functions': len([f for f in functions if len(f) > 20]),  # Rough estimate
                    'deep_nesting': content.count('    ') // 4,  # Rough indentation depth
                }
                
                complexity_score = sum(complexity_indicators.values())
                analysis_results[file_path]['complexity_score'] = complexity_score
                
                print(f"\\n📈 Complexity Analysis:")
                print(f"   Nested loops: {complexity_indicators['nested_loops']}")
                print(f"   Deep nesting levels: {complexity_indicators['deep_nesting']}")
                print(f"   Overall complexity score: {complexity_score}")
                
                # Syntax check
                print(f"\\n✅ Syntax Validation:")
                try:
                    compile(content, file_path, 'exec')
                    print(f"   ✅ Syntax is valid")
                except SyntaxError as e:
                    print(f"   ❌ Syntax error at line {e.lineno}: {e.msg}")
                    analysis_results[file_path]['quality_issues'].append({
                        'type': 'syntax_error',
                        'line': e.lineno,
                        'message': e.msg
                    })
                
            except Exception as e:
                print(f"   ❌ AST analysis failed: {e}")
        
        # Generic code quality checks
        print(f"\\n📋 Code Quality Checks:")
        
        # Long lines
        long_lines = [i+1 for i, line in enumerate(lines) if len(line) > 100]
        if long_lines:
            print(f"   ⚠️  Long lines (>100 chars): {len(long_lines)} lines")
            print(f"      Examples: {long_lines[:3]}")
        else:
            print(f"   ✅ Line length: All lines under 100 characters")
        
        # TODO comments
        todo_lines = [i+1 for i, line in enumerate(lines) if 'TODO' in line.upper() or 'FIXME' in line.upper()]
        if todo_lines:
            print(f"   📝 TODO/FIXME comments: {len(todo_lines)} found")
        else:
            print(f"   ✅ No TODO/FIXME comments found")
        
        # Comment ratio
        if analysis_results[file_path]['code_lines'] > 0:
            comment_ratio = analysis_results[file_path]['comment_lines'] / analysis_results[file_path]['code_lines']
            print(f"   📝 Comment ratio: {comment_ratio:.1%}")
            if comment_ratio < 0.1:
                print(f"      ⚠️  Low documentation (recommended: >10%)")
        
    except Exception as e:
        print(f"❌ Error analyzing {file_path}: {e}")

# Summary report
print(f"\\n{'='*60}")
print(f"SUMMARY REPORT")
print(f"{'='*60}")

total_files = len(analysis_results)
total_security_issues = sum(len(data['security_issues']) for data in analysis_results.values())
total_quality_issues = sum(len(data['quality_issues']) for data in analysis_results.values())
avg_complexity = sum(data['complexity_score'] for data in analysis_results.values()) / total_files if total_files > 0 else 0

print(f"📊 Overall Statistics:")
print(f"   Files analyzed: {total_files}")
print(f"   Security issues found: {total_security_issues}")
print(f"   Quality issues found: {total_quality_issues}")
print(f"   Average complexity score: {avg_complexity:.1f}")

# Risk assessment
risk_level = "LOW"
if total_security_issues > 0 or avg_complexity > 10:
    risk_level = "HIGH"
elif total_quality_issues > 5 or avg_complexity > 5:
    risk_level = "MEDIUM"

print(f"🎯 Overall Risk Level: {risk_level}")

print(f"\\n{'='*60}")
print(f"DETAILED FINDINGS BY FILE")
print(f"{'='*60}")

for file_path, data in analysis_results.items():
    print(f"\\n📁 {file_path}:")
    if data['security_issues']:
        for issue in data['security_issues']:
            print(f"   🚨 Security: {issue['type']} ({issue['severity']}) - {issue['count']} occurrences")
    if data['quality_issues']:
        for issue in data['quality_issues']:
            print(f"   ⚠️  Quality: {issue['type']} at line {issue.get('line', 'N/A')}")
    if not data['security_issues'] and not data['quality_issues']:
        print(f"   ✅ No major issues detected")
```

### Phase 3: Integrated Recommendations

Based on the combination of:
1. **Static analysis** from direct file access
2. **Dynamic analysis** from execution environment  
3. **Cross-validation** between both sources

I'll now provide comprehensive recommendations in the JSON schema format that leverages insights from both analysis methods.

## Hybrid Analysis Advantages

This comprehensive approach provides:
- ✅ **Immediate access** to full file content for context
- ✅ **Dynamic execution** capabilities for testing
- ✅ **Cross-validation** between static and dynamic findings
- ✅ **Comprehensive security** and quality analysis
- ✅ **Real-time syntax** and runtime validation
- ✅ **Advanced metrics** including complexity scoring
- ✅ **Multi-phase analysis** for maximum coverage

Present your final findings in the specified JSON schema format, incorporating insights from both the static template analysis and the dynamic execution environment analysis. 