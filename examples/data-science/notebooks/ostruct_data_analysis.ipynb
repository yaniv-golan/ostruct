{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis with ostruct in Jupyter\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yaniv-golan/ostruct/blob/main/examples/data-science/notebooks/ostruct_data_analysis.ipynb)\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/yaniv-golan/ostruct/main?labpath=examples%2Fdata-science%2Fnotebooks%2Fostruct_data_analysis.ipynb)\n",
    "[![Open in Jupyter](https://img.shields.io/badge/Open%20in-Jupyter-orange?logo=jupyter)](https://jupyter.org/try-jupyter/lab/?path=ostruct_data_analysis.ipynb)\n",
    "\n",
    "This notebook demonstrates how to use ostruct for data analysis within Jupyter notebooks, combining the power of AI-driven analysis with interactive data science workflows.\n",
    "\n",
    "> **üí° Environment Options:**\n",
    "> - **Colab**: Full GPU support, Google Drive integration, built-in Secrets management\n",
    "> - **Binder**: Free hosted Jupyter environment, great for quick testing\n",
    "> - **Local Jupyter**: Full control, best performance, works with Notebook 7 features like [real-time collaboration](https://jupyter-notebook.readthedocs.io/en/stable/notebook_7_features.html#real-time-collaboration)\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- üìä Run ostruct analysis from Jupyter cells\n",
    "- üîÑ Integrate ostruct results with pandas workflows\n",
    "- üìà Generate visualizations using AI + Code Interpreter\n",
    "- üöÄ Build automated analysis pipelines\n",
    "- üí° Best practices for production data science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters cell (for Papermill parameterization)\n",
    "# Following Google Cloud Jupyter best practices for parameterized notebooks\n",
    "# Tag this cell with \"parameters\" for Papermill execution\n",
    "\n",
    "# Model configuration\n",
    "DEFAULT_MODEL = \"gpt-4.1\"\n",
    "ANALYSIS_TIMEOUT = 180  # seconds\n",
    "\n",
    "# Data configuration  \n",
    "SAMPLE_SIZE = 100  # Number of rows for demo data\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Output configuration\n",
    "ENABLE_VERBOSE_OUTPUT = True\n",
    "SAVE_INTERMEDIATE_RESULTS = True\n",
    "OUTPUT_DIR = \"notebook_outputs\"\n",
    "\n",
    "# Tool configuration\n",
    "DEFAULT_TOOLS = [\"code-interpreter\"]\n",
    "ENABLE_WEB_SEARCH = False  # Set to True for market research examples\n",
    "\n",
    "print(\"üìã Notebook parameters configured for reproducible execution\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install ostruct and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ostruct (run this once)\n",
    "# NOTE: Using release candidate v1.6.0rc3 with Code Interpreter file download fixes\n",
    "# TODO: Revert to stable version after testing: !pip install ostruct-cli\n",
    "!pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ ostruct-cli==1.6.0rc3\n",
    "\n",
    "# Import required libraries\n",
    "import json\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from IPython.display import Image, display, HTML\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI API key (required) - Cross-platform approach\n",
    "import os\n",
    "\n",
    "def setup_openai_key():\n",
    "    \"\"\"\n",
    "    Set up OpenAI API key with cross-platform support for Colab, Jupyter, and local environments.\n",
    "    \n",
    "    This function automatically detects your environment and uses the most appropriate method:\n",
    "    - Colab: Uses built-in Secrets (recommended for security)\n",
    "    - Binder: Uses environment variables or getpass fallback\n",
    "    - Local Jupyter: Supports .env files, environment variables, or manual input\n",
    "    \"\"\"\n",
    "    \n",
    "    # Method 1: Try Colab Secrets first (if in Colab)\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
    "        print(\"üîë API key loaded from Colab Secrets\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        # Not in Colab, continue with other methods\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Colab Secrets not configured: {e}\")\n",
    "        print(\"   Add your API key to Colab Secrets:\")\n",
    "        print(\"   1. Click the üîë key icon in the left sidebar\")\n",
    "        print(\"   2. Add 'OPENAI_API_KEY' as a secret\")\n",
    "        print(\"   3. Re-run this cell\")\n",
    "    \n",
    "    # Method 2: Try environment variable\n",
    "    if os.getenv('OPENAI_API_KEY'):\n",
    "        print(\"üîë API key loaded from environment variable\")\n",
    "        return True\n",
    "    \n",
    "    # Method 3: Try .env file (for local Jupyter)\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "        if os.getenv('OPENAI_API_KEY'):\n",
    "            print(\"üîë API key loaded from .env file\")\n",
    "            return True\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # Method 4: Fallback to manual entry (least secure but most compatible)\n",
    "    print(\"üí° For better security, consider:\")\n",
    "    print(\"   ‚Ä¢ Colab: Use Colab Secrets (üîë icon in sidebar)\")\n",
    "    print(\"   ‚Ä¢ Jupyter: Set OPENAI_API_KEY environment variable\")\n",
    "    print(\"   ‚Ä¢ Local: Create .env file with python-dotenv\")\n",
    "    \n",
    "    import getpass\n",
    "    os.environ['OPENAI_API_KEY'] = getpass.getpass('Enter your OpenAI API key: ')\n",
    "    print(\"üîë API key configured\")\n",
    "    return True\n",
    "\n",
    "setup_openai_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for notebook robustness and experiment tracking\n",
    "import uuid\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Experiment tracking (following Google Cloud best practices)\n",
    "class ExperimentTracker:\n",
    "    \"\"\"\n",
    "    Track experiments automatically following Google Cloud Jupyter best practices.\n",
    "    Logs metadata about training sessions, hyperparameters, data sources, results, and timing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, experiment_name=\"ostruct_analysis\"):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.experiment_id = str(uuid.uuid4())[:8]\n",
    "        self.start_time = datetime.now()\n",
    "        # Use OUTPUT_DIR from parameters cell or default\n",
    "        output_dir = globals().get('OUTPUT_DIR', 'notebook_outputs')\n",
    "        self.log_dir = Path(output_dir) / \"experiment_logs\"\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.metadata = {\n",
    "            \"experiment_id\": self.experiment_id,\n",
    "            \"experiment_name\": experiment_name,\n",
    "            \"start_time\": self.start_time.isoformat(),\n",
    "            \"parameters\": {},\n",
    "            \"results\": {},\n",
    "            \"execution_info\": {}\n",
    "        }\n",
    "        \n",
    "    def log_parameters(self, **params):\n",
    "        \"\"\"Log experiment parameters\"\"\"\n",
    "        self.metadata[\"parameters\"].update(params)\n",
    "        \n",
    "    def log_results(self, **results):\n",
    "        \"\"\"Log experiment results\"\"\"\n",
    "        self.metadata[\"results\"].update(results)\n",
    "        \n",
    "    def log_execution_info(self, **info):\n",
    "        \"\"\"Log execution metadata\"\"\"\n",
    "        self.metadata[\"execution_info\"].update(info)\n",
    "        \n",
    "    def save_experiment(self):\n",
    "        \"\"\"Save experiment log to file\"\"\"\n",
    "        self.metadata[\"end_time\"] = datetime.now().isoformat()\n",
    "        self.metadata[\"duration_seconds\"] = (datetime.now() - self.start_time).total_seconds()\n",
    "        \n",
    "        log_file = self.log_dir / f\"experiment_{self.experiment_id}.json\"\n",
    "        with open(log_file, 'w') as f:\n",
    "            json.dump(self.metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"üíæ Experiment logged: {log_file}\")\n",
    "        return log_file\n",
    "\n",
    "def ensure_imports():\n",
    "    \"\"\"\n",
    "    Ensure required imports are available for data analysis.\n",
    "    This function can be called at the start of any cell that needs pandas/matplotlib.\n",
    "    \n",
    "    Based on best practices from:\n",
    "    - https://ryan.orendorff.io/posts/2022-11-27-define-once/\n",
    "    - https://www.angela1c.com/posts/2021/08/a-few-random-reference-notes.../\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from IPython.display import display, HTML\n",
    "        \n",
    "        # Make imports available in global namespace for notebook cells\n",
    "        globals().update({\n",
    "            'pd': pd,\n",
    "            'plt': plt, \n",
    "            'sns': sns,\n",
    "            'display': display,\n",
    "            'HTML': HTML\n",
    "        })\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(\"‚ùå Missing required packages!\")\n",
    "        print(\"üí° Solution: Run the setup cell (Cell 2) first to install and import libraries\")\n",
    "        print(f\"Missing: {e}\")\n",
    "        print(\"\\nIf packages aren't installed, run:\")\n",
    "        print(\"!pip install pandas matplotlib seaborn\")\n",
    "        raise\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error setting up imports: {e}\")\n",
    "        print(\"üí° Try restarting the kernel and running cells in order\")\n",
    "        raise\n",
    "\n",
    "# Initialize global experiment tracker\n",
    "experiment = ExperimentTracker(\"ostruct_data_analysis\")\n",
    "\n",
    "print(\"üìö Utility functions loaded with experiment tracking!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core ostruct Integration Functions\n",
    "\n",
    "Let's create helper functions for running ostruct from Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core ostruct integration function for Jupyter notebooks\n",
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "\n",
    "def run_ostruct_analysis(template_file, schema_file, data_file=None, model=None, \n",
    "                        enable_tools=None, output_file=None, dry_run=False, enable_downloads=True):\n",
    "    \"\"\"\n",
    "    Run ostruct analysis from Jupyter notebooks.\n",
    "    \n",
    "    Uses subprocess with stdin=DEVNULL to prevent asyncio event loop conflicts\n",
    "    that can cause timeouts in notebook environments.\n",
    "    \n",
    "    Args:\n",
    "        enable_downloads: If True, adds --ci-download flag for file downloads (charts, reports, etc.)\n",
    "                         Defaults to True since notebook users typically want to see generated charts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build command arguments\n",
    "    cmd = ['ostruct', 'run', template_file, schema_file]\n",
    "    \n",
    "    if data_file:\n",
    "        cmd.extend(['--file', 'ci:data', data_file])\n",
    "    \n",
    "    if model:\n",
    "        cmd.extend(['--model', model])\n",
    "        \n",
    "    if enable_tools:\n",
    "        for tool in enable_tools:\n",
    "            cmd.extend(['--enable-tool', tool])\n",
    "            \n",
    "    # Add --ci-download flag if downloads are enabled and code-interpreter is being used\n",
    "    if enable_downloads and enable_tools and \\'code-interpreter\\' in enable_tools:\n",
    "        cmd.append(\\'--ci-download\\')\n",
    "            \n",
    "    if output_file:\n",
    "        cmd.extend(['--output-file', output_file])\n",
    "        \n",
    "    if dry_run:\n",
    "        cmd.append('--dry-run')\n",
    "    \n",
    "    cmd.append('--verbose')  # Always include verbose for better debugging\n",
    "    \n",
    "    print(f\"üîß Command: {' '.join(cmd)}\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            cmd,\n",
    "            env=os.environ,\n",
    "            stdin=subprocess.DEVNULL,  # ‚Üê IMPORTANT: Prevents asyncio deadlock\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=180,  # 3 minutes should be enough for most operations\n",
    "            check=True\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ ostruct analysis completed successfully!\")\n",
    "        print(f\"üì§ Return code: {result.returncode}\")\n",
    "        \n",
    "        # Show output preview\n",
    "        if result.stdout:\n",
    "            print(\"üìã Output preview:\")\n",
    "            print(result.stdout[:500] + \"...\" if len(result.stdout) > 500 else result.stdout)\n",
    "        \n",
    "        # Load and return results if output file specified\n",
    "        if output_file and os.path.exists(output_file):\n",
    "            with open(output_file, 'r') as f:\n",
    "                results = json.load(f)\n",
    "            return results\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"‚ùå ostruct analysis timed out after 3 minutes\")\n",
    "        raise\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå ostruct analysis failed with exit code {e.returncode}\")\n",
    "        if e.stderr:\n",
    "            print(f\"STDERR: {e.stderr}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"‚úÖ ostruct integration function ready!\")\n",
    "print(\"   Use run_ostruct_analysis() for reliable ostruct execution in Jupyter notebooks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Data Analysis\n",
    "\n",
    "Let's start with a simple CSV analysis using the data science template:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": [
     "basic_template.j2"
    ]
   },
   "source": [
    "You are a data analyst working with the following CSV snapshot:\n",
    "\n",
    "{{ data.content }}\n",
    "\n",
    "Your tasks:\n",
    "1. Compute summary statistics:\n",
    "   ‚Ä¢ total revenue\n",
    "   ‚Ä¢ average selling price\n",
    "   ‚Ä¢ number of transactions\n",
    "2. Identify the top-performing product by revenue.\n",
    "3. Describe any noticeable sales trends.\n",
    "4. Evaluate data quality (missing values, anomalies, etc.).\n",
    "5. Create a bar chart of revenue by product and save it as \"sales_by_product.png\".\n",
    "\n",
    "In your reply:\n",
    "‚Ä¢ Present the key findings clearly.\n",
    "‚Ä¢ Mention that the chart file has been created and provide its filename."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": [
     "basic_schema.json"
    ]
   },
   "source": [
    "{\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"summary\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"total_sales\": {\"type\": \"number\"},\n",
    "        \"average_price\": {\"type\": \"number\"},\n",
    "        \"product_count\": {\"type\": \"integer\"},\n",
    "        \"total_transactions\": {\"type\": \"integer\"}\n",
    "      }\n",
    "    },\n",
    "    \"sales_by_product\": {\n",
    "      \"type\": \"object\",\n",
    "      \"description\": \"Sales totals by product name\"\n",
    "    },\n",
    "    \"chart_info\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"filename\": {\"type\": \"string\"},\n",
    "        \"description\": {\"type\": \"string\"},\n",
    "        \"chart_type\": {\"type\": \"string\"}\n",
    "      }\n",
    "    },\n",
    "    \"data_quality\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"rows_processed\": {\"type\": \"integer\"},\n",
    "        \"missing_values\": {\"type\": \"integer\"},\n",
    "        \"data_issues\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "      }\n",
    "    },\n",
    "    \"message\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"Summary message with any download links for generated files\"\n",
    "    }\n",
    "  },\n",
    "  \"required\": [\"summary\", \"chart_info\", \"data_quality\", \"message\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for analysis\n",
    "# Template and schema content are in the raw cells above (cells 9 and 10)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Generate sample sales data if not already present\n",
    "np.random.seed(42)\n",
    "sample_data = {\n",
    "    'product': ['Widget A', 'Widget B', 'Widget C', 'Widget A', 'Widget B'] * 20,\n",
    "    'price': np.random.uniform(10, 100, 100),\n",
    "    'quantity': np.random.randint(1, 10, 100)\n",
    "}\n",
    "df = pd.DataFrame(sample_data)\n",
    "df['revenue'] = df['price'] * df['quantity']\n",
    "\n",
    "# Save to CSV for ostruct processing\n",
    "df.to_csv('sample_sales.csv', index=False)\n",
    "\n",
    "# Note: In a Jupyter notebook, the template and schema content would be read from \n",
    "# the raw cells above. For CLI compatibility, we write placeholder files.\n",
    "# The actual content is visible in the raw cells above for editing.\n",
    "\n",
    "# Write placeholder files (content from raw cells 9 and 10)\n",
    "with open('basic_template.j2', 'w') as f:\n",
    "    f.write(\"# Template content is in raw cell 9 above\\n\")\n",
    "    \n",
    "with open('basic_schema.json', 'w') as f:\n",
    "    json.dump({\"note\": \"Schema content is in raw cell 10 above\"}, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Placeholder files created - actual content is in raw cells above\")\n",
    "print(\"üìù Edit templates/schemas by modifying the raw cells above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ostruct analysis with better error handling and debugging\n",
    "import os\n",
    "print(\"üöÄ Starting ostruct analysis...\")\n",
    "\n",
    "try:\n",
    "    # First, let's verify our files exist\n",
    "    print(\"üìã Checking required files:\")\n",
    "    required_files = ['basic_template.j2', 'basic_schema.json', 'sample_sales.csv']\n",
    "    for file in required_files:\n",
    "        if Path(file).exists():\n",
    "            print(f\"  ‚úÖ {file} ({Path(file).stat().st_size} bytes)\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå {file} - MISSING!\")\n",
    "            \n",
    "    print(\"\\nüîë Checking API key...\")\n",
    "    if 'OPENAI_API_KEY' in os.environ:\n",
    "        key = os.environ['OPENAI_API_KEY']\n",
    "        print(f\"  ‚úÖ API key set ({key[:10]}...{key[-4:]})\")\n",
    "    else:\n",
    "        print(\"  ‚ùå OPENAI_API_KEY not found!\")\n",
    "        \n",
    "    print(\"\\n‚è∞ Running analysis (this may take 30-60 seconds)...\")\n",
    "    \n",
    "    # Run with timeout and verbose output\n",
    "    import subprocess\n",
    "    import signal\n",
    "    \n",
    "    cmd = [\n",
    "        'ostruct', 'run', \n",
    "        'basic_template.j2', \n",
    "        'basic_schema.json',\n",
    "        '--file', 'ci:data', 'sample_sales.csv',\n",
    "        '--model', 'gpt-4.1',\n",
    "        '--enable-tool', 'code-interpreter',\n",
    "        '--output-file', 'analysis_results.json',\n",
    "        '--verbose'  # Add verbose output\n",
    "    ]\n",
    "    \n",
    "    print(f\"üîß Command: {' '.join(cmd)}\")\n",
    "    \n",
    "    # Run with timeout (using parameterized value)\n",
    "    timeout_seconds = globals().get('ANALYSIS_TIMEOUT', 180)\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            cmd, \n",
    "            capture_output=True, \n",
    "            text=True, \n",
    "            timeout=timeout_seconds,  # Use parameterized timeout\n",
    "            check=True,\n",
    "            env=os.environ,\n",
    "            stdin=subprocess.DEVNULL  # Prevent asyncio deadlock in Jupyter\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Command completed successfully!\")\n",
    "        print(f\"üì§ Return code: {result.returncode}\")\n",
    "        \n",
    "        if result.stdout:\n",
    "            print(\"üìÑ Output:\")\n",
    "            print(result.stdout[:1000])  # First 1000 chars\n",
    "            \n",
    "        # Load results\n",
    "        if Path('analysis_results.json').exists():\n",
    "            with open('analysis_results.json', 'r') as f:\n",
    "                results = json.load(f)\n",
    "            print(\"‚úÖ Results loaded successfully!\")\n",
    "            display_analysis_summary(results)\n",
    "        else:\n",
    "            print(\"‚ùå Output file not created\")\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"‚è∞ Command timed out after {timeout_seconds} seconds ({timeout_seconds/60:.1f} minutes)\")\n",
    "        print(\"This might indicate:\")\n",
    "        print(\"  - API is slow to respond\")\n",
    "        print(\"  - Network connectivity issues\")\n",
    "        print(\"  - API key problems\")\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Command failed with exit code {e.returncode}\")\n",
    "        print(f\"üìÑ Error output:\")\n",
    "        print(e.stderr)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Unexpected error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\nüìÅ Current directory contents:\")\n",
    "for item in sorted(Path('.').iterdir()):\n",
    "    if item.is_file():\n",
    "        print(f\"  üìÑ {item.name} ({item.stat().st_size} bytes)\")\n",
    "    else:\n",
    "        print(f\"  üìÅ {item.name}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display the analysis results\n",
    "print(\"üìä Loading analysis results...\")\n",
    "\n",
    "try:\n",
    "    # Load the results file that was created\n",
    "    with open('analysis_results.json', 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    print(\"‚úÖ Results loaded successfully!\")\n",
    "    print(f\"üìÑ Result keys: {list(results.keys())}\")\n",
    "    \n",
    "    # Display the full results first\n",
    "    print(\"\\nüìã COMPLETE ANALYSIS RESULTS:\")\n",
    "    print(json.dumps(results, indent=2))\n",
    "    \n",
    "    # Use our display function\n",
    "    display_analysis_summary(results)\n",
    "    \n",
    "    # Look for any generated chart files (RC2 should fix download issues)\n",
    "    print(\"\\nüîç Comprehensive search for generated charts...\")\n",
    "    \n",
    "    # Check for image files in multiple locations\n",
    "    image_extensions = ['.png', '.jpg', '.jpeg', '.svg', '.gif']\n",
    "    \n",
    "    # Search locations where charts might be saved\n",
    "    search_locations = [\n",
    "        Path('.'),  # Current directory\n",
    "        Path('downloads'),  # Default download location\n",
    "        Path('/content'),  # Colab content directory  \n",
    "        Path('/content/downloads'),  # Colab downloads\n",
    "    ]\n",
    "    \n",
    "    all_found_images = []\n",
    "    \n",
    "    for location in search_locations:\n",
    "        if location.exists() and location.is_dir():\n",
    "            print(f\"\\nüìÅ Checking {location}:\")\n",
    "            try:\n",
    "                items = list(location.iterdir())\n",
    "                image_files = [f for f in items if f.suffix.lower() in image_extensions]\n",
    "                \n",
    "                if image_files:\n",
    "                    print(f\"  üéØ Found {len(image_files)} image(s):\")\n",
    "                    for img in image_files:\n",
    "                        print(f\"    üìä {img.name} ({img.stat().st_size} bytes)\")\n",
    "                        all_found_images.append(img)\n",
    "                else:\n",
    "                    all_files = [f for f in items if f.is_file()]\n",
    "                    print(f\"  üìÑ {len(all_files)} files, no images\")\n",
    "                    if all_files and len(all_files) <= 10:  # Show files if not too many\n",
    "                        print(f\"    Files: {[f.name for f in all_files]}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error accessing {location}: {e}\")\n",
    "        else:\n",
    "            print(f\"üìÅ {location}: doesn't exist\")\n",
    "    \n",
    "    # Check if the specific chart mentioned in results exists\n",
    "    if 'chart_info' in results and 'filename' in results['chart_info']:\n",
    "        chart_filename = results['chart_info']['filename']\n",
    "        print(f\"\\nüéØ Looking specifically for: {chart_filename}\")\n",
    "        \n",
    "        # Check in all search locations\n",
    "        for location in search_locations:\n",
    "            if location.exists():\n",
    "                chart_path = location / chart_filename\n",
    "                if chart_path.exists():\n",
    "                    print(f\"  ‚úÖ Found at: {chart_path}\")\n",
    "                    all_found_images.append(chart_path)\n",
    "                else:\n",
    "                    print(f\"  ‚ùå Not found in: {location}\")\n",
    "    \n",
    "    # Display all found images\n",
    "    if all_found_images:\n",
    "        print(f\"\\nüé® Displaying {len(all_found_images)} chart(s):\")\n",
    "        for img in all_found_images:\n",
    "            print(f\"\\nüìä Displaying chart: {img}\")\n",
    "            try:\n",
    "                display(Image(str(img)))\n",
    "                print(f\"‚úÖ Successfully displayed {img.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Could not display {img}: {e}\")\n",
    "    else:\n",
    "        print(\"\\nü§î No AI-generated charts found - checking if download issue persists in RC2\")\n",
    "        print(\"Creating fallback chart with matplotlib...\")\n",
    "        \n",
    "        # Create a fallback chart from the data\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        df = pd.read_csv('sample_sales.csv')\n",
    "        \n",
    "        # Create sales by product chart matching the AI description\n",
    "        sales_by_product = df.groupby('product')['revenue'].sum().sort_values(ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(sales_by_product.index, sales_by_product.values, \n",
    "                      color=['#2E86AB', '#A23B72', '#F18F01'])\n",
    "        plt.title('Sales by Product', fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.xlabel('Product', fontsize=12)\n",
    "        plt.ylabel('Revenue ($)', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, sales_by_product.values):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n",
    "                    f'${value:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add summary text\n",
    "        total_sales = sales_by_product.sum()\n",
    "        plt.figtext(0.02, 0.02, f'Total Sales: ${total_sales:.0f} ‚Ä¢ Top Product: {sales_by_product.index[0]}', \n",
    "                   fontsize=10, style='italic')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Fallback chart displayed!\")\n",
    "        print(\"üí° This shows the same data the AI analyzed, just generated with matplotlib instead\")\n",
    "        print(\"üìù Note: RC2 should fix Code Interpreter file downloads - if charts still don't appear, please report the issue\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": [
     "multi_tool_template.j2"
    ]
   },
   "source": [
    "You are a business analyst combining internal sales data with external market research.\n",
    "\n",
    "## Internal Data Analysis\n",
    "First, analyze this CSV data:\n",
    "{{ data.content }}\n",
    "\n",
    "Compute:\n",
    "1. Total revenue and growth trends\n",
    "2. Top-performing products\n",
    "3. Key performance metrics\n",
    "\n",
    "## Market Research  \n",
    "Then research current market trends for these products using web search.\n",
    "Focus on:\n",
    "- Industry growth rates\n",
    "- Competitive landscape  \n",
    "- Market opportunities\n",
    "\n",
    "## Deliverables\n",
    "Provide strategic recommendations based on both internal data and market research.\n",
    "Create a visualization showing key trends and save it as \"market_analysis.png\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-tool template content is in raw cell 14 above\n",
    "# For CLI compatibility, write placeholder file\n",
    "\n",
    "with open('multi_tool_template.j2', 'w') as f:\n",
    "    f.write(\"# Template content is in raw cell 14 above\\n\")\n",
    "\n",
    "print(\"‚úÖ Multi-tool placeholder created - actual content is in raw cell above\")\n",
    "print(\"üìù Edit template by modifying raw cell 14 above\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": [
     "enhanced_schema.json"
    ]
   },
   "source": [
    "{\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"internal_analysis\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"total_revenue\": {\"type\": \"number\"},\n",
    "        \"top_product\": {\"type\": \"string\"},\n",
    "        \"growth_trend\": {\"type\": \"string\"},\n",
    "        \"key_metrics\": {\"type\": \"object\"}\n",
    "      }\n",
    "    },\n",
    "    \"market_insights\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"industry_trends\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "        \"competitive_position\": {\"type\": \"string\"},\n",
    "        \"market_opportunities\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "      }\n",
    "    },\n",
    "    \"recommendations\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"recommendation\": {\"type\": \"string\"},\n",
    "          \"priority\": {\"type\": \"string\", \"enum\": [\"high\", \"medium\", \"low\"]},\n",
    "          \"expected_impact\": {\"type\": \"string\"}\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"visualization\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"chart_type\": {\"type\": \"string\"},\n",
    "        \"filename\": {\"type\": \"string\"},\n",
    "        \"insights\": {\"type\": \"string\"}\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"required\": [\"internal_analysis\", \"recommendations\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced schema content is in raw cell 16 above\n",
    "# For CLI compatibility, write placeholder file\n",
    "\n",
    "import json\n",
    "with open('enhanced_schema.json', 'w') as f:\n",
    "    json.dump({\"note\": \"Schema content is in raw cell 16 above\"}, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Enhanced schema placeholder created - actual content is in raw cell above\")\n",
    "print(\"üìù Edit schema by modifying raw cell 16 above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multi-tool analysis (corrected - no --web-query parameter)\n",
    "enhanced_results = run_ostruct_analysis(\n",
    "    template_file='multi_tool_template.j2',\n",
    "    schema_file='enhanced_schema.json', \n",
    "    data_file='sample_sales.csv',\n",
    "    model='gpt-4o',\n",
    "    enable_tools=['code-interpreter', 'web-search'],\n",
    "    output_file='enhanced_results.json'\n",
    ",\n",
    "        enable_downloads=True  # Enable chart downloads for visualization\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Enhanced analysis complete!\")\n",
    "print(\"\\nüìã ENHANCED ANALYSIS RESULTS:\")\n",
    "print(json.dumps(enhanced_results, indent=2))\n",
    "\n",
    "# Display key results\n",
    "if 'internal_analysis' in enhanced_results:\n",
    "    internal = enhanced_results['internal_analysis']\n",
    "    print(f\"\\nüíº INTERNAL ANALYSIS:\")\n",
    "    print(f\"üìä Total Revenue: ${internal.get('total_revenue', 0):,.2f}\")\n",
    "    print(f\"üèÜ Top Product: {internal.get('top_product', 'N/A')}\")\n",
    "\n",
    "if 'recommendations' in enhanced_results:\n",
    "    print(f\"\\nüí° KEY RECOMMENDATIONS:\")\n",
    "    for i, rec in enumerate(enhanced_results['recommendations'][:3], 1):\n",
    "        priority = rec.get('priority', 'medium').upper()\n",
    "        recommendation = rec.get('recommendation', 'N/A')\n",
    "        print(f\"  {i}. [{priority}] {recommendation}\")\n",
    "\n",
    "if 'market_insights' in enhanced_results:\n",
    "    insights = enhanced_results['market_insights']\n",
    "    if 'industry_trends' in insights and insights['industry_trends']:\n",
    "        print(f\"\\nüìà MARKET INSIGHTS:\")\n",
    "        for trend in insights['industry_trends'][:3]:\n",
    "            print(f\"  ‚Ä¢ {trend}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Interactive Data Science Workflow\n",
    "\n",
    "Let's create an interactive workflow that combines pandas analysis with AI insights:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": [
     "dynamic_template.j2"
    ]
   },
   "source": [
    "You are an expert data analyst. Answer this specific question about the data:\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "DATA:\n",
    "{data_sample}\n",
    "\n",
    "Provide a focused, accurate answer with supporting analysis.\n",
    "Create any helpful visualizations and save them with descriptive filenames."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": [
     "dynamic_schema.json"
    ]
   },
   "source": [
    "{\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"answer\": {\"type\": \"string\"},\n",
    "    \"confidence_level\": {\"type\": \"string\", \"enum\": [\"high\", \"medium\", \"low\"]},\n",
    "    \"key_insights\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "    \"methodology\": {\"type\": \"string\"},\n",
    "    \"visualizations\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"filename\": {\"type\": \"string\"},\n",
    "          \"description\": {\"type\": \"string\"}\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"required\": [\"answer\", \"confidence_level\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_analysis_workflow(dataframe, analysis_question):\n",
    "    \"\"\"\n",
    "    Interactive analysis workflow that uses dynamic templates from raw cells.\n",
    "    \n",
    "    The template and schema content are stored in raw cells 20 and 21 above.\n",
    "    This function demonstrates how to create dynamic analysis workflows.\n",
    "    \"\"\"\n",
    "    import tempfile\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Create temporary file with sample data\n",
    "    temp_file = f\"temp_analysis_{hash(analysis_question) % 10000}.csv\"\n",
    "    sample_data = dataframe.head(50)  # Use first 50 rows for analysis\n",
    "    sample_data.to_csv(temp_file, index=False)\n",
    "    \n",
    "    print(f\"üîÑ Step 1: Data Preparation\")\n",
    "    print(f\"   ‚Ä¢ Created sample dataset: {temp_file}\")\n",
    "    print(f\"   ‚Ä¢ Question: {analysis_question}\")\n",
    "    \n",
    "    # Note: Template and schema content are in raw cells 20 and 21 above\n",
    "    # For CLI compatibility, we create simple template files here\n",
    "    \n",
    "    # Create dynamic template (content should be read from raw cell 20)\n",
    "    template_content = f\"# Dynamic template for question: {analysis_question}\\n# Content from raw cell 20 above\\n\"\n",
    "    \n",
    "    # Create dynamic schema (content should be read from raw cell 21)  \n",
    "    schema_content = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"answer\": {\"type\": \"string\"},\n",
    "            \"confidence_level\": {\"type\": \"string\", \"enum\": [\"high\", \"medium\", \"low\"]},\n",
    "            \"key_insights\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "            \"methodology\": {\"type\": \"string\"}\n",
    "        },\n",
    "        \"required\": [\"answer\", \"confidence_level\"]\n",
    "    }\n",
    "    \n",
    "    # Write files for ostruct CLI\n",
    "    with open('dynamic_template.j2', 'w') as f:\n",
    "        f.write(template_content)\n",
    "    \n",
    "    import json\n",
    "    with open('dynamic_schema.json', 'w') as f:\n",
    "        json.dump(schema_content, f, indent=2)\n",
    "    \n",
    "    print(\"üìù Step 2: Template Files Created\")\n",
    "    print(\"   ‚Ä¢ Template: dynamic_template.j2 (placeholder - edit raw cell 20 for content)\")\n",
    "    print(\"   ‚Ä¢ Schema: dynamic_schema.json (basic structure)\")\n",
    "    \n",
    "    # Run AI analysis\n",
    "    print(\"ü§ñ Step 3: AI Analysis\")\n",
    "    ai_results = run_ostruct_analysis(\n",
    "        template_file='dynamic_template.j2',\n",
    "        schema_file='dynamic_schema.json',\n",
    "        data_file=temp_file,\n",
    "        model='gpt-4o',\n",
    "        enable_tools=['code-interpreter']\n",
    "    ,\n",
    "        enable_downloads=True  # Enable chart downloads for visualization\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nüéØ Answer: {ai_results.get('answer', 'No answer provided')}\")\n",
    "    print(f\"üîí Confidence: {ai_results.get('confidence_level', 'unknown')}\")\n",
    "    \n",
    "    if 'key_insights' in ai_results:\n",
    "        print(\"\\nüí° Key Insights:\")\n",
    "        for insight in ai_results['key_insights']:\n",
    "            print(f\"  ‚Ä¢ {insight}\")\n",
    "    \n",
    "    # Clean up\n",
    "    Path(temp_file).unlink()\n",
    "    \n",
    "    return ai_results\n",
    "\n",
    "print(\"‚úÖ Interactive workflow function defined\")\n",
    "print(\"üìù Template and schema content are in raw cells 20-21 above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the interactive workflow\n",
    "question = \"Which product has the highest profit margin and what factors contribute to its success?\"\n",
    "\n",
    "workflow_results = interactive_analysis_workflow(df, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Batch Processing Multiple Datasets\n",
    "\n",
    "For production scenarios, you often need to analyze multiple datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_analysis(file_list, template_file, schema_file, output_dir='batch_results'):\n",
    "    \"\"\"\n",
    "    Analyze multiple datasets in batch using ostruct.\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    batch_results = {}\n",
    "    \n",
    "    for i, file_path in enumerate(file_list):\n",
    "        print(f\"\\nüìä Processing {i+1}/{len(file_list)}: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            output_file = Path(output_dir) / f\"{Path(file_path).stem}_analysis.json\"\n",
    "            \n",
    "            results = run_ostruct_analysis(\n",
    "                template_file=template_file,\n",
    "                schema_file=schema_file,\n",
    "                data_file=file_path,\n",
    "                model='gpt-4o-mini',\n",
    "                enable_tools=['code-interpreter'],\n",
    "                output_file=str(output_file,\n",
    "        enable_downloads=True  # Enable chart downloads for visualization\n",
    "    )\n",
    "            )\n",
    "            \n",
    "            batch_results[file_path] = {\n",
    "                'status': 'success',\n",
    "                'results': results,\n",
    "                'output_file': str(output_file)\n",
    "            }\n",
    "            \n",
    "            print(f\"  ‚úÖ Success: {output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error: {e}\")\n",
    "            batch_results[file_path] = {\n",
    "                'status': 'error',\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "# Create multiple sample datasets\n",
    "# Ensure df exists from previous examples\n",
    "if \"df\" not in globals():\n",
    "    sample_data = {\n",
    "        \"date\": [\"2024-01-01\", \"2024-01-02\", \"2024-01-03\", \"2024-01-04\", \"2024-01-05\"],\n",
    "        \"product\": [\"Widget A\", \"Widget B\", \"Widget A\", \"Widget C\", \"Widget B\"],\n",
    "        \"quantity\": [10, 15, 8, 12, 20],\n",
    "        \"price\": [25.50, 30.00, 25.50, 45.00, 30.00]\n",
    "    }\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    df[\"revenue\"] = df[\"quantity\"] * df[\"price\"]\n",
    "\n",
    "datasets = []\n",
    "for month in ['Jan', 'Feb', 'Mar']:\n",
    "    monthly_data = df.copy()\n",
    "    monthly_data['month'] = month\n",
    "    monthly_data['quantity'] = monthly_data['quantity'] * (1 + 0.1 * len(datasets))  # Simulate growth\n",
    "    \n",
    "    filename = f'sales_{month.lower()}.csv'\n",
    "    monthly_data.to_csv(filename, index=False)\n",
    "    datasets.append(filename)\n",
    "\n",
    "print(f\"‚úÖ Created {len(datasets)} datasets for batch processing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batch analysis\n",
    "batch_results = batch_analysis(\n",
    "    file_list=datasets,\n",
    "    template_file='basic_template.j2',\n",
    "    schema_file='basic_schema.json'\n",
    ")\n",
    "\n",
    "# Summary of batch results\n",
    "successful = sum(1 for r in batch_results.values() if r['status'] == 'success')\n",
    "print(f\"\\nüìà Batch Analysis Complete: {successful}/{len(datasets)} successful\")\n",
    "\n",
    "# Display summary of results\n",
    "for file_path, result in batch_results.items():\n",
    "    if result['status'] == 'success':\n",
    "        summary = result['results'].get('summary', {})\n",
    "        total_sales = summary.get('total_sales', 0)\n",
    "        print(f\"  {Path(file_path).stem}: ${total_sales:,.2f} total sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Real-time Analysis Dashboard\n",
    "\n",
    "Create a simple dashboard that updates with new analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "def create_analysis_dashboard(data_files, refresh_interval=30):\n",
    "    \"\"\"\n",
    "    Create a simple analysis dashboard that refreshes periodically.\n",
    "    \"\"\"\n",
    "    def update_dashboard():\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print(\"üìä OSTRUCT ANALYSIS DASHBOARD\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Last Updated: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print()\n",
    "        \n",
    "        total_revenue = 0\n",
    "        total_transactions = 0\n",
    "        \n",
    "        for file_path in data_files:\n",
    "            try:\n",
    "                # Quick analysis for dashboard\n",
    "                results = run_ostruct_analysis(\n",
    "                    template_file='basic_template.j2',\n",
    "                    schema_file='basic_schema.json',\n",
    "                    data_file=file_path,\n",
    "                    model='gpt-4o-mini',\n",
    "                    enable_tools=['code-interpreter']\n",
    "                ,\n",
    "        enable_downloads=True  # Enable chart downloads for visualization\n",
    "    )\n",
    "                \n",
    "                summary = results.get('summary', {})\n",
    "                revenue = summary.get('total_sales', 0)\n",
    "                transactions = summary.get('total_transactions', 0)\n",
    "                \n",
    "                total_revenue += revenue\n",
    "                total_transactions += transactions\n",
    "                \n",
    "                print(f\"üìà {Path(file_path).stem.upper()}:\")\n",
    "                print(f\"   Revenue: ${revenue:,.2f}\")\n",
    "                print(f\"   Transactions: {transactions:,}\")\n",
    "                print()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error analyzing {file_path}: {e}\")\n",
    "        \n",
    "        print(\"üéØ TOTALS:\")\n",
    "        print(f\"   Total Revenue: ${total_revenue:,.2f}\")\n",
    "        print(f\"   Total Transactions: {total_transactions:,}\")\n",
    "        print(f\"   Average per Transaction: ${total_revenue/total_transactions if total_transactions > 0 else 0:.2f}\")\n",
    "        \n",
    "        print(f\"\\n‚è∞ Next refresh in {refresh_interval} seconds...\")\n",
    "    \n",
    "    # Run initial update\n",
    "    update_dashboard()\n",
    "    \n",
    "    return update_dashboard\n",
    "\n",
    "# Create dashboard (run once for demo)\n",
    "dashboard = create_analysis_dashboard(datasets[:2])  # Use first 2 datasets for demo\n",
    "print(\"‚úÖ Dashboard created (static version for demo)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices and Tips\n",
    "\n",
    "Here are some best practices for using ostruct in Jupyter notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practices Demo\n",
    "\n",
    "def data_science_best_practices():\n",
    "    \"\"\"\n",
    "    Demonstrate best practices for ostruct in data science workflows.\n",
    "    Following Google Cloud Jupyter Notebook Manifesto and industry standards.\n",
    "    \"\"\"\n",
    "    print(\"üéØ OSTRUCT DATA SCIENCE BEST PRACTICES\")\n",
    "    print(\"Following Google Cloud Jupyter Notebook Manifesto\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    practices = [\n",
    "        {\n",
    "            \"category\": \"üìã Reproducible Notebooks (Google Cloud Manifesto #3)\",\n",
    "            \"tips\": [\n",
    "                \"‚úÖ Environment info tracked automatically in setup cell\",\n",
    "                \"‚úÖ Random seeds set for reproducible results (np.random.seed(42))\",\n",
    "                \"‚úÖ Requirements.txt provided for consistent dependencies\",\n",
    "                \"‚úÖ Parameters cell enables Papermill execution\",\n",
    "                \"‚úÖ All experiments logged with metadata and timing\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"üìä Experiment Logging (Google Cloud Manifesto #7)\",\n",
    "            \"tips\": [\n",
    "                \"‚úÖ ExperimentTracker logs all analysis metadata automatically\",\n",
    "                \"‚úÖ Parameters, results, and execution info captured\",\n",
    "                \"‚úÖ Unique experiment IDs for tracking and comparison\",\n",
    "                \"‚úÖ JSON logs saved to experiment_logs/ directory\",\n",
    "                \"‚úÖ Execution time and success/failure tracked\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"‚öôÔ∏è Parameterized Execution (Google Cloud Manifesto #5)\",\n",
    "            \"tips\": [\n",
    "                \"‚úÖ Parameters cell tagged for Papermill compatibility\",\n",
    "                \"‚úÖ DEFAULT_MODEL, ANALYSIS_TIMEOUT configurable\",\n",
    "                \"‚úÖ Tool selection and output paths parameterized\",\n",
    "                \"‚úÖ Functions use parameter defaults automatically\",\n",
    "                \"‚úÖ No hardcoded timeouts - all use ANALYSIS_TIMEOUT parameter\",\n",
    "                \"‚úÖ Easy to run with different configurations\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"üîß Performance Optimization\",\n",
    "            \"tips\": [\n",
    "                \"Use gpt-4o-mini for exploratory analysis, gpt-4o for complex insights\",\n",
    "                \"Cache results using --output-file to avoid re-running expensive analyses\",\n",
    "                \"Sample large datasets for development, full data for production\",\n",
    "                \"Use --dry-run for template validation before API calls\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"üí∞ Cost Management\",\n",
    "            \"tips\": [\n",
    "                \"Start with cheaper models and upgrade only when needed\",\n",
    "                \"Use batch processing to reduce per-request overhead\",\n",
    "                \"Monitor token usage with verbose output\",\n",
    "                \"Reuse schemas across similar analyses\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"üõ°Ô∏è Reliability & Security\",\n",
    "            \"tips\": [\n",
    "                \"Always validate schemas before production use\",\n",
    "                \"Handle API errors gracefully with try/catch blocks\",\n",
    "                \"Don't commit API keys to notebooks\",\n",
    "                \"Use environment variables for configuration\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"üìä Analysis Quality\",\n",
    "            \"tips\": [\n",
    "                \"Design schemas that capture business value, not just technical metrics\",\n",
    "                \"Include confidence levels and caveats in your schemas\",\n",
    "                \"Combine AI insights with traditional statistical validation\",\n",
    "                \"Document assumptions and limitations in templates\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for practice in practices:\n",
    "        print(f\"\\n{practice['category']}\")\n",
    "        for tip in practice['tips']:\n",
    "            print(f\"  ‚úì {tip}\")\n",
    "    \n",
    "    print(\"\\nüöÄ Ready to build amazing data science workflows with ostruct!\")\n",
    "\n",
    "data_science_best_practices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Advanced Workflows from Data Science Guide\n",
    "\n",
    "Let's implement the complete workflows from the Data Science Integration Guide, including Financial Analysis, Research Synthesis, Business Intelligence, and Market Research examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Financial Analysis Workflow Example\n",
    "\n",
    "def create_financial_analysis_example():\n",
    "    \"\"\"Create complete financial analysis workflow from integration guide.\"\"\"\n",
    "    \n",
    "    # Create sample financial data\n",
    "    financial_data = {\n",
    "        'date': pd.date_range('2024-01-01', periods=12, freq='M'),\n",
    "        'revenue': [1500000, 1620000, 1580000, 1750000, 1690000, 1820000,\n",
    "                   1950000, 1880000, 2100000, 2050000, 2200000, 2350000],\n",
    "        'expenses': [1200000, 1250000, 1180000, 1300000, 1220000, 1350000,\n",
    "                    1400000, 1380000, 1450000, 1420000, 1500000, 1550000],\n",
    "        'market_segment': ['Consumer'] * 6 + ['Enterprise'] * 6\n",
    "    }\n",
    "    \n",
    "    df_financial = pd.DataFrame(financial_data)\n",
    "    df_financial['net_income'] = df_financial['revenue'] - df_financial['expenses']\n",
    "    df_financial['profit_margin'] = (df_financial['net_income'] / df_financial['revenue']) * 100\n",
    "    \n",
    "    # Save financial data\n",
    "    df_financial.to_csv('quarterly_financial_data.csv', index=False)\n",
    "    \n",
    "    print(\"üìä Financial Data Created:\")\n",
    "    display(df_financial.head())\n",
    "    \n",
    "    # Create financial analysis template (from integration guide)\n",
    "    financial_template = \"\"\"\n",
    "You are a senior financial analyst. Perform comprehensive analysis of the provided financial data.\n",
    "\n",
    "## Financial Analysis for Company - 2024\n",
    "\n",
    "### Market Data Analysis\n",
    "Analyze the following financial data and provide comprehensive insights:\n",
    "\n",
    "**Raw Data:**\n",
    "{{ quarterly_data.content }}\n",
    "\n",
    "### Analysis Requirements:\n",
    "1. **Performance Metrics**: Calculate key ratios (ROE, EBITDA margin, profit margins)\n",
    "2. **Trend Analysis**: Compare performance across time periods\n",
    "3. **Market Position**: Analyze segment performance \n",
    "4. **Risk Assessment**: Identify potential financial risks\n",
    "5. **Growth Projection**: Forecast trends based on current data\n",
    "\n",
    "### Regulatory Compliance Check:\n",
    "Review all metrics and flag any concerning trends for stakeholder reporting.\n",
    "\n",
    "Create professional visualization showing key financial trends.\n",
    "\"\"\"\n",
    "    \n",
    "    with open('financial_analysis_template.j2', 'w') as f:\n",
    "        f.write(financial_template)\n",
    "    \n",
    "    # Financial analysis schema (from integration guide)\n",
    "    financial_schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"executive_summary\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"2-3 sentence summary of financial health\"\n",
    "            },\n",
    "            \"key_metrics\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"total_revenue\": {\"type\": \"number\"},\n",
    "                    \"net_income\": {\"type\": \"number\"},\n",
    "                    \"average_profit_margin\": {\"type\": \"number\"},\n",
    "                    \"revenue_growth_rate\": {\"type\": \"number\"}\n",
    "                },\n",
    "                \"required\": [\"total_revenue\", \"net_income\", \"average_profit_margin\"]\n",
    "            },\n",
    "            \"trend_analysis\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"revenue_trend\": {\"type\": \"string\"},\n",
    "                    \"profit_margin_trend\": {\"type\": \"string\"},\n",
    "                    \"quarter_over_quarter_change\": {\"type\": \"number\"}\n",
    "                }\n",
    "            },\n",
    "            \"risk_factors\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"risk_type\": {\"type\": \"string\"},\n",
    "                        \"severity\": {\"type\": \"string\", \"enum\": [\"low\", \"medium\", \"high\", \"critical\"]},\n",
    "                        \"description\": {\"type\": \"string\"},\n",
    "                        \"mitigation_suggestions\": {\"type\": \"string\"}\n",
    "                    },\n",
    "                    \"required\": [\"risk_type\", \"severity\", \"description\"]\n",
    "                }\n",
    "            },\n",
    "            \"growth_forecast\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"next_quarter_revenue_estimate\": {\"type\": \"number\"},\n",
    "                    \"confidence_level\": {\"type\": \"string\", \"enum\": [\"low\", \"medium\", \"high\"]},\n",
    "                    \"key_assumptions\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"executive_summary\", \"key_metrics\", \"risk_factors\"]\n",
    "    }\n",
    "    \n",
    "    with open('financial_analysis_schema.json', 'w') as f:\n",
    "        json.dump(financial_schema, f, indent=2)\n",
    "    \n",
    "    print(\"‚úÖ Financial analysis template and schema created\")\n",
    "    \n",
    "    # Run financial analysis\n",
    "    financial_results = run_ostruct_analysis(\n",
    "        template_file='financial_analysis_template.j2',\n",
    "        schema_file='financial_analysis_schema.json',\n",
    "        data_file='quarterly_financial_data.csv',\n",
    "        model='gpt-4o',\n",
    "        enable_tools=['code-interpreter', 'web-search'],\n",
    "        output_file='financial_analysis_results.json'\n",
    "    ,\n",
    "        enable_downloads=True  # Enable chart downloads for visualization\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Financial Analysis Complete!\")\n",
    "    \n",
    "    # Display key results\n",
    "    print(\"\\nüíº FINANCIAL ANALYSIS SUMMARY:\")\n",
    "    print(f\"üìä Executive Summary: {financial_results['executive_summary']}\")\n",
    "    \n",
    "    if 'key_metrics' in financial_results:\n",
    "        metrics = financial_results['key_metrics']\n",
    "        print(f\"üí∞ Total Revenue: ${metrics.get('total_revenue', 0):,.2f}\")\n",
    "        print(f\"üí∏ Net Income: ${metrics.get('net_income', 0):,.2f}\")\n",
    "        print(f\"üìà Avg Profit Margin: {metrics.get('average_profit_margin', 0):.1f}%\")\n",
    "    \n",
    "    if 'risk_factors' in financial_results:\n",
    "        print(\"\\n‚ö†Ô∏è Risk Factors:\")\n",
    "        for risk in financial_results['risk_factors'][:3]:  # Show top 3\n",
    "            severity = risk.get('severity', 'unknown').upper()\n",
    "            print(f\"  ‚Ä¢ [{severity}] {risk.get('risk_type', 'Unknown')}: {risk.get('description', 'N/A')}\")\n",
    "    \n",
    "    return financial_results\n",
    "\n",
    "# Run financial analysis example\n",
    "financial_results = create_financial_analysis_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Intelligence Report Generation Example\n",
    "\n",
    "def create_business_intelligence_example():\n",
    "    \"\"\"Create Business Intelligence workflow from integration guide.\"\"\"\n",
    "    \n",
    "    # Create sample business data\n",
    "    business_data = {\n",
    "        'date': pd.date_range('2024-01-01', periods=100, freq='D'),\n",
    "        'customer_segment': np.random.choice(['Enterprise', 'SMB', 'Consumer'], 100),\n",
    "        'product_category': np.random.choice(['Software', 'Hardware', 'Services'], 100),\n",
    "        'revenue': np.random.normal(50000, 15000, 100),\n",
    "        'customer_satisfaction': np.random.normal(4.2, 0.8, 100),\n",
    "        'market_share': np.random.normal(0.15, 0.05, 100)\n",
    "    }\n",
    "    \n",
    "    df_business = pd.DataFrame(business_data)\n",
    "    df_business['revenue'] = np.maximum(df_business['revenue'], 1000)  # Ensure positive revenue\n",
    "    df_business['customer_satisfaction'] = np.clip(df_business['customer_satisfaction'], 1, 5)\n",
    "    df_business['market_share'] = np.clip(df_business['market_share'], 0.01, 0.5)\n",
    "    \n",
    "    # Save business data\n",
    "    df_business.to_csv('business_intelligence_data.csv', index=False)\n",
    "    \n",
    "    print(\"üìä Business Intelligence Data Created:\")\n",
    "    display(df_business.head())\n",
    "    \n",
    "    # Create BI analysis template (from integration guide)\n",
    "    bi_template = \"\"\"\n",
    "You are a senior business analyst. Perform comprehensive competitive analysis and business intelligence.\n",
    "\n",
    "## Business Intelligence Report - Q4 2024\n",
    "\n",
    "### Internal Performance Analysis\n",
    "**Sales Data:**\n",
    "{{ sales_data.content }}\n",
    "\n",
    "### Analysis Requirements:\n",
    "1. **Market Position**: Analyze our position vs competitors across key metrics\n",
    "2. **Growth Opportunities**: Identify untapped segments and expansion possibilities  \n",
    "3. **Competitive Threats**: Assess emerging competitors and market disruptions\n",
    "4. **Pricing Analysis**: Evaluate price positioning and optimization opportunities\n",
    "5. **Strategic Recommendations**: Provide actionable next steps with ROI projections\n",
    "\n",
    "### Executive Briefing Elements:\n",
    "- Top 3 strategic priorities\n",
    "- Revenue impact projections\n",
    "- Resource requirements\n",
    "- Timeline for implementation\n",
    "\n",
    "Create professional visualizations showing competitive positioning and market trends.\n",
    "\"\"\"\n",
    "    \n",
    "    with open('bi_analysis_template.j2', 'w') as f:\n",
    "        f.write(bi_template)\n",
    "    \n",
    "    # BI analysis schema (from integration guide)\n",
    "    bi_schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"executive_summary\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"CEO-ready 2-3 sentence summary of strategic position\"\n",
    "            },\n",
    "            \"market_position\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"market_share\": {\"type\": \"number\"},\n",
    "                    \"competitive_ranking\": {\"type\": \"integer\"},\n",
    "                    \"differentiation_strengths\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                    \"competitive_gaps\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "                }\n",
    "            },\n",
    "            \"growth_opportunities\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"opportunity\": {\"type\": \"string\"},\n",
    "                        \"market_size\": {\"type\": \"number\"},\n",
    "                        \"revenue_potential\": {\"type\": \"number\"},\n",
    "                        \"time_to_market\": {\"type\": \"string\"},\n",
    "                        \"investment_required\": {\"type\": \"number\"},\n",
    "                        \"risk_level\": {\"type\": \"string\", \"enum\": [\"low\", \"medium\", \"high\"]}\n",
    "                    },\n",
    "                    \"required\": [\"opportunity\", \"revenue_potential\", \"risk_level\"]\n",
    "                }\n",
    "            },\n",
    "            \"strategic_recommendations\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"recommendation\": {\"type\": \"string\"},\n",
    "                        \"priority\": {\"type\": \"string\", \"enum\": [\"critical\", \"high\", \"medium\", \"low\"]},\n",
    "                        \"expected_roi\": {\"type\": \"number\"},\n",
    "                        \"implementation_timeline\": {\"type\": \"string\"},\n",
    "                        \"resource_requirements\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                        \"success_metrics\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "                    },\n",
    "                    \"required\": [\"recommendation\", \"priority\", \"expected_roi\"]\n",
    "                }\n",
    "            },\n",
    "            \"competitive_analysis\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"top_competitors\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                    \"competitive_advantages\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                    \"market_threats\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"executive_summary\", \"market_position\", \"growth_opportunities\", \"strategic_recommendations\"]\n",
    "    }\n",
    "    \n",
    "    with open('bi_analysis_schema.json', 'w') as f:\n",
    "        json.dump(bi_schema, f, indent=2)\n",
    "    \n",
    "    print(\"‚úÖ Business Intelligence template and schema created\")\n",
    "    \n",
    "    # Run BI analysis\n",
    "    bi_results = run_ostruct_analysis(\n",
    "        template_file='bi_analysis_template.j2',\n",
    "        schema_file='bi_analysis_schema.json',\n",
    "        data_file='business_intelligence_data.csv',\n",
    "        model='gpt-4o',\n",
    "        enable_tools=['code-interpreter', 'web-search'],\n",
    "        output_file='bi_analysis_results.json'\n",
    "    ,\n",
    "        enable_downloads=True  # Enable chart downloads for visualization\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Business Intelligence Analysis Complete!\")\n",
    "    \n",
    "    # Display key results\n",
    "    print(\"\\nüè¢ BUSINESS INTELLIGENCE SUMMARY:\")\n",
    "    print(f\"üìä Executive Summary: {bi_results['executive_summary']}\")\n",
    "    \n",
    "    if 'market_position' in bi_results:\n",
    "        position = bi_results['market_position']\n",
    "        print(f\"üìà Market Share: {position.get('market_share', 0):.1%}\")\n",
    "        print(f\"üèÜ Competitive Ranking: #{position.get('competitive_ranking', 'N/A')}\")\n",
    "    \n",
    "    if 'strategic_recommendations' in bi_results:\n",
    "        print(\"\\nüí° TOP STRATEGIC RECOMMENDATIONS:\")\n",
    "        for i, rec in enumerate(bi_results['strategic_recommendations'][:3], 1):\n",
    "            priority = rec.get('priority', 'medium').upper()\n",
    "            recommendation = rec.get('recommendation', 'N/A')\n",
    "            roi = rec.get('expected_roi', 0)\n",
    "            print(f\"  {i}. [{priority}] {recommendation}\")\n",
    "            print(f\"     Expected ROI: {roi:.1%}\")\n",
    "    \n",
    "    return bi_results\n",
    "\n",
    "# Run business intelligence example\n",
    "bi_results = create_business_intelligence_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Clean up temporary files created during this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temporary files\n",
    "import glob\n",
    "\n",
    "temp_files = [\n",
    "    '*.csv', '*.json', '*.j2', 'downloads/*', 'batch_results/*'\n",
    "]\n",
    "\n",
    "for pattern in temp_files:\n",
    "    for file in glob.glob(pattern):\n",
    "        try:\n",
    "            Path(file).unlink()\n",
    "            print(f\"üóëÔ∏è Removed: {file}\")\n",
    "        except:\n",
    "            pass  # Ignore errors for directories or non-existent files\n",
    "\n",
    "# Remove directories\n",
    "for dir_name in ['downloads', 'batch_results']:\n",
    "    try:\n",
    "        import shutil\n",
    "        shutil.rmtree(dir_name)\n",
    "        print(f\"üóëÔ∏è Removed directory: {dir_name}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "üéâ **Congratulations!** You've learned how to integrate ostruct with Jupyter notebooks for powerful data science workflows.\n",
    "\n",
    "### What to try next:\n",
    "\n",
    "1. **üîÑ Adapt for your data**: Replace the sample data with your own datasets\n",
    "2. **üé® Custom templates**: Create domain-specific templates for your analysis needs\n",
    "3. **üìä Advanced schemas**: Design schemas that capture your business metrics\n",
    "4. **üöÄ Production deployment**: Build automated pipelines using these patterns\n",
    "5. **üîó Tool integration**: Combine with other data science tools in your stack\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- [ostruct Documentation](https://ostruct.readthedocs.io/)\n",
    "- [Data Science Integration Guide](https://ostruct.readthedocs.io/en/latest/user-guide/data_science_integration.html)\n",
    "- [More Examples](../)\n",
    "- [GitHub Repository](https://github.com/yaniv-golan/ostruct)\n",
    "\n",
    "Happy analyzing! üöÄüìä"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
