{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis with ostruct in Jupyter\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yaniv-golan/ostruct/blob/main/examples/data-science/notebooks/ostruct_data_analysis.ipynb)\n",
    "\n",
    "This notebook demonstrates how to use ostruct for data analysis within Jupyter notebooks, combining the power of AI-driven analysis with interactive data science workflows.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- üìä Run ostruct analysis from Jupyter cells\n",
    "- üîÑ Integrate ostruct results with pandas workflows\n",
    "- üìà Generate visualizations using AI + Code Interpreter\n",
    "- üöÄ Build automated analysis pipelines\n",
    "- üí° Best practices for production data science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install ostruct and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install ostruct (run this once)\n# NOTE: Using release candidate v1.6.0rc1 to test Code Interpreter file download fix\n# TODO: Revert to stable version after testing: !pip install ostruct-cli\n!pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ ostruct-cli==1.6.0rc1\n\n# Import required libraries\nimport json\nimport subprocess\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom IPython.display import Image, display, HTML\n\n# Set up plotting style\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n\nprint(\"‚úÖ Setup complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI API key (required)\n",
    "import os\n",
    "\n",
    "# Option 1: Set via environment variable\n",
    "# os.environ['OPENAI_API_KEY'] = 'your-api-key-here'\n",
    "\n",
    "# Option 2: Use getpass for secure input\n",
    "import getpass\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    os.environ['OPENAI_API_KEY'] = getpass.getpass('Enter your OpenAI API key: ')\n",
    "\n",
    "print(\"üîë API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core ostruct Integration Functions\n",
    "\n",
    "Let's create helper functions for running ostruct from Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ostruct_analysis(template_file, schema_file, data_file=None, model='gpt-4o-mini', \n",
    "                        enable_tools=None, web_query=None, output_file=None):\n",
    "    \"\"\"\n",
    "    Run ostruct analysis from Jupyter and return results.\n",
    "    \n",
    "    Args:\n",
    "        template_file: Path to Jinja2 template\n",
    "        schema_file: Path to JSON schema\n",
    "        data_file: Optional data file path\n",
    "        model: OpenAI model to use\n",
    "        enable_tools: List of tools to enable ['code-interpreter', 'web-search', 'file-search']\n",
    "        web_query: Web search query if web-search enabled\n",
    "        output_file: Optional output file path\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    cmd = ['ostruct', 'run', template_file, schema_file, '--model', model]\n",
    "    \n",
    "    # Add data file if provided\n",
    "    if data_file:\n",
    "        cmd.extend(['--file', f'ci:data', data_file])\n",
    "    \n",
    "    # Enable tools\n",
    "    if enable_tools:\n",
    "        for tool in enable_tools:\n",
    "            cmd.extend(['--enable-tool', tool])\n",
    "    \n",
    "    # Add web query\n",
    "    if web_query:\n",
    "        cmd.extend(['--web-query', web_query])\n",
    "    \n",
    "    # Add output file\n",
    "    if output_file:\n",
    "        cmd.extend(['--output-file', output_file])\n",
    "    \n",
    "    print(f\"üöÄ Running: {' '.join(cmd)}\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "        \n",
    "        if output_file:\n",
    "            # Read from output file\n",
    "            with open(output_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        else:\n",
    "            # Parse stdout\n",
    "            return json.loads(result.stdout)\n",
    "            \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        print(f\"Stderr: {e.stderr}\")\n",
    "        raise\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå JSON parsing error: {e}\")\n",
    "        print(f\"Raw output: {result.stdout}\")\n",
    "        raise\n",
    "\n",
    "def display_analysis_summary(results):\n",
    "    \"\"\"\n",
    "    Display a formatted summary of analysis results.\n",
    "    \"\"\"\n",
    "    if 'summary' in results:\n",
    "        summary = results['summary']\n",
    "        print(\"üìä ANALYSIS SUMMARY\")\n",
    "        print(\"=\" * 40)\n",
    "        for key, value in summary.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                if 'sales' in key.lower() or 'revenue' in key.lower():\n",
    "                    print(f\"{key.replace('_', ' ').title():<20} ${value:,.2f}\")\n",
    "                else:\n",
    "                    print(f\"{key.replace('_', ' ').title():<20} {value:,}\")\n",
    "            else:\n",
    "                print(f\"{key.replace('_', ' ').title():<20} {value}\")\n",
    "        print()\n",
    "    \n",
    "    # Display chart info if available\n",
    "    if 'chart_info' in results:\n",
    "        chart = results['chart_info']\n",
    "        print(f\"üìà Generated Chart: {chart.get('filename', 'N/A')}\")\n",
    "        print(f\"   Description: {chart.get('description', 'N/A')}\")\n",
    "        \n",
    "        # Try to display the chart if it exists\n",
    "        chart_path = Path('downloads') / chart.get('filename', '')\n",
    "        if chart_path.exists():\n",
    "            display(Image(str(chart_path)))\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Data Analysis\n",
    "\n",
    "Let's start with a simple CSV analysis using the data science template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create sample data for analysis\nsample_data = {\n    'date': ['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05'],\n    'product': ['Widget A', 'Widget B', 'Widget A', 'Widget C', 'Widget B'],\n    'quantity': [10, 15, 8, 12, 20],\n    'price': [25.50, 30.00, 25.50, 45.00, 30.00]\n}\n\ndf = pd.DataFrame(sample_data)\ndf['revenue'] = df['quantity'] * df['price']\n\n# Save to CSV for ostruct analysis\ndf.to_csv('sample_sales.csv', index=False)\n\nprint(\"üìã Sample Data Created:\")\ndisplay(df)\n\n# Create basic template and schema for this demo\n# (In practice, you'd have these files prepared)\n\nbasic_template = \"\"\"\nYou are a data analyst. Analyze the provided sales data and generate insights.\n\n## Sales Data Analysis\n\n**Data to analyze:**\n{{ data.content }}\n\n## Analysis Requirements:\n1. **Summary Statistics**: Calculate total sales, average price, transaction count\n2. **Product Performance**: Identify top-performing products\n3. **Trends**: Analyze sales patterns and trends\n4. **Data Quality**: Assess data completeness and note any issues\n5. **Visualization**: Create a chart showing sales by product\n\n## Output Format:\nProvide analysis in the structured JSON format specified in the schema.\n\"\"\"\n\nwith open('basic_template.j2', 'w') as f:\n    f.write(basic_template)\n\nbasic_schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"summary\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"total_sales\": {\"type\": \"number\"},\n                \"average_price\": {\"type\": \"number\"},\n                \"product_count\": {\"type\": \"integer\"},\n                \"total_transactions\": {\"type\": \"integer\"}\n            }\n        },\n        \"sales_by_product\": {\n            \"type\": \"object\",\n            \"description\": \"Sales totals by product name\"\n        },\n        \"chart_info\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"filename\": {\"type\": \"string\"},\n                \"description\": {\"type\": \"string\"},\n                \"chart_type\": {\"type\": \"string\"}\n            }\n        },\n        \"data_quality\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"rows_processed\": {\"type\": \"integer\"},\n                \"missing_values\": {\"type\": \"integer\"},\n                \"data_issues\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n            }\n        }\n    },\n    \"required\": [\"summary\", \"chart_info\", \"data_quality\"]\n}\n\nwith open('basic_schema.json', 'w') as f:\n    json.dump(basic_schema, f, indent=2)\n\nprint(\"‚úÖ Template and schema files created for demo\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run ostruct analysis with better error handling and debugging\nprint(\"üöÄ Starting ostruct analysis...\")\n\ntry:\n    # First, let's verify our files exist\n    print(\"üìã Checking required files:\")\n    required_files = ['basic_template.j2', 'basic_schema.json', 'sample_sales.csv']\n    for file in required_files:\n        if Path(file).exists():\n            print(f\"  ‚úÖ {file} ({Path(file).stat().st_size} bytes)\")\n        else:\n            print(f\"  ‚ùå {file} - MISSING!\")\n            \n    print(\"\\nüîë Checking API key...\")\n    if 'OPENAI_API_KEY' in os.environ:\n        key = os.environ['OPENAI_API_KEY']\n        print(f\"  ‚úÖ API key set ({key[:10]}...{key[-4:]})\")\n    else:\n        print(\"  ‚ùå OPENAI_API_KEY not found!\")\n        \n    print(\"\\n‚è∞ Running analysis (this may take 30-60 seconds)...\")\n    \n    # Run with timeout and verbose output\n    import subprocess\n    import signal\n    \n    cmd = [\n        'ostruct', 'run', \n        'basic_template.j2', \n        'basic_schema.json',\n        '--file', 'ci:data', 'sample_sales.csv',\n        '--model', 'gpt-4o-mini',\n        '--enable-tool', 'code-interpreter',\n        '--output-file', 'analysis_results.json',\n        '--verbose'  # Add verbose output\n    ]\n    \n    print(f\"üîß Command: {' '.join(cmd)}\")\n    \n    # Run with timeout\n    try:\n        result = subprocess.run(\n            cmd, \n            capture_output=True, \n            text=True, \n            timeout=120,  # 2 minute timeout\n            check=True\n        )\n        \n        print(\"‚úÖ Command completed successfully!\")\n        print(f\"üì§ Return code: {result.returncode}\")\n        \n        if result.stdout:\n            print(\"üìÑ Output:\")\n            print(result.stdout[:1000])  # First 1000 chars\n            \n        # Load results\n        if Path('analysis_results.json').exists():\n            with open('analysis_results.json', 'r') as f:\n                results = json.load(f)\n            print(\"‚úÖ Results loaded successfully!\")\n            display_analysis_summary(results)\n        else:\n            print(\"‚ùå Output file not created\")\n            \n    except subprocess.TimeoutExpired:\n        print(\"‚è∞ Command timed out after 2 minutes\")\n        print(\"This might indicate:\")\n        print(\"  - API is slow to respond\")\n        print(\"  - Network connectivity issues\")\n        print(\"  - API key problems\")\n        \n    except subprocess.CalledProcessError as e:\n        print(f\"‚ùå Command failed with exit code {e.returncode}\")\n        print(f\"üìÑ Error output:\")\n        print(e.stderr)\n        \nexcept Exception as e:\n    print(f\"‚ùå Unexpected error: {e}\")\n    import traceback\n    traceback.print_exc()\n\nprint(\"\\nüìÅ Current directory contents:\")\nfor item in sorted(Path('.').iterdir()):\n    if item.is_file():\n        print(f\"  üìÑ {item.name} ({item.stat().st_size} bytes)\")\n    else:\n        print(f\"  üìÅ {item.name}/\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Load and display the analysis results\nprint(\"üìä Loading analysis results...\")\n\ntry:\n    # Load the results file that was created\n    with open('analysis_results.json', 'r') as f:\n        results = json.load(f)\n    \n    print(\"‚úÖ Results loaded successfully!\")\n    \n    # Display the full results first\n    print(\"\\nüìã COMPLETE ANALYSIS RESULTS:\")\n    print(json.dumps(results, indent=2))\n    \n    # Use our display function\n    display_analysis_summary(results)\n    \n    # Look for any generated chart files with the RC fix\n    print(\"\\nüîç Looking for generated charts (should work with v1.6.0rc1)...\")\n    \n    # Check for image files in current directory and downloads\n    image_extensions = ['.png', '.jpg', '.jpeg', '.svg', '.gif']\n    \n    # Search locations where charts might be saved\n    search_locations = [\n        Path('.'),  # Current directory\n        Path('downloads'),  # Default download location\n    ]\n    \n    all_found_images = []\n    \n    for location in search_locations:\n        if location.exists() and location.is_dir():\n            print(f\"\\nüìÅ Checking {location}:\")\n            try:\n                items = list(location.iterdir())\n                image_files = [f for f in items if f.suffix.lower() in image_extensions]\n                \n                if image_files:\n                    print(f\"  üéØ Found {len(image_files)} image(s):\")\n                    for img in image_files:\n                        print(f\"    üìä {img.name} ({img.stat().st_size} bytes)\")\n                        all_found_images.append(img)\n                else:\n                    all_files = [f for f in items if f.is_file()]\n                    print(f\"  üìÑ {len(all_files)} files, no images\")\n                        \n            except Exception as e:\n                print(f\"  ‚ùå Error: {e}\")\n        else:\n            print(f\"üìÅ {location}: doesn't exist\")\n    \n    # Display all found images\n    if all_found_images:\n        print(f\"\\nüé® Displaying {len(all_found_images)} chart(s):\")\n        for img in all_found_images:\n            print(f\"\\nüìä Chart: {img}\")\n            try:\n                display(Image(str(img)))\n                print(f\"‚úÖ Successfully displayed {img.name}\")\n            except Exception as e:\n                print(f\"‚ùå Could not display {img}: {e}\")\n    else:\n        print(\"\\nüìà Creating backup chart with matplotlib...\")\n        \n        # Create a fallback chart from the data\n        import pandas as pd\n        import matplotlib.pyplot as plt\n        \n        df = pd.read_csv('sample_sales.csv')\n        \n        # Create sales by product chart\n        sales_by_product = df.groupby('product')['revenue'].sum().sort_values(ascending=False)\n        \n        plt.figure(figsize=(10, 6))\n        bars = plt.bar(sales_by_product.index, sales_by_product.values, \n                      color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n        plt.title('Sales by Product', fontsize=16, fontweight='bold')\n        plt.xlabel('Product', fontsize=12)\n        plt.ylabel('Revenue ($)', fontsize=12)\n        plt.xticks(rotation=45)\n        \n        # Add value labels on bars\n        for bar, value in zip(bars, sales_by_product.values):\n            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n                    f'${value:.0f}', ha='center', va='bottom')\n        \n        plt.tight_layout()\n        plt.grid(axis='y', alpha=0.3)\n        plt.show()\n        \n        print(\"‚úÖ Backup chart displayed!\")\n\nexcept Exception as e:\n    print(f\"‚ùå Error: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more complex template for multi-tool analysis\n",
    "multi_tool_template = \"\"\"\n",
    "You are a senior data analyst. Perform comprehensive analysis combining internal data with external market research.\n",
    "\n",
    "# Multi-Source Analysis Task\n",
    "\n",
    "## Internal Data Analysis\n",
    "{{ data.content }}\n",
    "\n",
    "## Market Context (Web Research)\n",
    "{% if web_search_results %}\n",
    "{{ web_search_results }}\n",
    "{% else %}\n",
    "No web research data available. Focus on internal data analysis.\n",
    "{% endif %}\n",
    "\n",
    "## Analysis Requirements\n",
    "1. **Internal Performance**: Analyze sales trends, top products, growth patterns\n",
    "2. **Market Context**: Compare with industry trends and competitor performance  \n",
    "3. **Strategic Insights**: Identify opportunities and risks\n",
    "4. **Recommendations**: Provide 3-5 actionable recommendations\n",
    "5. **Visualization**: Create a professional chart showing key insights\n",
    "\n",
    "## Output Format\n",
    "Provide comprehensive analysis in the specified JSON structure with business insights and actionable recommendations.\n",
    "\"\"\"\n",
    "\n",
    "# Save template\n",
    "with open('multi_tool_template.j2', 'w') as f:\n",
    "    f.write(multi_tool_template)\n",
    "\n",
    "print(\"‚úÖ Multi-tool template created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced schema for multi-tool analysis\n",
    "enhanced_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"internal_analysis\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"total_revenue\": {\"type\": \"number\"},\n",
    "                \"top_product\": {\"type\": \"string\"},\n",
    "                \"growth_trend\": {\"type\": \"string\"},\n",
    "                \"key_metrics\": {\"type\": \"object\"}\n",
    "            }\n",
    "        },\n",
    "        \"market_insights\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"industry_trends\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                \"competitive_position\": {\"type\": \"string\"},\n",
    "                \"market_opportunities\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "            }\n",
    "        },\n",
    "        \"recommendations\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"recommendation\": {\"type\": \"string\"},\n",
    "                    \"priority\": {\"type\": \"string\", \"enum\": [\"high\", \"medium\", \"low\"]},\n",
    "                    \"expected_impact\": {\"type\": \"string\"}\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"visualization\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"chart_type\": {\"type\": \"string\"},\n",
    "                \"filename\": {\"type\": \"string\"},\n",
    "                \"insights\": {\"type\": \"string\"}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"internal_analysis\", \"recommendations\"]\n",
    "}\n",
    "\n",
    "# Save schema\n",
    "with open('enhanced_schema.json', 'w') as f:\n",
    "    json.dump(enhanced_schema, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Enhanced schema created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multi-tool analysis\n",
    "enhanced_results = run_ostruct_analysis(\n",
    "    template_file='multi_tool_template.j2',\n",
    "    schema_file='enhanced_schema.json', \n",
    "    data_file='sample_sales.csv',\n",
    "    model='gpt-4o',\n",
    "    enable_tools=['code-interpreter', 'web-search'],\n",
    "    web_query='widget sales industry trends 2024 market analysis',\n",
    "    output_file='enhanced_results.json'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Enhanced analysis complete!\")\n",
    "print(json.dumps(enhanced_results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Interactive Data Science Workflow\n",
    "\n",
    "Let's create an interactive workflow that combines pandas analysis with AI insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_analysis_workflow(dataframe, analysis_question):\n",
    "    \"\"\"\n",
    "    Interactive workflow combining pandas analysis with AI insights.\n",
    "    \"\"\"\n",
    "    print(f\"üîç Analyzing: {analysis_question}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Basic pandas analysis\n",
    "    print(\"üìä Step 1: Basic Statistics\")\n",
    "    print(dataframe.describe())\n",
    "    print()\n",
    "    \n",
    "    # Step 2: Save data for AI analysis\n",
    "    temp_file = 'temp_analysis.csv'\n",
    "    dataframe.to_csv(temp_file, index=False)\n",
    "    \n",
    "    # Step 3: Create dynamic template based on question\n",
    "    dynamic_template = f\"\"\"\n",
    "You are a data scientist. Answer this specific question about the provided dataset:\n",
    "\n",
    "**Question**: {analysis_question}\n",
    "\n",
    "**Dataset**: Analyze the provided CSV data to answer the question.\n",
    "\n",
    "## Analysis Requirements:\n",
    "1. Load and examine the data thoroughly\n",
    "2. Perform relevant statistical analysis to answer the question\n",
    "3. Create appropriate visualizations\n",
    "4. Provide clear, data-driven insights\n",
    "5. Include confidence levels and any caveats\n",
    "\n",
    "Focus specifically on answering: \"{analysis_question}\"\n",
    "\n",
    "Provide your analysis in the structured format below.\n",
    "\"\"\"\n",
    "    \n",
    "    with open('dynamic_template.j2', 'w') as f:\n",
    "        f.write(dynamic_template)\n",
    "    \n",
    "    # Dynamic schema\n",
    "    dynamic_schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"question\": {\"type\": \"string\"},\n",
    "            \"answer\": {\"type\": \"string\"},\n",
    "            \"supporting_evidence\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "            \"confidence_level\": {\"type\": \"string\", \"enum\": [\"high\", \"medium\", \"low\"]},\n",
    "            \"key_insights\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "            \"chart_info\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"filename\": {\"type\": \"string\"},\n",
    "                    \"description\": {\"type\": \"string\"}\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"question\", \"answer\", \"confidence_level\"]\n",
    "    }\n",
    "    \n",
    "    with open('dynamic_schema.json', 'w') as f:\n",
    "        json.dump(dynamic_schema, f, indent=2)\n",
    "    \n",
    "    # Step 4: Run AI analysis\n",
    "    print(\"ü§ñ Step 2: AI Analysis\")\n",
    "    ai_results = run_ostruct_analysis(\n",
    "        template_file='dynamic_template.j2',\n",
    "        schema_file='dynamic_schema.json',\n",
    "        data_file=temp_file,\n",
    "        model='gpt-4o',\n",
    "        enable_tools=['code-interpreter']\n",
    "    )\n",
    "    \n",
    "    # Step 5: Display results\n",
    "    print(f\"\\nüéØ Answer: {ai_results['answer']}\")\n",
    "    print(f\"üîí Confidence: {ai_results['confidence_level']}\")\n",
    "    \n",
    "    if 'key_insights' in ai_results:\n",
    "        print(\"\\nüí° Key Insights:\")\n",
    "        for insight in ai_results['key_insights']:\n",
    "            print(f\"  ‚Ä¢ {insight}\")\n",
    "    \n",
    "    # Clean up\n",
    "    Path(temp_file).unlink()\n",
    "    \n",
    "    return ai_results\n",
    "\n",
    "print(\"‚úÖ Interactive workflow function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the interactive workflow\n",
    "question = \"Which product has the highest profit margin and what factors contribute to its success?\"\n",
    "\n",
    "workflow_results = interactive_analysis_workflow(df, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Batch Processing Multiple Datasets\n",
    "\n",
    "For production scenarios, you often need to analyze multiple datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_analysis(file_list, template_file, schema_file, output_dir='batch_results'):\n",
    "    \"\"\"\n",
    "    Analyze multiple datasets in batch using ostruct.\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    batch_results = {}\n",
    "    \n",
    "    for i, file_path in enumerate(file_list):\n",
    "        print(f\"\\nüìä Processing {i+1}/{len(file_list)}: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            output_file = Path(output_dir) / f\"{Path(file_path).stem}_analysis.json\"\n",
    "            \n",
    "            results = run_ostruct_analysis(\n",
    "                template_file=template_file,\n",
    "                schema_file=schema_file,\n",
    "                data_file=file_path,\n",
    "                model='gpt-4o-mini',\n",
    "                enable_tools=['code-interpreter'],\n",
    "                output_file=str(output_file)\n",
    "            )\n",
    "            \n",
    "            batch_results[file_path] = {\n",
    "                'status': 'success',\n",
    "                'results': results,\n",
    "                'output_file': str(output_file)\n",
    "            }\n",
    "            \n",
    "            print(f\"  ‚úÖ Success: {output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error: {e}\")\n",
    "            batch_results[file_path] = {\n",
    "                'status': 'error',\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "# Create multiple sample datasets\n",
    "datasets = []\n",
    "for month in ['Jan', 'Feb', 'Mar']:\n",
    "    monthly_data = df.copy()\n",
    "    monthly_data['month'] = month\n",
    "    monthly_data['quantity'] = monthly_data['quantity'] * (1 + 0.1 * len(datasets))  # Simulate growth\n",
    "    \n",
    "    filename = f'sales_{month.lower()}.csv'\n",
    "    monthly_data.to_csv(filename, index=False)\n",
    "    datasets.append(filename)\n",
    "\n",
    "print(f\"‚úÖ Created {len(datasets)} datasets for batch processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batch analysis\n",
    "batch_results = batch_analysis(\n",
    "    file_list=datasets,\n",
    "    template_file='../analysis/templates/main.j2',\n",
    "    schema_file='../analysis/schemas/main.json'\n",
    ")\n",
    "\n",
    "# Summary of batch results\n",
    "successful = sum(1 for r in batch_results.values() if r['status'] == 'success')\n",
    "print(f\"\\nüìà Batch Analysis Complete: {successful}/{len(datasets)} successful\")\n",
    "\n",
    "# Display summary of results\n",
    "for file_path, result in batch_results.items():\n",
    "    if result['status'] == 'success':\n",
    "        summary = result['results'].get('summary', {})\n",
    "        total_sales = summary.get('total_sales', 0)\n",
    "        print(f\"  {Path(file_path).stem}: ${total_sales:,.2f} total sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Real-time Analysis Dashboard\n",
    "\n",
    "Create a simple dashboard that updates with new analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "def create_analysis_dashboard(data_files, refresh_interval=30):\n",
    "    \"\"\"\n",
    "    Create a simple analysis dashboard that refreshes periodically.\n",
    "    \"\"\"\n",
    "    def update_dashboard():\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print(\"üìä OSTRUCT ANALYSIS DASHBOARD\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Last Updated: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print()\n",
    "        \n",
    "        total_revenue = 0\n",
    "        total_transactions = 0\n",
    "        \n",
    "        for file_path in data_files:\n",
    "            try:\n",
    "                # Quick analysis for dashboard\n",
    "                results = run_ostruct_analysis(\n",
    "                    template_file='../analysis/templates/main.j2',\n",
    "                    schema_file='../analysis/schemas/main.json',\n",
    "                    data_file=file_path,\n",
    "                    model='gpt-4o-mini',\n",
    "                    enable_tools=['code-interpreter']\n",
    "                )\n",
    "                \n",
    "                summary = results.get('summary', {})\n",
    "                revenue = summary.get('total_sales', 0)\n",
    "                transactions = summary.get('total_transactions', 0)\n",
    "                \n",
    "                total_revenue += revenue\n",
    "                total_transactions += transactions\n",
    "                \n",
    "                print(f\"üìà {Path(file_path).stem.upper()}:\")\n",
    "                print(f\"   Revenue: ${revenue:,.2f}\")\n",
    "                print(f\"   Transactions: {transactions:,}\")\n",
    "                print()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error analyzing {file_path}: {e}\")\n",
    "        \n",
    "        print(\"üéØ TOTALS:\")\n",
    "        print(f\"   Total Revenue: ${total_revenue:,.2f}\")\n",
    "        print(f\"   Total Transactions: {total_transactions:,}\")\n",
    "        print(f\"   Average per Transaction: ${total_revenue/total_transactions if total_transactions > 0 else 0:.2f}\")\n",
    "        \n",
    "        print(f\"\\n‚è∞ Next refresh in {refresh_interval} seconds...\")\n",
    "    \n",
    "    # Run initial update\n",
    "    update_dashboard()\n",
    "    \n",
    "    return update_dashboard\n",
    "\n",
    "# Create dashboard (run once for demo)\n",
    "dashboard = create_analysis_dashboard(datasets[:2])  # Use first 2 datasets for demo\n",
    "print(\"‚úÖ Dashboard created (static version for demo)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices and Tips\n",
    "\n",
    "Here are some best practices for using ostruct in Jupyter notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practices Demo\n",
    "\n",
    "def data_science_best_practices():\n",
    "    \"\"\"\n",
    "    Demonstrate best practices for ostruct in data science workflows.\n",
    "    \"\"\"\n",
    "    print(\"üéØ OSTRUCT DATA SCIENCE BEST PRACTICES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    practices = [\n",
    "        {\n",
    "            \"category\": \"üîß Performance Optimization\",\n",
    "            \"tips\": [\n",
    "                \"Use gpt-4o-mini for exploratory analysis, gpt-4o for complex insights\",\n",
    "                \"Cache results using --output-file to avoid re-running expensive analyses\",\n",
    "                \"Sample large datasets for development, full data for production\",\n",
    "                \"Use --dry-run for template validation before API calls\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"üí∞ Cost Management\",\n",
    "            \"tips\": [\n",
    "                \"Start with cheaper models and upgrade only when needed\",\n",
    "                \"Use batch processing to reduce per-request overhead\",\n",
    "                \"Monitor token usage with verbose output\",\n",
    "                \"Reuse schemas across similar analyses\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"üõ°Ô∏è Reliability & Security\",\n",
    "            \"tips\": [\n",
    "                \"Always validate schemas before production use\",\n",
    "                \"Handle API errors gracefully with try/catch blocks\",\n",
    "                \"Don't commit API keys to notebooks\",\n",
    "                \"Use environment variables for configuration\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"üìä Analysis Quality\",\n",
    "            \"tips\": [\n",
    "                \"Design schemas that capture business value, not just technical metrics\",\n",
    "                \"Include confidence levels and caveats in your schemas\",\n",
    "                \"Combine AI insights with traditional statistical validation\",\n",
    "                \"Document assumptions and limitations in templates\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for practice in practices:\n",
    "        print(f\"\\n{practice['category']}\")\n",
    "        for tip in practice['tips']:\n",
    "            print(f\"  ‚úì {tip}\")\n",
    "    \n",
    "    print(\"\\nüöÄ Ready to build amazing data science workflows with ostruct!\")\n",
    "\n",
    "data_science_best_practices()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Example 6: Advanced Workflows from Data Science Guide\n\nLet's implement the complete workflows from the Data Science Integration Guide, including Financial Analysis, Research Synthesis, Business Intelligence, and Market Research examples.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Financial Analysis Workflow Example\n\ndef create_financial_analysis_example():\n    \"\"\"Create complete financial analysis workflow from integration guide.\"\"\"\n    \n    # Create sample financial data\n    financial_data = {\n        'date': pd.date_range('2024-01-01', periods=12, freq='M'),\n        'revenue': [1500000, 1620000, 1580000, 1750000, 1690000, 1820000,\n                   1950000, 1880000, 2100000, 2050000, 2200000, 2350000],\n        'expenses': [1200000, 1250000, 1180000, 1300000, 1220000, 1350000,\n                    1400000, 1380000, 1450000, 1420000, 1500000, 1550000],\n        'market_segment': ['Consumer'] * 6 + ['Enterprise'] * 6\n    }\n    \n    df_financial = pd.DataFrame(financial_data)\n    df_financial['net_income'] = df_financial['revenue'] - df_financial['expenses']\n    df_financial['profit_margin'] = (df_financial['net_income'] / df_financial['revenue']) * 100\n    \n    # Save financial data\n    df_financial.to_csv('quarterly_financial_data.csv', index=False)\n    \n    print(\"üìä Financial Data Created:\")\n    display(df_financial.head())\n    \n    # Create financial analysis template (from integration guide)\n    financial_template = \"\"\"\nYou are a senior financial analyst. Perform comprehensive analysis of the provided financial data.\n\n## Financial Analysis for Company - 2024\n\n### Market Data Analysis\nAnalyze the following financial data and provide comprehensive insights:\n\n**Raw Data:**\n{{ quarterly_data.content }}\n\n### Analysis Requirements:\n1. **Performance Metrics**: Calculate key ratios (ROE, EBITDA margin, profit margins)\n2. **Trend Analysis**: Compare performance across time periods\n3. **Market Position**: Analyze segment performance \n4. **Risk Assessment**: Identify potential financial risks\n5. **Growth Projection**: Forecast trends based on current data\n\n### Regulatory Compliance Check:\nReview all metrics and flag any concerning trends for stakeholder reporting.\n\nCreate professional visualization showing key financial trends.\n\"\"\"\n    \n    with open('financial_analysis_template.j2', 'w') as f:\n        f.write(financial_template)\n    \n    # Financial analysis schema (from integration guide)\n    financial_schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"executive_summary\": {\n                \"type\": \"string\",\n                \"description\": \"2-3 sentence summary of financial health\"\n            },\n            \"key_metrics\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"total_revenue\": {\"type\": \"number\"},\n                    \"net_income\": {\"type\": \"number\"},\n                    \"average_profit_margin\": {\"type\": \"number\"},\n                    \"revenue_growth_rate\": {\"type\": \"number\"}\n                },\n                \"required\": [\"total_revenue\", \"net_income\", \"average_profit_margin\"]\n            },\n            \"trend_analysis\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"revenue_trend\": {\"type\": \"string\"},\n                    \"profit_margin_trend\": {\"type\": \"string\"},\n                    \"quarter_over_quarter_change\": {\"type\": \"number\"}\n                }\n            },\n            \"risk_factors\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"risk_type\": {\"type\": \"string\"},\n                        \"severity\": {\"type\": \"string\", \"enum\": [\"low\", \"medium\", \"high\", \"critical\"]},\n                        \"description\": {\"type\": \"string\"},\n                        \"mitigation_suggestions\": {\"type\": \"string\"}\n                    },\n                    \"required\": [\"risk_type\", \"severity\", \"description\"]\n                }\n            },\n            \"growth_forecast\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"next_quarter_revenue_estimate\": {\"type\": \"number\"},\n                    \"confidence_level\": {\"type\": \"string\", \"enum\": [\"low\", \"medium\", \"high\"]},\n                    \"key_assumptions\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n                }\n            }\n        },\n        \"required\": [\"executive_summary\", \"key_metrics\", \"risk_factors\"]\n    }\n    \n    with open('financial_analysis_schema.json', 'w') as f:\n        json.dump(financial_schema, f, indent=2)\n    \n    print(\"‚úÖ Financial analysis template and schema created\")\n    \n    # Run financial analysis\n    financial_results = run_ostruct_analysis(\n        template_file='financial_analysis_template.j2',\n        schema_file='financial_analysis_schema.json',\n        data_file='quarterly_financial_data.csv',\n        model='gpt-4o',\n        enable_tools=['code-interpreter', 'web-search'],\n        web_query='financial market trends Q4 2024 analysis',\n        output_file='financial_analysis_results.json'\n    )\n    \n    print(\"‚úÖ Financial Analysis Complete!\")\n    \n    # Display key results\n    print(\"\\nüíº FINANCIAL ANALYSIS SUMMARY:\")\n    print(f\"üìä Executive Summary: {financial_results['executive_summary']}\")\n    \n    if 'key_metrics' in financial_results:\n        metrics = financial_results['key_metrics']\n        print(f\"üí∞ Total Revenue: ${metrics.get('total_revenue', 0):,.2f}\")\n        print(f\"üí∏ Net Income: ${metrics.get('net_income', 0):,.2f}\")\n        print(f\"üìà Avg Profit Margin: {metrics.get('average_profit_margin', 0):.1f}%\")\n    \n    if 'risk_factors' in financial_results:\n        print(\"\\n‚ö†Ô∏è Risk Factors:\")\n        for risk in financial_results['risk_factors'][:3]:  # Show top 3\n            severity = risk.get('severity', 'unknown').upper()\n            print(f\"  ‚Ä¢ [{severity}] {risk.get('risk_type', 'Unknown')}: {risk.get('description', 'N/A')}\")\n    \n    return financial_results\n\n# Run financial analysis example\nfinancial_results = create_financial_analysis_example()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Business Intelligence Report Generation Example\n\ndef create_business_intelligence_example():\n    \"\"\"Create Business Intelligence workflow from integration guide.\"\"\"\n    \n    # Create sample business data\n    business_data = {\n        'date': pd.date_range('2024-01-01', periods=100, freq='D'),\n        'customer_segment': np.random.choice(['Enterprise', 'SMB', 'Consumer'], 100),\n        'product_category': np.random.choice(['Software', 'Hardware', 'Services'], 100),\n        'revenue': np.random.normal(50000, 15000, 100),\n        'customer_satisfaction': np.random.normal(4.2, 0.8, 100),\n        'market_share': np.random.normal(0.15, 0.05, 100)\n    }\n    \n    df_business = pd.DataFrame(business_data)\n    df_business['revenue'] = np.maximum(df_business['revenue'], 1000)  # Ensure positive revenue\n    df_business['customer_satisfaction'] = np.clip(df_business['customer_satisfaction'], 1, 5)\n    df_business['market_share'] = np.clip(df_business['market_share'], 0.01, 0.5)\n    \n    # Save business data\n    df_business.to_csv('business_intelligence_data.csv', index=False)\n    \n    print(\"üìä Business Intelligence Data Created:\")\n    display(df_business.head())\n    \n    # Create BI analysis template (from integration guide)\n    bi_template = \"\"\"\nYou are a senior business analyst. Perform comprehensive competitive analysis and business intelligence.\n\n## Business Intelligence Report - Q4 2024\n\n### Internal Performance Analysis\n**Sales Data:**\n{{ sales_data.content }}\n\n### Analysis Requirements:\n1. **Market Position**: Analyze our position vs competitors across key metrics\n2. **Growth Opportunities**: Identify untapped segments and expansion possibilities  \n3. **Competitive Threats**: Assess emerging competitors and market disruptions\n4. **Pricing Analysis**: Evaluate price positioning and optimization opportunities\n5. **Strategic Recommendations**: Provide actionable next steps with ROI projections\n\n### Executive Briefing Elements:\n- Top 3 strategic priorities\n- Revenue impact projections\n- Resource requirements\n- Timeline for implementation\n\nCreate professional visualizations showing competitive positioning and market trends.\n\"\"\"\n    \n    with open('bi_analysis_template.j2', 'w') as f:\n        f.write(bi_template)\n    \n    # BI analysis schema (from integration guide)\n    bi_schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"executive_summary\": {\n                \"type\": \"string\",\n                \"description\": \"CEO-ready 2-3 sentence summary of strategic position\"\n            },\n            \"market_position\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"market_share\": {\"type\": \"number\"},\n                    \"competitive_ranking\": {\"type\": \"integer\"},\n                    \"differentiation_strengths\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                    \"competitive_gaps\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n                }\n            },\n            \"growth_opportunities\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"opportunity\": {\"type\": \"string\"},\n                        \"market_size\": {\"type\": \"number\"},\n                        \"revenue_potential\": {\"type\": \"number\"},\n                        \"time_to_market\": {\"type\": \"string\"},\n                        \"investment_required\": {\"type\": \"number\"},\n                        \"risk_level\": {\"type\": \"string\", \"enum\": [\"low\", \"medium\", \"high\"]}\n                    },\n                    \"required\": [\"opportunity\", \"revenue_potential\", \"risk_level\"]\n                }\n            },\n            \"strategic_recommendations\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"recommendation\": {\"type\": \"string\"},\n                        \"priority\": {\"type\": \"string\", \"enum\": [\"critical\", \"high\", \"medium\", \"low\"]},\n                        \"expected_roi\": {\"type\": \"number\"},\n                        \"implementation_timeline\": {\"type\": \"string\"},\n                        \"resource_requirements\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                        \"success_metrics\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n                    },\n                    \"required\": [\"recommendation\", \"priority\", \"expected_roi\"]\n                }\n            },\n            \"competitive_analysis\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"top_competitors\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                    \"competitive_advantages\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                    \"market_threats\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n                }\n            }\n        },\n        \"required\": [\"executive_summary\", \"market_position\", \"growth_opportunities\", \"strategic_recommendations\"]\n    }\n    \n    with open('bi_analysis_schema.json', 'w') as f:\n        json.dump(bi_schema, f, indent=2)\n    \n    print(\"‚úÖ Business Intelligence template and schema created\")\n    \n    # Run BI analysis\n    bi_results = run_ostruct_analysis(\n        template_file='bi_analysis_template.j2',\n        schema_file='bi_analysis_schema.json',\n        data_file='business_intelligence_data.csv',\n        model='gpt-4o',\n        enable_tools=['code-interpreter', 'web-search'],\n        web_query='business intelligence market trends 2024 competitive analysis',\n        output_file='bi_analysis_results.json'\n    )\n    \n    print(\"‚úÖ Business Intelligence Analysis Complete!\")\n    \n    # Display key results\n    print(\"\\nüè¢ BUSINESS INTELLIGENCE SUMMARY:\")\n    print(f\"üìä Executive Summary: {bi_results['executive_summary']}\")\n    \n    if 'market_position' in bi_results:\n        position = bi_results['market_position']\n        print(f\"üìà Market Share: {position.get('market_share', 0):.1%}\")\n        print(f\"üèÜ Competitive Ranking: #{position.get('competitive_ranking', 'N/A')}\")\n    \n    if 'strategic_recommendations' in bi_results:\n        print(\"\\nüí° TOP STRATEGIC RECOMMENDATIONS:\")\n        for i, rec in enumerate(bi_results['strategic_recommendations'][:3], 1):\n            priority = rec.get('priority', 'medium').upper()\n            recommendation = rec.get('recommendation', 'N/A')\n            roi = rec.get('expected_roi', 0)\n            print(f\"  {i}. [{priority}] {recommendation}\")\n            print(f\"     Expected ROI: {roi:.1%}\")\n    \n    return bi_results\n\n# Run business intelligence example\nbi_results = create_business_intelligence_example()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Clean up temporary files created during this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temporary files\n",
    "import glob\n",
    "\n",
    "temp_files = [\n",
    "    '*.csv', '*.json', '*.j2', 'downloads/*', 'batch_results/*'\n",
    "]\n",
    "\n",
    "for pattern in temp_files:\n",
    "    for file in glob.glob(pattern):\n",
    "        try:\n",
    "            Path(file).unlink()\n",
    "            print(f\"üóëÔ∏è Removed: {file}\")\n",
    "        except:\n",
    "            pass  # Ignore errors for directories or non-existent files\n",
    "\n",
    "# Remove directories\n",
    "for dir_name in ['downloads', 'batch_results']:\n",
    "    try:\n",
    "        import shutil\n",
    "        shutil.rmtree(dir_name)\n",
    "        print(f\"üóëÔ∏è Removed directory: {dir_name}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "üéâ **Congratulations!** You've learned how to integrate ostruct with Jupyter notebooks for powerful data science workflows.\n",
    "\n",
    "### What to try next:\n",
    "\n",
    "1. **üîÑ Adapt for your data**: Replace the sample data with your own datasets\n",
    "2. **üé® Custom templates**: Create domain-specific templates for your analysis needs\n",
    "3. **üìä Advanced schemas**: Design schemas that capture your business metrics\n",
    "4. **üöÄ Production deployment**: Build automated pipelines using these patterns\n",
    "5. **üîó Tool integration**: Combine with other data science tools in your stack\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- [ostruct Documentation](https://ostruct.readthedocs.io/)\n",
    "- [Data Science Integration Guide](https://ostruct.readthedocs.io/en/latest/user-guide/data_science_integration.html)\n",
    "- [More Examples](../)\n",
    "- [GitHub Repository](https://github.com/yaniv-golan/ostruct)\n",
    "\n",
    "Happy analyzing! üöÄüìä"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
