{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Analysis with ostruct in Jupyter\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yaniv-golan/ostruct/blob/main/examples/data-science/notebooks/ostruct_data_analysis.ipynb)\n",
        "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/yaniv-golan/ostruct/main?labpath=examples%2Fdata-science%2Fnotebooks%2Fostruct_data_analysis.ipynb)\n",
        "[![Open in Jupyter](https://img.shields.io/badge/Open%20in-Jupyter-orange?logo=jupyter)](https://jupyter.org/try-jupyter/lab/?path=ostruct_data_analysis.ipynb)\n",
        "\n",
        "This notebook demonstrates how to use ostruct for data analysis within Jupyter notebooks, combining the power of AI-driven analysis with interactive data science workflows.\n",
        "\n",
        "> **\ud83d\udca1 Environment Options:**\n",
        "> - **Colab**: Full GPU support, Google Drive integration, built-in Secrets management\n",
        "> - **Binder**: Free hosted Jupyter environment, great for quick testing\n",
        "> - **Local Jupyter**: Full control, best performance, works with Notebook 7 features like [real-time collaboration](https://jupyter-notebook.readthedocs.io/en/stable/notebook_7_features.html#real-time-collaboration)\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- \ud83d\udcca Run ostruct analysis from Jupyter cells\n",
        "- \ud83d\udd04 Integrate ostruct results with pandas workflows\n",
        "- \ud83d\udcc8 Generate visualizations using AI + Code Interpreter\n",
        "- \ud83d\ude80 Build automated analysis pipelines\n",
        "- \ud83d\udca1 Best practices for production data science"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters cell (for Papermill parameterization)\n",
        "# Following Google Cloud Jupyter best practices for parameterized notebooks\n",
        "# Tag this cell with \"parameters\" for Papermill execution\n",
        "\n",
        "# Model configuration\n",
        "DEFAULT_MODEL = \"gpt-4.1\"\n",
        "ANALYSIS_TIMEOUT = 180  # seconds\n",
        "\n",
        "# Data configuration  \n",
        "SAMPLE_SIZE = 100  # Number of rows for demo data\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Output configuration\n",
        "ENABLE_VERBOSE_OUTPUT = True\n",
        "SAVE_INTERMEDIATE_RESULTS = True\n",
        "OUTPUT_DIR = \"notebook_outputs\"\n",
        "\n",
        "# Tool configuration\n",
        "DEFAULT_TOOLS = [\"code-interpreter\"]\n",
        "ENABLE_WEB_SEARCH = False  # Set to True for market research examples\n",
        "\n",
        "print(\"\ud83d\udccb Notebook parameters configured for reproducible execution\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Installation\n",
        "\n",
        "First, let's install ostruct and set up our environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ostruct (run this once)\n",
        "!pip install ostruct-cli\n",
        "\n",
        "# Import required libraries\n",
        "import json\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from IPython.display import Image, display, HTML\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"\u2705 Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up OpenAI API key (required) - Cross-platform approach\n",
        "import os\n",
        "\n",
        "def setup_openai_key():\n",
        "    \"\"\"\n",
        "    Set up OpenAI API key with cross-platform support for Colab, Jupyter, and local environments.\n",
        "    \n",
        "    This function automatically detects your environment and uses the most appropriate method:\n",
        "    - Colab: Uses built-in Secrets (recommended for security)\n",
        "    - Binder: Uses environment variables or getpass fallback\n",
        "    - Local Jupyter: Supports .env files, environment variables, or manual input\n",
        "    \"\"\"\n",
        "    \n",
        "    # Method 1: Try Colab Secrets first (if in Colab)\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "        print(\"\ud83d\udd11 API key loaded from Colab Secrets\")\n",
        "        return True\n",
        "    except ImportError:\n",
        "        # Not in Colab, continue with other methods\n",
        "        pass\n",
        "    except Exception as e:\n",
        "        print(f\"\u26a0\ufe0f  Colab Secrets not configured: {e}\")\n",
        "        print(\"   Add your API key to Colab Secrets:\")\n",
        "        print(\"   1. Click the \ud83d\udd11 key icon in the left sidebar\")\n",
        "        print(\"   2. Add 'OPENAI_API_KEY' as a secret\")\n",
        "        print(\"   3. Re-run this cell\")\n",
        "    \n",
        "    # Method 2: Try environment variable\n",
        "    if os.getenv('OPENAI_API_KEY'):\n",
        "        print(\"\ud83d\udd11 API key loaded from environment variable\")\n",
        "        return True\n",
        "    \n",
        "    # Method 3: Try .env file (for local Jupyter)\n",
        "    try:\n",
        "        from dotenv import load_dotenv\n",
        "        load_dotenv()\n",
        "        if os.getenv('OPENAI_API_KEY'):\n",
        "            print(\"\ud83d\udd11 API key loaded from .env file\")\n",
        "            return True\n",
        "    except ImportError:\n",
        "        pass\n",
        "    \n",
        "    # Method 4: Fallback to manual entry (least secure but most compatible)\n",
        "    print(\"\ud83d\udca1 For better security, consider:\")\n",
        "    print(\"   \u2022 Colab: Use Colab Secrets (\ud83d\udd11 icon in sidebar)\")\n",
        "    print(\"   \u2022 Jupyter: Set OPENAI_API_KEY environment variable\")\n",
        "    print(\"   \u2022 Local: Create .env file with python-dotenv\")\n",
        "    \n",
        "    import getpass\n",
        "    os.environ['OPENAI_API_KEY'] = getpass.getpass('Enter your OpenAI API key: ')\n",
        "    print(\"\ud83d\udd11 API key configured\")\n",
        "    return True\n",
        "\n",
        "setup_openai_key()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility functions for notebook robustness and experiment tracking\n",
        "import uuid\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# Experiment tracking (following Google Cloud best practices)\n",
        "class ExperimentTracker:\n",
        "    \"\"\"\n",
        "    Track experiments automatically following Google Cloud Jupyter best practices.\n",
        "    Logs metadata about training sessions, hyperparameters, data sources, results, and timing.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, experiment_name=\"ostruct_analysis\"):\n",
        "        self.experiment_name = experiment_name\n",
        "        self.experiment_id = str(uuid.uuid4())[:8]\n",
        "        self.start_time = datetime.now()\n",
        "        # Use OUTPUT_DIR from parameters cell or default\n",
        "        output_dir = globals().get('OUTPUT_DIR', 'notebook_outputs')\n",
        "        self.log_dir = Path(output_dir) / \"experiment_logs\"\n",
        "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        self.metadata = {\n",
        "            \"experiment_id\": self.experiment_id,\n",
        "            \"experiment_name\": experiment_name,\n",
        "            \"start_time\": self.start_time.isoformat(),\n",
        "            \"parameters\": {},\n",
        "            \"results\": {},\n",
        "            \"execution_info\": {}\n",
        "        }\n",
        "        \n",
        "    def log_parameters(self, **params):\n",
        "        \"\"\"Log experiment parameters\"\"\"\n",
        "        self.metadata[\"parameters\"].update(params)\n",
        "        \n",
        "    def log_results(self, **results):\n",
        "        \"\"\"Log experiment results\"\"\"\n",
        "        self.metadata[\"results\"].update(results)\n",
        "        \n",
        "    def log_execution_info(self, **info):\n",
        "        \"\"\"Log execution metadata\"\"\"\n",
        "        self.metadata[\"execution_info\"].update(info)\n",
        "        \n",
        "    def save_experiment(self):\n",
        "        \"\"\"Save experiment log to file\"\"\"\n",
        "        self.metadata[\"end_time\"] = datetime.now().isoformat()\n",
        "        self.metadata[\"duration_seconds\"] = (datetime.now() - self.start_time).total_seconds()\n",
        "        \n",
        "        log_file = self.log_dir / f\"experiment_{self.experiment_id}.json\"\n",
        "        with open(log_file, 'w') as f:\n",
        "            json.dump(self.metadata, f, indent=2)\n",
        "        \n",
        "        print(f\"\ud83d\udcbe Experiment logged: {log_file}\")\n",
        "        return log_file\n",
        "\n",
        "def ensure_imports():\n",
        "    \"\"\"\n",
        "    Ensure required imports are available for data analysis.\n",
        "    This function can be called at the start of any cell that needs pandas/matplotlib.\n",
        "    \n",
        "    Based on best practices from:\n",
        "    - https://ryan.orendorff.io/posts/2022-11-27-define-once/\n",
        "    - https://www.angela1c.com/posts/2021/08/a-few-random-reference-notes.../\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "        from IPython.display import display, HTML\n",
        "        \n",
        "        # Make imports available in global namespace for notebook cells\n",
        "        globals().update({\n",
        "            'pd': pd,\n",
        "            'plt': plt, \n",
        "            'sns': sns,\n",
        "            'display': display,\n",
        "            'HTML': HTML\n",
        "        })\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except ImportError as e:\n",
        "        print(\"\u274c Missing required packages!\")\n",
        "        print(\"\ud83d\udca1 Solution: Run the setup cell (Cell 2) first to install and import libraries\")\n",
        "        print(f\"Missing: {e}\")\n",
        "        print(\"\\nIf packages aren't installed, run:\")\n",
        "        print(\"!pip install pandas matplotlib seaborn\")\n",
        "        raise\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Unexpected error setting up imports: {e}\")\n",
        "        print(\"\ud83d\udca1 Try restarting the kernel and running cells in order\")\n",
        "        raise\n",
        "\n",
        "# Initialize global experiment tracker\n",
        "experiment = ExperimentTracker(\"ostruct_data_analysis\")\n",
        "\n",
        "print(\"\ud83d\udcda Utility functions loaded with experiment tracking!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core ostruct Integration Functions\n",
        "\n",
        "Let's create helper functions for running ostruct from Jupyter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_ostruct_analysis(template_file, schema_file, data_file=None, model=None, \n",
        "                        enable_tools=None, output_file=None):\n",
        "    \"\"\"\n",
        "    Run ostruct analysis from Jupyter and return results.\n",
        "    Following Google Cloud Jupyter best practices for parameterized execution.\n",
        "    \n",
        "    Args:\n",
        "        template_file: Path to Jinja2 template\n",
        "        schema_file: Path to JSON schema\n",
        "        data_file: Optional data file path\n",
        "        model: OpenAI model to use (uses DEFAULT_MODEL parameter if not specified)\n",
        "        enable_tools: List of tools to enable ['code-interpreter', 'web-search', 'file-search']\n",
        "        output_file: Optional output file path\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with analysis results\n",
        "    \"\"\"\n",
        "    # Use parameterized defaults (following Google Cloud best practices)\n",
        "    if model is None:\n",
        "        model = globals().get('DEFAULT_MODEL', 'gpt-4.1')\n",
        "    if enable_tools is None:\n",
        "        enable_tools = globals().get('DEFAULT_TOOLS', ['code-interpreter'])\n",
        "    \n",
        "    # Log experiment parameters\n",
        "    experiment.log_parameters(\n",
        "        template_file=template_file,\n",
        "        schema_file=schema_file,\n",
        "        data_file=data_file,\n",
        "        model=model,\n",
        "        enable_tools=enable_tools\n",
        "    )\n",
        "    \n",
        "    cmd = ['ostruct', 'run', template_file, schema_file, '--model', model]\n",
        "    \n",
        "    # Add data file if provided\n",
        "    if data_file:\n",
        "        cmd.extend(['--file', f'ci:data', data_file])\n",
        "    \n",
        "    # Enable tools\n",
        "    if enable_tools:\n",
        "        for tool in enable_tools:\n",
        "            cmd.extend(['--enable-tool', tool])\n",
        "    \n",
        "    # Add output file\n",
        "    if output_file:\n",
        "        cmd.extend(['--output-file', output_file])\n",
        "    \n",
        "    print(f\"\ud83d\ude80 Running: {' '.join(cmd)}\")\n",
        "    \n",
        "    try:\n",
        "        # Use parameterized timeout\n",
        "        timeout = globals().get('ANALYSIS_TIMEOUT', 180)\n",
        "        \n",
        "        result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=timeout)\n",
        "        \n",
        "        # Log execution info\n",
        "        experiment.log_execution_info(\n",
        "            command=' '.join(cmd),\n",
        "            return_code=result.returncode,\n",
        "            execution_time=timeout  # Will be updated with actual time in save_experiment\n",
        "        )\n",
        "        \n",
        "        if output_file:\n",
        "            # Read from output file\n",
        "            with open(output_file, 'r') as f:\n",
        "                analysis_results = json.load(f)\n",
        "        else:\n",
        "            # Parse stdout\n",
        "            analysis_results = json.loads(result.stdout)\n",
        "        \n",
        "        # Log results for experiment tracking\n",
        "        experiment.log_results(\n",
        "            analysis_successful=True,\n",
        "            result_keys=list(analysis_results.keys()) if isinstance(analysis_results, dict) else None,\n",
        "            output_file=output_file\n",
        "        )\n",
        "        \n",
        "        return analysis_results\n",
        "            \n",
        "    except subprocess.TimeoutExpired:\n",
        "        timeout_seconds = globals().get('ANALYSIS_TIMEOUT', 180)\n",
        "        print(f\"\u23f0 Command timed out after {timeout_seconds} seconds ({timeout_seconds/60:.1f} minutes)\")\n",
        "        print(\"This might indicate:\")\n",
        "        print(\"  - API is slow to respond\")\n",
        "        print(\"  - Network connectivity issues\")\n",
        "        print(\"  - Complex analysis taking longer than expected\")\n",
        "        raise\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"\u274c Error: {e}\")\n",
        "        print(f\"Stderr: {e.stderr}\")\n",
        "        raise\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"\u274c JSON parsing error: {e}\")\n",
        "        print(f\"Raw output: {result.stdout}\")\n",
        "        raise\n",
        "\n",
        "def display_analysis_summary(results):\n",
        "    \"\"\"\n",
        "    Display a formatted summary of analysis results.\n",
        "    \"\"\"\n",
        "    if 'summary' in results:\n",
        "        summary = results['summary']\n",
        "        print(\"\ud83d\udcca ANALYSIS SUMMARY\")\n",
        "        print(\"=\" * 40)\n",
        "        for key, value in summary.items():\n",
        "            if isinstance(value, (int, float)):\n",
        "                if 'sales' in key.lower() or 'revenue' in key.lower():\n",
        "                    print(f\"{key.replace('_', ' ').title():<20} ${value:,.2f}\")\n",
        "                else:\n",
        "                    print(f\"{key.replace('_', ' ').title():<20} {value:,}\")\n",
        "            else:\n",
        "                print(f\"{key.replace('_', ' ').title():<20} {value}\")\n",
        "        print()\n",
        "    \n",
        "    # Display chart info if available\n",
        "    if 'chart_info' in results:\n",
        "        chart = results['chart_info']\n",
        "        print(f\"\ud83d\udcc8 Generated Chart: {chart.get('filename', 'N/A')}\")\n",
        "        print(f\"   Description: {chart.get('description', 'N/A')}\")\n",
        "        \n",
        "        # Try to display the chart if it exists\n",
        "        chart_path = Path('downloads') / chart.get('filename', '')\n",
        "        if chart_path.exists():\n",
        "            display(Image(str(chart_path)))\n",
        "\n",
        "print(\"\u2705 Helper functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: Basic Data Analysis\n",
        "\n",
        "Let's start with a simple CSV analysis using the data science template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample data for analysis\n",
        "# Ensure required imports are available (robust approach for notebook execution)\n",
        "ensure_imports()\n",
        "\n",
        "sample_data = {\n",
        "    'date': ['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05'],\n",
        "    'product': ['Widget A', 'Widget B', 'Widget A', 'Widget C', 'Widget B'],\n",
        "    'quantity': [10, 15, 8, 12, 20],\n",
        "    'price': [25.50, 30.00, 25.50, 45.00, 30.00]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(sample_data)\n",
        "df['revenue'] = df['quantity'] * df['price']\n",
        "\n",
        "# Save to CSV for ostruct analysis\n",
        "df.to_csv('sample_sales.csv', index=False)\n",
        "\n",
        "print(\"\ud83d\udccb Sample Data Created:\")\n",
        "display(df)\n",
        "\n",
        "# Create basic template and schema for this demo\n",
        "# (In practice, you'd have these files prepared)\n",
        "\n",
        "basic_template = \"\"\"\n",
        "You are a data analyst. Analyze the provided sales data and generate insights.\n",
        "\n",
        "## Sales Data Analysis\n",
        "\n",
        "**Data to analyze:**\n",
        "{{ data.content }}\n",
        "\n",
        "## Analysis Requirements:\n",
        "1. **Summary Statistics**: Calculate total sales, average price, transaction count\n",
        "2. **Product Performance**: Identify top-performing products\n",
        "3. **Trends**: Analyze sales patterns and trends\n",
        "4. **Data Quality**: Assess data completeness and note any issues\n",
        "5. **Visualization**: Create a chart showing sales by product\n",
        "\n",
        "## Output Format:\n",
        "Provide analysis in the structured JSON format specified in the schema.\n",
        "\"\"\"\n",
        "\n",
        "with open('basic_template.j2', 'w') as f:\n",
        "    f.write(basic_template)\n",
        "\n",
        "basic_schema = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"summary\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"total_sales\": {\"type\": \"number\"},\n",
        "                \"average_price\": {\"type\": \"number\"},\n",
        "                \"product_count\": {\"type\": \"integer\"},\n",
        "                \"total_transactions\": {\"type\": \"integer\"}\n",
        "            }\n",
        "        },\n",
        "        \"sales_by_product\": {\n",
        "            \"type\": \"object\",\n",
        "            \"description\": \"Sales totals by product name\"\n",
        "        },\n",
        "        \"chart_info\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"filename\": {\"type\": \"string\"},\n",
        "                \"description\": {\"type\": \"string\"},\n",
        "                \"chart_type\": {\"type\": \"string\"}\n",
        "            }\n",
        "        },\n",
        "        \"data_quality\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"rows_processed\": {\"type\": \"integer\"},\n",
        "                \"missing_values\": {\"type\": \"integer\"},\n",
        "                \"data_issues\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    \"required\": [\"summary\", \"chart_info\", \"data_quality\"]\n",
        "}\n",
        "\n",
        "with open('basic_schema.json', 'w') as f:\n",
        "    json.dump(basic_schema, f, indent=2)\n",
        "\n",
        "print(\"\u2705 Template and schema files created for demo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run ostruct analysis with comprehensive error handling\n",
        "print(\"\ud83d\ude80 Starting ostruct analysis...\")\n",
        "\n",
        "try:\n",
        "    # First, let's verify our files exist\n",
        "    print(\"\ud83d\udccb Checking required files:\")\n",
        "    required_files = ['basic_template.j2', 'basic_schema.json', 'sample_sales.csv']\n",
        "    for file in required_files:\n",
        "        if Path(file).exists():\n",
        "            print(f\"  \u2705 {file} ({Path(file).stat().st_size} bytes)\")\n",
        "        else:\n",
        "            print(f\"  \u274c {file} - MISSING!\")\n",
        "            \n",
        "    print(\"\\n\ud83d\udd11 Checking API key...\")\n",
        "    if 'OPENAI_API_KEY' in os.environ:\n",
        "        key = os.environ['OPENAI_API_KEY']\n",
        "        print(f\"  \u2705 API key set ({key[:10]}...{key[-4:]})\")\n",
        "    else:\n",
        "        print(\"  \u274c OPENAI_API_KEY not found!\")\n",
        "        \n",
        "    print(\"\\n\u23f0 Running analysis (this may take 30-60 seconds)...\")\n",
        "    \n",
        "    # Run with timeout and verbose output\n",
        "    import subprocess\n",
        "    import signal\n",
        "    \n",
        "    cmd = [\n",
        "        'ostruct', 'run', \n",
        "        'basic_template.j2', \n",
        "        'basic_schema.json',\n",
        "        '--file', 'ci:data', 'sample_sales.csv',\n",
        "        '--model', 'gpt-4.1',\n",
        "        '--enable-tool', 'code-interpreter',\n",
        "        '--output-file', 'analysis_results.json',\n",
        "        '--verbose'  # Add verbose output\n",
        "    ]\n",
        "    \n",
        "    print(f\"\ud83d\udd27 Command: {' '.join(cmd)}\")\n",
        "    \n",
        "    # Run with timeout (using parameterized value)\n",
        "    timeout_seconds = globals().get('ANALYSIS_TIMEOUT', 180)\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            cmd, \n",
        "            capture_output=True, \n",
        "            text=True, \n",
        "            timeout=timeout_seconds,  # Use parameterized timeout\n",
        "            check=True\n",
        "        )\n",
        "        \n",
        "        print(\"\u2705 Command completed successfully!\")\n",
        "        print(f\"\ud83d\udce4 Return code: {result.returncode}\")\n",
        "        \n",
        "        if result.stdout:\n",
        "            print(\"\ud83d\udcc4 Output:\")\n",
        "            print(result.stdout[:1000])  # First 1000 chars\n",
        "            \n",
        "        # Load results\n",
        "        if Path('analysis_results.json').exists():\n",
        "            with open('analysis_results.json', 'r') as f:\n",
        "                results = json.load(f)\n",
        "            print(\"\u2705 Results loaded successfully!\")\n",
        "            display_analysis_summary(results)\n",
        "        else:\n",
        "            print(\"\u274c Output file not created\")\n",
        "            \n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(f\"\u23f0 Command timed out after {timeout_seconds} seconds ({timeout_seconds/60:.1f} minutes)\")\n",
        "        print(\"This might indicate:\")\n",
        "        print(\"  - API is slow to respond\")\n",
        "        print(\"  - Network connectivity issues\")\n",
        "        print(\"  - API key problems\")\n",
        "        \n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"\u274c Command failed with exit code {e.returncode}\")\n",
        "        print(f\"\ud83d\udcc4 Error output:\")\n",
        "        print(e.stderr)\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Unexpected error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n\ud83d\udcc1 Current directory contents:\")\n",
        "for item in sorted(Path('.').iterdir()):\n",
        "    if item.is_file():\n",
        "        print(f\"  \ud83d\udcc4 {item.name} ({item.stat().st_size} bytes)\")\n",
        "    else:\n",
        "        print(f\"  \ud83d\udcc1 {item.name}/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and display the analysis results\n",
        "print(\"\ud83d\udcca Loading analysis results...\")\n",
        "\n",
        "try:\n",
        "    # Load the results file that was created\n",
        "    with open('analysis_results.json', 'r') as f:\n",
        "        results = json.load(f)\n",
        "    \n",
        "    print(\"\u2705 Results loaded successfully!\")\n",
        "    print(f\"\ud83d\udcc4 Result keys: {list(results.keys())}\")\n",
        "    \n",
        "    # Display the full results first\n",
        "    print(\"\\n\ud83d\udccb COMPLETE ANALYSIS RESULTS:\")\n",
        "    print(json.dumps(results, indent=2))\n",
        "    \n",
        "    # Use our display function\n",
        "    display_analysis_summary(results)\n",
        "    \n",
        "    # Look for any generated chart files with the RC fix\n",
        "    print(\"\\n\ud83d\udd0d Comprehensive search for generated charts...\")\n",
        "    \n",
        "    # Check for image files in multiple locations\n",
        "    image_extensions = ['.png', '.jpg', '.jpeg', '.svg', '.gif']\n",
        "    \n",
        "    # Search locations where charts might be saved\n",
        "    search_locations = [\n",
        "        Path('.'),  # Current directory\n",
        "        Path('downloads'),  # Default download location\n",
        "        Path('/content'),  # Colab content directory  \n",
        "        Path('/content/downloads'),  # Colab downloads\n",
        "    ]\n",
        "    \n",
        "    all_found_images = []\n",
        "    \n",
        "    for location in search_locations:\n",
        "        if location.exists() and location.is_dir():\n",
        "            print(f\"\\n\ud83d\udcc1 Checking {location}:\")\n",
        "            try:\n",
        "                items = list(location.iterdir())\n",
        "                image_files = [f for f in items if f.suffix.lower() in image_extensions]\n",
        "                \n",
        "                if image_files:\n",
        "                    print(f\"  \ud83c\udfaf Found {len(image_files)} image(s):\")\n",
        "                    for img in image_files:\n",
        "                        print(f\"    \ud83d\udcca {img.name} ({img.stat().st_size} bytes)\")\n",
        "                        all_found_images.append(img)\n",
        "                else:\n",
        "                    all_files = [f for f in items if f.is_file()]\n",
        "                    print(f\"  \ud83d\udcc4 {len(all_files)} files, no images\")\n",
        "                    if all_files and len(all_files) <= 10:  # Show files if not too many\n",
        "                        print(f\"    Files: {[f.name for f in all_files]}\")\n",
        "                        \n",
        "            except Exception as e:\n",
        "                print(f\"  \u274c Error accessing {location}: {e}\")\n",
        "        else:\n",
        "            print(f\"\ud83d\udcc1 {location}: doesn't exist\")\n",
        "    \n",
        "    # Check if the specific chart mentioned in results exists\n",
        "    if 'chart_info' in results and 'filename' in results['chart_info']:\n",
        "        chart_filename = results['chart_info']['filename']\n",
        "        print(f\"\\n\ud83c\udfaf Looking specifically for: {chart_filename}\")\n",
        "        \n",
        "        # Check in all search locations\n",
        "        for location in search_locations:\n",
        "            if location.exists():\n",
        "                chart_path = location / chart_filename\n",
        "                if chart_path.exists():\n",
        "                    print(f\"  \u2705 Found at: {chart_path}\")\n",
        "                    all_found_images.append(chart_path)\n",
        "                else:\n",
        "                    print(f\"  \u274c Not found in: {location}\")\n",
        "    \n",
        "    # Display all found images\n",
        "    if all_found_images:\n",
        "        print(f\"\\n\ud83c\udfa8 Displaying {len(all_found_images)} chart(s):\")\n",
        "        for img in all_found_images:\n",
        "            print(f\"\\n\ud83d\udcca Displaying chart: {img}\")\n",
        "            try:\n",
        "                display(Image(str(img)))\n",
        "                print(f\"\u2705 Successfully displayed {img.name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"\u274c Could not display {img}: {e}\")\n",
        "    else:\n",
        "        print(\"\\n\ud83e\udd14 No AI-generated charts found - the download bug still exists!\")\n",
        "        print(\"Creating fallback chart with matplotlib...\")\n",
        "        \n",
        "        # Create a fallback chart from the data\n",
        "        import pandas as pd\n",
        "        import matplotlib.pyplot as plt\n",
        "        \n",
        "        df = pd.read_csv('sample_sales.csv')\n",
        "        \n",
        "        # Create sales by product chart matching the AI description\n",
        "        sales_by_product = df.groupby('product')['revenue'].sum().sort_values(ascending=False)\n",
        "        \n",
        "        plt.figure(figsize=(10, 6))\n",
        "        bars = plt.bar(sales_by_product.index, sales_by_product.values, \n",
        "                      color=['#2E86AB', '#A23B72', '#F18F01'])\n",
        "        plt.title('Sales by Product', fontsize=16, fontweight='bold', pad=20)\n",
        "        plt.xlabel('Product', fontsize=12)\n",
        "        plt.ylabel('Revenue ($)', fontsize=12)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        \n",
        "        # Add value labels on bars\n",
        "        for bar, value in zip(bars, sales_by_product.values):\n",
        "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n",
        "                    f'${value:.0f}', ha='center', va='bottom', fontweight='bold')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.grid(axis='y', alpha=0.3)\n",
        "        \n",
        "        # Add summary text\n",
        "        total_sales = sales_by_product.sum()\n",
        "        plt.figtext(0.02, 0.02, f'Total Sales: ${total_sales:.0f} \u2022 Top Product: {sales_by_product.index[0]}', \n",
        "                   fontsize=10, style='italic')\n",
        "        \n",
        "        plt.show()\n",
        "        \n",
        "        print(\"\u2705 Fallback chart displayed!\")\n",
        "        print(\"\ud83d\udca1 This shows the same data the AI analyzed, just generated with matplotlib instead\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a more complex template for multi-tool analysis\n",
        "multi_tool_template = \"\"\"\n",
        "You are a senior data analyst. Perform comprehensive analysis combining internal data with market research.\n",
        "\n",
        "# Multi-Source Analysis Task\n",
        "\n",
        "## Internal Data Analysis\n",
        "{{ data.content }}\n",
        "\n",
        "## Market Context Analysis\n",
        "{% if web_search_enabled %}\n",
        "**Market Research Instructions:**\n",
        "Use web search to find current information about:\n",
        "- Widget sales industry trends for 2024\n",
        "- Market analysis and competitive landscape  \n",
        "- Industry benchmarks and performance metrics\n",
        "- Recent developments affecting the widget market\n",
        "\n",
        "Incorporate these findings into your analysis and cite sources.\n",
        "{% else %}\n",
        "**Limited Market Context:**\n",
        "Web search is not available. Focus on internal data analysis only.\n",
        "{% endif %}\n",
        "\n",
        "## Analysis Requirements\n",
        "1. **Internal Performance**: Analyze sales trends, top products, growth patterns\n",
        "2. **Market Context**: Compare with industry trends and competitor performance (if web search available)\n",
        "3. **Strategic Insights**: Identify opportunities and risks\n",
        "4. **Recommendations**: Provide 3-5 actionable recommendations with rationale\n",
        "5. **Visualization**: Create a professional chart showing key insights\n",
        "\n",
        "## Output Format\n",
        "Provide comprehensive analysis in the specified JSON structure with business insights and actionable recommendations.\n",
        "\"\"\"\n",
        "\n",
        "# Save template\n",
        "with open('multi_tool_template.j2', 'w') as f:\n",
        "    f.write(multi_tool_template)\n",
        "\n",
        "print(\"\u2705 Multi-tool template created with proper web search integration\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create enhanced schema for multi-tool analysis\n",
        "enhanced_schema = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"internal_analysis\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"total_revenue\": {\"type\": \"number\"},\n",
        "                \"top_product\": {\"type\": \"string\"},\n",
        "                \"growth_trend\": {\"type\": \"string\"},\n",
        "                \"key_metrics\": {\"type\": \"object\"}\n",
        "            }\n",
        "        },\n",
        "        \"market_insights\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"industry_trends\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "                \"competitive_position\": {\"type\": \"string\"},\n",
        "                \"market_opportunities\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
        "            }\n",
        "        },\n",
        "        \"recommendations\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"recommendation\": {\"type\": \"string\"},\n",
        "                    \"priority\": {\"type\": \"string\", \"enum\": [\"high\", \"medium\", \"low\"]},\n",
        "                    \"expected_impact\": {\"type\": \"string\"}\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"visualization\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"chart_type\": {\"type\": \"string\"},\n",
        "                \"filename\": {\"type\": \"string\"},\n",
        "                \"insights\": {\"type\": \"string\"}\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    \"required\": [\"internal_analysis\", \"recommendations\"]\n",
        "}\n",
        "\n",
        "# Save schema\n",
        "with open('enhanced_schema.json', 'w') as f:\n",
        "    json.dump(enhanced_schema, f, indent=2)\n",
        "\n",
        "print(\"\u2705 Enhanced schema created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run multi-tool analysis (corrected - no --web-query parameter)\n",
        "enhanced_results = run_ostruct_analysis(\n",
        "    template_file='multi_tool_template.j2',\n",
        "    schema_file='enhanced_schema.json', \n",
        "    data_file='sample_sales.csv',\n",
        "    model='gpt-4o',\n",
        "    enable_tools=['code-interpreter', 'web-search'],\n",
        "    output_file='enhanced_results.json'\n",
        ")\n",
        "\n",
        "print(\"\u2705 Enhanced analysis complete!\")\n",
        "print(\"\\n\ud83d\udccb ENHANCED ANALYSIS RESULTS:\")\n",
        "print(json.dumps(enhanced_results, indent=2))\n",
        "\n",
        "# Display key results\n",
        "if 'internal_analysis' in enhanced_results:\n",
        "    internal = enhanced_results['internal_analysis']\n",
        "    print(f\"\\n\ud83d\udcbc INTERNAL ANALYSIS:\")\n",
        "    print(f\"\ud83d\udcca Total Revenue: ${internal.get('total_revenue', 0):,.2f}\")\n",
        "    print(f\"\ud83c\udfc6 Top Product: {internal.get('top_product', 'N/A')}\")\n",
        "\n",
        "if 'recommendations' in enhanced_results:\n",
        "    print(f\"\\n\ud83d\udca1 KEY RECOMMENDATIONS:\")\n",
        "    for i, rec in enumerate(enhanced_results['recommendations'][:3], 1):\n",
        "        priority = rec.get('priority', 'medium').upper()\n",
        "        recommendation = rec.get('recommendation', 'N/A')\n",
        "        print(f\"  {i}. [{priority}] {recommendation}\")\n",
        "\n",
        "if 'market_insights' in enhanced_results:\n",
        "    insights = enhanced_results['market_insights']\n",
        "    if 'industry_trends' in insights and insights['industry_trends']:\n",
        "        print(f\"\\n\ud83d\udcc8 MARKET INSIGHTS:\")\n",
        "        for trend in insights['industry_trends'][:3]:\n",
        "            print(f\"  \u2022 {trend}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3: Interactive Data Science Workflow\n",
        "\n",
        "Let's create an interactive workflow that combines pandas analysis with AI insights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interactive_analysis_workflow(dataframe, analysis_question):\n",
        "    \"\"\"\n",
        "    Interactive workflow combining pandas analysis with AI insights.\n",
        "    \"\"\"\n",
        "    print(f\"\ud83d\udd0d Analyzing: {analysis_question}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Step 1: Basic pandas analysis\n",
        "    print(\"\ud83d\udcca Step 1: Basic Statistics\")\n",
        "    print(dataframe.describe())\n",
        "    print()\n",
        "    \n",
        "    # Step 2: Save data for AI analysis\n",
        "    temp_file = 'temp_analysis.csv'\n",
        "    dataframe.to_csv(temp_file, index=False)\n",
        "    \n",
        "    # Step 3: Create dynamic template based on question\n",
        "    dynamic_template = f\"\"\"\n",
        "You are a data scientist. Answer this specific question about the provided dataset:\n",
        "\n",
        "**Question**: {analysis_question}\n",
        "\n",
        "**Dataset**: Analyze the provided CSV data to answer the question.\n",
        "\n",
        "## Analysis Requirements:\n",
        "1. Load and examine the data thoroughly\n",
        "2. Perform relevant statistical analysis to answer the question\n",
        "3. Create appropriate visualizations\n",
        "4. Provide clear, data-driven insights\n",
        "5. Include confidence levels and any caveats\n",
        "\n",
        "Focus specifically on answering: \"{analysis_question}\"\n",
        "\n",
        "Provide your analysis in the structured format below.\n",
        "\"\"\"\n",
        "    \n",
        "    with open('dynamic_template.j2', 'w') as f:\n",
        "        f.write(dynamic_template)\n",
        "    \n",
        "    # Dynamic schema\n",
        "    dynamic_schema = {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"question\": {\"type\": \"string\"},\n",
        "            \"answer\": {\"type\": \"string\"},\n",
        "            \"supporting_evidence\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "            \"confidence_level\": {\"type\": \"string\", \"enum\": [\"high\", \"medium\", \"low\"]},\n",
        "            \"key_insights\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "            \"chart_info\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"filename\": {\"type\": \"string\"},\n",
        "                    \"description\": {\"type\": \"string\"}\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"question\", \"answer\", \"confidence_level\"]\n",
        "    }\n",
        "    \n",
        "    with open('dynamic_schema.json', 'w') as f:\n",
        "        json.dump(dynamic_schema, f, indent=2)\n",
        "    \n",
        "    # Step 4: Run AI analysis\n",
        "    print(\"\ud83e\udd16 Step 2: AI Analysis\")\n",
        "    ai_results = run_ostruct_analysis(\n",
        "        template_file='dynamic_template.j2',\n",
        "        schema_file='dynamic_schema.json',\n",
        "        data_file=temp_file,\n",
        "        model='gpt-4o',\n",
        "        enable_tools=['code-interpreter']\n",
        "    )\n",
        "    \n",
        "    # Step 5: Display results\n",
        "    print(f\"\\n\ud83c\udfaf Answer: {ai_results['answer']}\")\n",
        "    print(f\"\ud83d\udd12 Confidence: {ai_results['confidence_level']}\")\n",
        "    \n",
        "    if 'key_insights' in ai_results:\n",
        "        print(\"\\n\ud83d\udca1 Key Insights:\")\n",
        "        for insight in ai_results['key_insights']:\n",
        "            print(f\"  \u2022 {insight}\")\n",
        "    \n",
        "    # Clean up\n",
        "    Path(temp_file).unlink()\n",
        "    \n",
        "    return ai_results\n",
        "\n",
        "print(\"\u2705 Interactive workflow function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the interactive workflow\n",
        "question = \"Which product has the highest profit margin and what factors contribute to its success?\"\n",
        "\n",
        "workflow_results = interactive_analysis_workflow(df, question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 4: Batch Processing Multiple Datasets\n",
        "\n",
        "For production scenarios, you often need to analyze multiple datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def batch_analysis(file_list, template_file, schema_file, output_dir='batch_results'):\n",
        "    \"\"\"\n",
        "    Analyze multiple datasets in batch using ostruct.\n",
        "    \"\"\"\n",
        "    Path(output_dir).mkdir(exist_ok=True)\n",
        "    batch_results = {}\n",
        "    \n",
        "    for i, file_path in enumerate(file_list):\n",
        "        print(f\"\\n\ud83d\udcca Processing {i+1}/{len(file_list)}: {file_path}\")\n",
        "        \n",
        "        try:\n",
        "            output_file = Path(output_dir) / f\"{Path(file_path).stem}_analysis.json\"\n",
        "            \n",
        "            results = run_ostruct_analysis(\n",
        "                template_file=template_file,\n",
        "                schema_file=schema_file,\n",
        "                data_file=file_path,\n",
        "                model='gpt-4o-mini',\n",
        "                enable_tools=['code-interpreter'],\n",
        "                output_file=str(output_file)\n",
        "            )\n",
        "            \n",
        "            batch_results[file_path] = {\n",
        "                'status': 'success',\n",
        "                'results': results,\n",
        "                'output_file': str(output_file)\n",
        "            }\n",
        "            \n",
        "            print(f\"  \u2705 Success: {output_file}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  \u274c Error: {e}\")\n",
        "            batch_results[file_path] = {\n",
        "                'status': 'error',\n",
        "                'error': str(e)\n",
        "            }\n",
        "    \n",
        "    return batch_results\n",
        "\n",
        "# Create multiple sample datasets\n",
        "datasets = []\n",
        "for month in ['Jan', 'Feb', 'Mar']:\n",
        "    monthly_data = df.copy()\n",
        "    monthly_data['month'] = month\n",
        "    monthly_data['quantity'] = monthly_data['quantity'] * (1 + 0.1 * len(datasets))  # Simulate growth\n",
        "    \n",
        "    filename = f'sales_{month.lower()}.csv'\n",
        "    monthly_data.to_csv(filename, index=False)\n",
        "    datasets.append(filename)\n",
        "\n",
        "print(f\"\u2705 Created {len(datasets)} datasets for batch processing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run batch analysis\n",
        "batch_results = batch_analysis(\n",
        "    file_list=datasets,\n",
        "    template_file='../analysis/templates/main.j2',\n",
        "    schema_file='../analysis/schemas/main.json'\n",
        ")\n",
        "\n",
        "# Summary of batch results\n",
        "successful = sum(1 for r in batch_results.values() if r['status'] == 'success')\n",
        "print(f\"\\n\ud83d\udcc8 Batch Analysis Complete: {successful}/{len(datasets)} successful\")\n",
        "\n",
        "# Display summary of results\n",
        "for file_path, result in batch_results.items():\n",
        "    if result['status'] == 'success':\n",
        "        summary = result['results'].get('summary', {})\n",
        "        total_sales = summary.get('total_sales', 0)\n",
        "        print(f\"  {Path(file_path).stem}: ${total_sales:,.2f} total sales\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 5: Real-time Analysis Dashboard\n",
        "\n",
        "Create a simple dashboard that updates with new analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "def create_analysis_dashboard(data_files, refresh_interval=30):\n",
        "    \"\"\"\n",
        "    Create a simple analysis dashboard that refreshes periodically.\n",
        "    \"\"\"\n",
        "    def update_dashboard():\n",
        "        clear_output(wait=True)\n",
        "        \n",
        "        print(\"\ud83d\udcca OSTRUCT ANALYSIS DASHBOARD\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Last Updated: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print()\n",
        "        \n",
        "        total_revenue = 0\n",
        "        total_transactions = 0\n",
        "        \n",
        "        for file_path in data_files:\n",
        "            try:\n",
        "                # Quick analysis for dashboard\n",
        "                results = run_ostruct_analysis(\n",
        "                    template_file='../analysis/templates/main.j2',\n",
        "                    schema_file='../analysis/schemas/main.json',\n",
        "                    data_file=file_path,\n",
        "                    model='gpt-4o-mini',\n",
        "                    enable_tools=['code-interpreter']\n",
        "                )\n",
        "                \n",
        "                summary = results.get('summary', {})\n",
        "                revenue = summary.get('total_sales', 0)\n",
        "                transactions = summary.get('total_transactions', 0)\n",
        "                \n",
        "                total_revenue += revenue\n",
        "                total_transactions += transactions\n",
        "                \n",
        "                print(f\"\ud83d\udcc8 {Path(file_path).stem.upper()}:\")\n",
        "                print(f\"   Revenue: ${revenue:,.2f}\")\n",
        "                print(f\"   Transactions: {transactions:,}\")\n",
        "                print()\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"\u274c Error analyzing {file_path}: {e}\")\n",
        "        \n",
        "        print(\"\ud83c\udfaf TOTALS:\")\n",
        "        print(f\"   Total Revenue: ${total_revenue:,.2f}\")\n",
        "        print(f\"   Total Transactions: {total_transactions:,}\")\n",
        "        print(f\"   Average per Transaction: ${total_revenue/total_transactions if total_transactions > 0 else 0:.2f}\")\n",
        "        \n",
        "        print(f\"\\n\u23f0 Next refresh in {refresh_interval} seconds...\")\n",
        "    \n",
        "    # Run initial update\n",
        "    update_dashboard()\n",
        "    \n",
        "    return update_dashboard\n",
        "\n",
        "# Create dashboard (run once for demo)\n",
        "dashboard = create_analysis_dashboard(datasets[:2])  # Use first 2 datasets for demo\n",
        "print(\"\u2705 Dashboard created (static version for demo)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices and Tips\n",
        "\n",
        "Here are some best practices for using ostruct in Jupyter notebooks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Best Practices Demo\n",
        "\n",
        "def data_science_best_practices():\n",
        "    \"\"\"\n",
        "    Demonstrate best practices for ostruct in data science workflows.\n",
        "    Following Google Cloud Jupyter Notebook Manifesto and industry standards.\n",
        "    \"\"\"\n",
        "    print(\"\ud83c\udfaf OSTRUCT DATA SCIENCE BEST PRACTICES\")\n",
        "    print(\"Following Google Cloud Jupyter Notebook Manifesto\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    practices = [\n",
        "        {\n",
        "            \"category\": \"\ud83d\udccb Reproducible Notebooks (Google Cloud Manifesto #3)\",\n",
        "            \"tips\": [\n",
        "                \"\u2705 Environment info tracked automatically in setup cell\",\n",
        "                \"\u2705 Random seeds set for reproducible results (np.random.seed(42))\",\n",
        "                \"\u2705 Requirements.txt provided for consistent dependencies\",\n",
        "                \"\u2705 Parameters cell enables Papermill execution\",\n",
        "                \"\u2705 All experiments logged with metadata and timing\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"category\": \"\ud83d\udcca Experiment Logging (Google Cloud Manifesto #7)\",\n",
        "            \"tips\": [\n",
        "                \"\u2705 ExperimentTracker logs all analysis metadata automatically\",\n",
        "                \"\u2705 Parameters, results, and execution info captured\",\n",
        "                \"\u2705 Unique experiment IDs for tracking and comparison\",\n",
        "                \"\u2705 JSON logs saved to experiment_logs/ directory\",\n",
        "                \"\u2705 Execution time and success/failure tracked\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"category\": \"\u2699\ufe0f Parameterized Execution (Google Cloud Manifesto #5)\",\n",
        "            \"tips\": [\n",
        "                \"\u2705 Parameters cell tagged for Papermill compatibility\",\n",
        "                \"\u2705 DEFAULT_MODEL, ANALYSIS_TIMEOUT configurable\",\n",
        "                \"\u2705 Tool selection and output paths parameterized\",\n",
        "                \"\u2705 Functions use parameter defaults automatically\",\n",
        "                \"\u2705 No hardcoded timeouts - all use ANALYSIS_TIMEOUT parameter\",\n",
        "                \"\u2705 Easy to run with different configurations\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"category\": \"\ud83d\udd27 Performance Optimization\",\n",
        "            \"tips\": [\n",
        "                \"Use gpt-4o-mini for exploratory analysis, gpt-4o for complex insights\",\n",
        "                \"Cache results using --output-file to avoid re-running expensive analyses\",\n",
        "                \"Sample large datasets for development, full data for production\",\n",
        "                \"Use --dry-run for template validation before API calls\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"category\": \"\ud83d\udcb0 Cost Management\",\n",
        "            \"tips\": [\n",
        "                \"Start with cheaper models and upgrade only when needed\",\n",
        "                \"Use batch processing to reduce per-request overhead\",\n",
        "                \"Monitor token usage with verbose output\",\n",
        "                \"Reuse schemas across similar analyses\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"category\": \"\ud83d\udee1\ufe0f Reliability & Security\",\n",
        "            \"tips\": [\n",
        "                \"Always validate schemas before production use\",\n",
        "                \"Handle API errors gracefully with try/catch blocks\",\n",
        "                \"Don't commit API keys to notebooks\",\n",
        "                \"Use environment variables for configuration\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"category\": \"\ud83d\udcca Analysis Quality\",\n",
        "            \"tips\": [\n",
        "                \"Design schemas that capture business value, not just technical metrics\",\n",
        "                \"Include confidence levels and caveats in your schemas\",\n",
        "                \"Combine AI insights with traditional statistical validation\",\n",
        "                \"Document assumptions and limitations in templates\"\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    for practice in practices:\n",
        "        print(f\"\\n{practice['category']}\")\n",
        "        for tip in practice['tips']:\n",
        "            print(f\"  \u2713 {tip}\")\n",
        "    \n",
        "    print(\"\\n\ud83d\ude80 Ready to build amazing data science workflows with ostruct!\")\n",
        "\n",
        "data_science_best_practices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 6: Advanced Workflows from Data Science Guide\n",
        "\n",
        "Let's implement the complete workflows from the Data Science Integration Guide, including Financial Analysis, Research Synthesis, Business Intelligence, and Market Research examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Financial Analysis Workflow Example\n",
        "\n",
        "def create_financial_analysis_example():\n",
        "    \"\"\"Create complete financial analysis workflow from integration guide.\"\"\"\n",
        "    \n",
        "    # Create sample financial data\n",
        "    financial_data = {\n",
        "        'date': pd.date_range('2024-01-01', periods=12, freq='M'),\n",
        "        'revenue': [1500000, 1620000, 1580000, 1750000, 1690000, 1820000,\n",
        "                   1950000, 1880000, 2100000, 2050000, 2200000, 2350000],\n",
        "        'expenses': [1200000, 1250000, 1180000, 1300000, 1220000, 1350000,\n",
        "                    1400000, 1380000, 1450000, 1420000, 1500000, 1550000],\n",
        "        'market_segment': ['Consumer'] * 6 + ['Enterprise'] * 6\n",
        "    }\n",
        "    \n",
        "    df_financial = pd.DataFrame(financial_data)\n",
        "    df_financial['net_income'] = df_financial['revenue'] - df_financial['expenses']\n",
        "    df_financial['profit_margin'] = (df_financial['net_income'] / df_financial['revenue']) * 100\n",
        "    \n",
        "    # Save financial data\n",
        "    df_financial.to_csv('quarterly_financial_data.csv', index=False)\n",
        "    \n",
        "    print(\"\ud83d\udcca Financial Data Created:\")\n",
        "    display(df_financial.head())\n",
        "    \n",
        "    # Create financial analysis template (from integration guide)\n",
        "    financial_template = \"\"\"\n",
        "You are a senior financial analyst. Perform comprehensive analysis of the provided financial data.\n",
        "\n",
        "## Financial Analysis for Company - 2024\n",
        "\n",
        "### Market Data Analysis\n",
        "Analyze the following financial data and provide comprehensive insights:\n",
        "\n",
        "**Raw Data:**\n",
        "{{ quarterly_data.content }}\n",
        "\n",
        "### Analysis Requirements:\n",
        "1. **Performance Metrics**: Calculate key ratios (ROE, EBITDA margin, profit margins)\n",
        "2. **Trend Analysis**: Compare performance across time periods\n",
        "3. **Market Position**: Analyze segment performance \n",
        "4. **Risk Assessment**: Identify potential financial risks\n",
        "5. **Growth Projection**: Forecast trends based on current data\n",
        "\n",
        "### Regulatory Compliance Check:\n",
        "Review all metrics and flag any concerning trends for stakeholder reporting.\n",
        "\n",
        "Create professional visualization showing key financial trends.\n",
        "\"\"\"\n",
        "    \n",
        "    with open('financial_analysis_template.j2', 'w') as f:\n",
        "        f.write(financial_template)\n",
        "    \n",
        "    # Financial analysis schema (from integration guide)\n",
        "    financial_schema = {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"executive_summary\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"2-3 sentence summary of financial health\"\n",
        "            },\n",
        "            \"key_metrics\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"total_revenue\": {\"type\": \"number\"},\n",
        "                    \"net_income\": {\"type\": \"number\"},\n",
        "                    \"average_profit_margin\": {\"type\": \"number\"},\n",
        "                    \"revenue_growth_rate\": {\"type\": \"number\"}\n",
        "                },\n",
        "                \"required\": [\"total_revenue\", \"net_income\", \"average_profit_margin\"]\n",
        "            },\n",
        "            \"trend_analysis\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"revenue_trend\": {\"type\": \"string\"},\n",
        "                    \"profit_margin_trend\": {\"type\": \"string\"},\n",
        "                    \"quarter_over_quarter_change\": {\"type\": \"number\"}\n",
        "                }\n",
        "            },\n",
        "            \"risk_factors\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"risk_type\": {\"type\": \"string\"},\n",
        "                        \"severity\": {\"type\": \"string\", \"enum\": [\"low\", \"medium\", \"high\", \"critical\"]},\n",
        "                        \"description\": {\"type\": \"string\"},\n",
        "                        \"mitigation_suggestions\": {\"type\": \"string\"}\n",
        "                    },\n",
        "                    \"required\": [\"risk_type\", \"severity\", \"description\"]\n",
        "                }\n",
        "            },\n",
        "            \"growth_forecast\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"next_quarter_revenue_estimate\": {\"type\": \"number\"},\n",
        "                    \"confidence_level\": {\"type\": \"string\", \"enum\": [\"low\", \"medium\", \"high\"]},\n",
        "                    \"key_assumptions\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"executive_summary\", \"key_metrics\", \"risk_factors\"]\n",
        "    }\n",
        "    \n",
        "    with open('financial_analysis_schema.json', 'w') as f:\n",
        "        json.dump(financial_schema, f, indent=2)\n",
        "    \n",
        "    print(\"\u2705 Financial analysis template and schema created\")\n",
        "    \n",
        "    # Run financial analysis\n",
        "    financial_results = run_ostruct_analysis(\n",
        "        template_file='financial_analysis_template.j2',\n",
        "        schema_file='financial_analysis_schema.json',\n",
        "        data_file='quarterly_financial_data.csv',\n",
        "        model='gpt-4o',\n",
        "        enable_tools=['code-interpreter', 'web-search'],\n",
        "        web_query='financial market trends Q4 2024 analysis',\n",
        "        output_file='financial_analysis_results.json'\n",
        "    )\n",
        "    \n",
        "    print(\"\u2705 Financial Analysis Complete!\")\n",
        "    \n",
        "    # Display key results\n",
        "    print(\"\\n\ud83d\udcbc FINANCIAL ANALYSIS SUMMARY:\")\n",
        "    print(f\"\ud83d\udcca Executive Summary: {financial_results['executive_summary']}\")\n",
        "    \n",
        "    if 'key_metrics' in financial_results:\n",
        "        metrics = financial_results['key_metrics']\n",
        "        print(f\"\ud83d\udcb0 Total Revenue: ${metrics.get('total_revenue', 0):,.2f}\")\n",
        "        print(f\"\ud83d\udcb8 Net Income: ${metrics.get('net_income', 0):,.2f}\")\n",
        "        print(f\"\ud83d\udcc8 Avg Profit Margin: {metrics.get('average_profit_margin', 0):.1f}%\")\n",
        "    \n",
        "    if 'risk_factors' in financial_results:\n",
        "        print(\"\\n\u26a0\ufe0f Risk Factors:\")\n",
        "        for risk in financial_results['risk_factors'][:3]:  # Show top 3\n",
        "            severity = risk.get('severity', 'unknown').upper()\n",
        "            print(f\"  \u2022 [{severity}] {risk.get('risk_type', 'Unknown')}: {risk.get('description', 'N/A')}\")\n",
        "    \n",
        "    return financial_results\n",
        "\n",
        "# Run financial analysis example\n",
        "financial_results = create_financial_analysis_example()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Business Intelligence Report Generation Example\n",
        "\n",
        "def create_business_intelligence_example():\n",
        "    \"\"\"Create Business Intelligence workflow from integration guide.\"\"\"\n",
        "    \n",
        "    # Create sample business data\n",
        "    business_data = {\n",
        "        'date': pd.date_range('2024-01-01', periods=100, freq='D'),\n",
        "        'customer_segment': np.random.choice(['Enterprise', 'SMB', 'Consumer'], 100),\n",
        "        'product_category': np.random.choice(['Software', 'Hardware', 'Services'], 100),\n",
        "        'revenue': np.random.normal(50000, 15000, 100),\n",
        "        'customer_satisfaction': np.random.normal(4.2, 0.8, 100),\n",
        "        'market_share': np.random.normal(0.15, 0.05, 100)\n",
        "    }\n",
        "    \n",
        "    df_business = pd.DataFrame(business_data)\n",
        "    df_business['revenue'] = np.maximum(df_business['revenue'], 1000)  # Ensure positive revenue\n",
        "    df_business['customer_satisfaction'] = np.clip(df_business['customer_satisfaction'], 1, 5)\n",
        "    df_business['market_share'] = np.clip(df_business['market_share'], 0.01, 0.5)\n",
        "    \n",
        "    # Save business data\n",
        "    df_business.to_csv('business_intelligence_data.csv', index=False)\n",
        "    \n",
        "    print(\"\ud83d\udcca Business Intelligence Data Created:\")\n",
        "    display(df_business.head())\n",
        "    \n",
        "    # Create BI analysis template (from integration guide)\n",
        "    bi_template = \"\"\"\n",
        "You are a senior business analyst. Perform comprehensive competitive analysis and business intelligence.\n",
        "\n",
        "## Business Intelligence Report - Q4 2024\n",
        "\n",
        "### Internal Performance Analysis\n",
        "**Sales Data:**\n",
        "{{ sales_data.content }}\n",
        "\n",
        "### Analysis Requirements:\n",
        "1. **Market Position**: Analyze our position vs competitors across key metrics\n",
        "2. **Growth Opportunities**: Identify untapped segments and expansion possibilities  \n",
        "3. **Competitive Threats**: Assess emerging competitors and market disruptions\n",
        "4. **Pricing Analysis**: Evaluate price positioning and optimization opportunities\n",
        "5. **Strategic Recommendations**: Provide actionable next steps with ROI projections\n",
        "\n",
        "### Executive Briefing Elements:\n",
        "- Top 3 strategic priorities\n",
        "- Revenue impact projections\n",
        "- Resource requirements\n",
        "- Timeline for implementation\n",
        "\n",
        "Create professional visualizations showing competitive positioning and market trends.\n",
        "\"\"\"\n",
        "    \n",
        "    with open('bi_analysis_template.j2', 'w') as f:\n",
        "        f.write(bi_template)\n",
        "    \n",
        "    # BI analysis schema (from integration guide)\n",
        "    bi_schema = {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"executive_summary\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"CEO-ready 2-3 sentence summary of strategic position\"\n",
        "            },\n",
        "            \"market_position\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"market_share\": {\"type\": \"number\"},\n",
        "                    \"competitive_ranking\": {\"type\": \"integer\"},\n",
        "                    \"differentiation_strengths\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "                    \"competitive_gaps\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
        "                }\n",
        "            },\n",
        "            \"growth_opportunities\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"opportunity\": {\"type\": \"string\"},\n",
        "                        \"market_size\": {\"type\": \"number\"},\n",
        "                        \"revenue_potential\": {\"type\": \"number\"},\n",
        "                        \"time_to_market\": {\"type\": \"string\"},\n",
        "                        \"investment_required\": {\"type\": \"number\"},\n",
        "                        \"risk_level\": {\"type\": \"string\", \"enum\": [\"low\", \"medium\", \"high\"]}\n",
        "                    },\n",
        "                    \"required\": [\"opportunity\", \"revenue_potential\", \"risk_level\"]\n",
        "                }\n",
        "            },\n",
        "            \"strategic_recommendations\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"recommendation\": {\"type\": \"string\"},\n",
        "                        \"priority\": {\"type\": \"string\", \"enum\": [\"critical\", \"high\", \"medium\", \"low\"]},\n",
        "                        \"expected_roi\": {\"type\": \"number\"},\n",
        "                        \"implementation_timeline\": {\"type\": \"string\"},\n",
        "                        \"resource_requirements\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "                        \"success_metrics\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
        "                    },\n",
        "                    \"required\": [\"recommendation\", \"priority\", \"expected_roi\"]\n",
        "                }\n",
        "            },\n",
        "            \"competitive_analysis\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"top_competitors\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "                    \"competitive_advantages\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "                    \"market_threats\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"executive_summary\", \"market_position\", \"growth_opportunities\", \"strategic_recommendations\"]\n",
        "    }\n",
        "    \n",
        "    with open('bi_analysis_schema.json', 'w') as f:\n",
        "        json.dump(bi_schema, f, indent=2)\n",
        "    \n",
        "    print(\"\u2705 Business Intelligence template and schema created\")\n",
        "    \n",
        "    # Run BI analysis\n",
        "    bi_results = run_ostruct_analysis(\n",
        "        template_file='bi_analysis_template.j2',\n",
        "        schema_file='bi_analysis_schema.json',\n",
        "        data_file='business_intelligence_data.csv',\n",
        "        model='gpt-4o',\n",
        "        enable_tools=['code-interpreter', 'web-search'],\n",
        "        web_query='business intelligence market trends 2024 competitive analysis',\n",
        "        output_file='bi_analysis_results.json'\n",
        "    )\n",
        "    \n",
        "    print(\"\u2705 Business Intelligence Analysis Complete!\")\n",
        "    \n",
        "    # Display key results\n",
        "    print(\"\\n\ud83c\udfe2 BUSINESS INTELLIGENCE SUMMARY:\")\n",
        "    print(f\"\ud83d\udcca Executive Summary: {bi_results['executive_summary']}\")\n",
        "    \n",
        "    if 'market_position' in bi_results:\n",
        "        position = bi_results['market_position']\n",
        "        print(f\"\ud83d\udcc8 Market Share: {position.get('market_share', 0):.1%}\")\n",
        "        print(f\"\ud83c\udfc6 Competitive Ranking: #{position.get('competitive_ranking', 'N/A')}\")\n",
        "    \n",
        "    if 'strategic_recommendations' in bi_results:\n",
        "        print(\"\\n\ud83d\udca1 TOP STRATEGIC RECOMMENDATIONS:\")\n",
        "        for i, rec in enumerate(bi_results['strategic_recommendations'][:3], 1):\n",
        "            priority = rec.get('priority', 'medium').upper()\n",
        "            recommendation = rec.get('recommendation', 'N/A')\n",
        "            roi = rec.get('expected_roi', 0)\n",
        "            print(f\"  {i}. [{priority}] {recommendation}\")\n",
        "            print(f\"     Expected ROI: {roi:.1%}\")\n",
        "    \n",
        "    return bi_results\n",
        "\n",
        "# Run business intelligence example\n",
        "bi_results = create_business_intelligence_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup\n",
        "\n",
        "Clean up temporary files created during this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup temporary files\n",
        "import glob\n",
        "\n",
        "temp_files = [\n",
        "    '*.csv', '*.json', '*.j2', 'downloads/*', 'batch_results/*'\n",
        "]\n",
        "\n",
        "for pattern in temp_files:\n",
        "    for file in glob.glob(pattern):\n",
        "        try:\n",
        "            Path(file).unlink()\n",
        "            print(f\"\ud83d\uddd1\ufe0f Removed: {file}\")\n",
        "        except:\n",
        "            pass  # Ignore errors for directories or non-existent files\n",
        "\n",
        "# Remove directories\n",
        "for dir_name in ['downloads', 'batch_results']:\n",
        "    try:\n",
        "        import shutil\n",
        "        shutil.rmtree(dir_name)\n",
        "        print(f\"\ud83d\uddd1\ufe0f Removed directory: {dir_name}\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"\u2705 Cleanup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "\ud83c\udf89 **Congratulations!** You've learned how to integrate ostruct with Jupyter notebooks for powerful data science workflows.\n",
        "\n",
        "### What to try next:\n",
        "\n",
        "1. **\ud83d\udd04 Adapt for your data**: Replace the sample data with your own datasets\n",
        "2. **\ud83c\udfa8 Custom templates**: Create domain-specific templates for your analysis needs\n",
        "3. **\ud83d\udcca Advanced schemas**: Design schemas that capture your business metrics\n",
        "4. **\ud83d\ude80 Production deployment**: Build automated pipelines using these patterns\n",
        "5. **\ud83d\udd17 Tool integration**: Combine with other data science tools in your stack\n",
        "\n",
        "### Resources:\n",
        "\n",
        "- [ostruct Documentation](https://ostruct.readthedocs.io/)\n",
        "- [Data Science Integration Guide](https://ostruct.readthedocs.io/en/latest/user-guide/data_science_integration.html)\n",
        "- [More Examples](../)\n",
        "- [GitHub Repository](https://github.com/yaniv-golan/ostruct)\n",
        "\n",
        "Happy analyzing! \ud83d\ude80\ud83d\udcca"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}