# ostruct

> **Schema‑first CLI.** Locally renders sandboxed Jinja2 templates → produces the final prompt *and* JSON Schema only; no OS‑level side‑effects are possible because the template runs inside a restricted Jinja `SandboxedEnvironment`([jinja.palletsprojects.com][1]). The prompt and schema are then sent to OpenAI's **Structured Outputs** endpoint, where the model both *generates* and *validates* the JSON so it exactly matches the supplied schema — eliminating client‑side re‑parsing or retries([OpenAI][2], [OpenAI Community][3]). This design turns ostruct into a pure orchestration layer that also adds multi‑tool context (Web Search, File Search, Code‑Interpreter, MCP) and strict file‑routing security([Ostruct][4], [Ostruct][5]).

A single binary `ostruct` covers every workflow (classic templates, self‑executing **OST** files, scaffolding, environment setup, model registry) while guarding against accidental data leaks through explicit `--allow/--strict-urls` gates([Ostruct][6], [GitHub][7]). All flags are grouped below so an LLM can parse or emit any valid invocation without further docs.

---

* **Mental model bullets**

  * *Templates execute locally* within a Jinja2 sandbox; they may read passed variables / file aliases, perform filters and control‑flow, and emit text; controlled file access occurs through the LazyFileContent system.
  * *OpenAI enforces validity*: the `response_format={"type":"json_object","schema":…}` parameter activates constrained decoding; any schema violation triggers an API error rather than malformed output reaching the CLI([Microsoft Learn][8], [developer.mamezou-tech.com][9]). `ostruct` then re-validates with Pydantic for additional type safety.
  * *LLM cost hygiene*: include only the schema‑relevant parts of large files; use `--dry-run` for offline token estimates and to preview the fully‑expanded prompt without burning quota.
  * *Security*: default deny on unknown paths; override with `--allow` flags or by listing approved roots in a policy file; `.gitignore` respected unless `--ignore-gitignore` is set.
  * *Tool routing*: attach data to specific tools via `[target:]alias path` (`ci:` for Code Interpreter, `fs:` for File Search, `web:` for Web Search, `ud:` for direct PDF/vision input, `prompt:` for inline‑only, `auto:` for type-based routing). **Known issue**: OpenAI Responses API `file_search` returns empty results despite successful vector store creation (upstream bug, affects all models).
  * *JSON Schema rules* mirror OpenAI: top‑level must be `"type":"object"`, every property declares `"type"` and `"description"`, arrays require `"items"`, `$defs` allowed, avoid `oneOf/anyOf` for best success rate([developer.mamezou-tech.com][9]).

* **Command surface (every sub‑command)**

  ```
  ostruct run TEMPLATE SCHEMA [OPTIONS]        # main execution path
  ostruct runx OST_FILE [ARGS]                # self‑executing template with front‑matter
  ostruct files {upload|list|gc|bind|rm|diagnose|vector-stores}  # file mgmt/cache
  ostruct scaffold template                  # quick-start template files
  ostruct setup {windows-register|windows-unregister}
  ostruct models {list|update}
  ostruct --quick-ref                         # concise usage examples
  ```

* **Global attachment & security flags**

  ```
  -F/--file      [target:]ALIAS PATH    # targets: prompt|ci|fs|ud|auto
  -D/--dir       [target:]ALIAS PATH
  -C/--collect   [target:]ALIAS @filelist
     --recursive --pattern GLOB
  --allow DIR               --allow-file FILE
  --allow-list FILE         --allow-insecure-url URL
  --path-security {permissive|warn|strict}
  --strict-urls | --no-strict-urls
  --max-file-size SIZE      --ignore-gitignore | --gitignore-file PATH
  ```

* **Files command flags**

  ```
  # upload: --file|--dir|--collect PATH, --tools {user-data|file-search|code-interpreter}, --tag KEY=VALUE, --vector-store NAME, --pattern GLOB, --dry-run, --progress {none|basic|detailed}, --json
  # list: --vector-store NAME, --tool {user-data|ud|code-interpreter|ci|file-search|fs}, --tag KEY=VALUE, --columns LIST, --no-truncate, --max-col-width N, --json
  # gc: --older-than DURATION, --json
  # bind FILE_ID: --tools {user-data|file-search|code-interpreter}, --json
  # rm|diagnose FILE_ID: --json
  # vector-stores: --json
  ```

* **Variables**

  ```
  -V/--var name=value           # string
  -J/--json-var name='JSON'     # raw JSON (must be valid)
  ```

* **Model & sampling**

  ```
  -m/--model NAME                 --temperature FLOAT
  --top-p FLOAT                   --max-output-tokens INT
  --frequency-penalty FLOAT       --presence-penalty FLOAT
  --reasoning-effort {low|medium|high}
  ```

* **System‑prompt controls** `--sys-prompt TEXT | --sys-file FILE | --ignore-task-sysprompt`

* **API / config** `--api-key`, `--timeout`, `--config PATH`
  * **API key sources** (in order of precedence): `--api-key` flag → `OPENAI_API_KEY` env var → `.env` file

* **Output, validation & debug**

  ```
  --output-file FILE          --dry-run | --dry-run-json  # --dry-run-json auto-enables dry-run
  --run-summary-json          --progress {none|basic|detailed}
  --verbose | --debug | --debug-validation
  --template-debug pre-expand,vars,preview,steps,post-expand|all
  --help-debug                # troubleshooting guide
  ```

* **Tool‑integration toggles & per‑tool flags**

  ```
  --tool-choice [auto|none|required|code-interpreter|file-search|web-search]  # auto=model picks, none=template-only, required=force tool use, single tool=restrict to that tool
  --enable-tool/--disable-tool web-search   --ws-context-size LEVEL
  --ws-country CC  --ws-region REGION  --ws-city CITY
  --ci-cleanup  --ci-download  --ci-download-dir DIR  --ci-duplicate-outputs MODE  --ci-download-timeout SEC
  --fs-cleanup  --fs-store-name NAME        --fs-timeout SEC  --fs-retries N
  --mcp-server LABEL@URL    --mcp-headers JSON
  --mcp-require-approval {always|never}     --mcp-allowed-tools JSON
  ```

* **MCP** `--mcp-server label@url --mcp-headers '{"Authorization":"Bearer key"}' --mcp-allowed-tools '["tool"]' --mcp-require-approval never`. Auth format critical: Bearer token. Filter tools to reduce context. Template check: `{% if mcp_enabled %}`. Ex: `--mcp-server api@https://x.com/mcp/sse --mcp-headers '{"Authorization":"Bearer abc"}'`

* **Web Search tool patterns**

  * **When to use**: Real-time information (news, prices, events), fact verification, current data beyond training cutoff, research requiring citations.
  * **Context size**: `low` (3-5 results, faster), `medium` (5-8 results, balanced), `high` (8-12 results, comprehensive). Affects token usage and response time.
  * **Geographic targeting**: `--ws-country` for country-specific results, `--ws-region` for broader regional focus, `--ws-city` for local information.
  * **Integration**: Use `--tool-choice web-search` to force web search use, or `auto` to let model decide based on query needs.
  * **Output format**: Web search results appear in LLM response with automatic citations as numbered references [1], [2] with URLs.
  * **Template detection**: Use `{% if web_search_enabled %}` to conditionally prompt for web search when tool is available.
  * **Result access**: Search results integrated into model's response context; no separate template variable - model includes findings and citations directly in output.

* **Jupyter/Notebook integration**

  * **CRITICAL subprocess pattern**: `subprocess.run(cmd, env=os.environ, stdin=subprocess.DEVNULL, capture_output=True, text=True, timeout=180, check=True)` - prevents asyncio deadlock
  * **API key setup**: Cross-platform support - Colab Secrets (`userdata.get('OPENAI_API_KEY')`), environment variables, .env files with secure getpass fallback
  * **Template detection**: Use `{% if code_interpreter_enabled %}` for computational sections, `{% if web_search_enabled %}` for current data needs
  * **File routing**: `ci:` for datasets/notebooks, `fs:` for research papers/docs, `ud:` for images/PDFs, `auto:` for mixed content
  * **Production patterns**: Batch processing with error handling, timeout management, intermediate result caching, automated cleanup of temp files

* **Use case mental models**

  * **Business intelligence**: `--file ci:sales quarterly_data.csv --file fs:reports market_research/ --enable-tool web-search` → competitive analysis with internal metrics
  * **Security analysis**: `--file fs:logs system_logs/ --file ci:vulns scan_results.csv --enable-tool code-interpreter` → threat assessment with data correlation
  * **Document processing**: `--file fs:docs contracts/ --file ud:images diagrams/ --enable-tool file-search` → multi-format document synthesis and analysis
  * **Financial reporting**: `--file ci:data financials.csv --enable-tool web-search --ws-context-size high` → regulatory compliance + market context analysis
  * **Research synthesis**: `--file fs:papers literature/ --enable-tool web-search --enable-tool code-interpreter` → academic research + current data validation
  * **Risk assessment**: Multi-tool orchestration combining historical data, document analysis, and current market intelligence for comprehensive risk modeling

* **Schema design patterns** (**CRITICAL**: Include `"message": {"type": "string"}` when using `--ci-download` for file generation)

  * **Data science**: `{"summary": {"type": "object"}, "insights": {"type": "array", "items": {"type": "string"}}, "visualizations": {"type": "array"}, "message": {"type": "string"}, "confidence": {"type": "string", "enum": ["high", "medium", "low"]}}`
  * **Business intelligence**: `{"executive_summary": {"type": "string"}, "key_metrics": {"type": "object"}, "recommendations": {"type": "array"}, "risk_factors": {"type": "array"}, "message": {"type": "string"}}`
  * **Security analysis**: `{"threat_level": {"type": "string", "enum": ["critical", "high", "medium", "low"]}, "vulnerabilities": {"type": "array"}, "mitigation_steps": {"type": "array"}, "message": {"type": "string"}}`
  * **Document processing**: `{"document_type": {"type": "string"}, "key_findings": {"type": "array"}, "extracted_data": {"type": "object"}, "summary": {"type": "string"}, "message": {"type": "string"}}`
  * **Financial reporting**: `{"financial_health": {"type": "string"}, "key_ratios": {"type": "object"}, "trend_analysis": {"type": "object"}, "risk_assessment": {"type": "array"}, "message": {"type": "string"}}`
  * **Research synthesis**: `{"research_question": {"type": "string"}, "findings": {"type": "array"}, "evidence_quality": {"type": "string"}, "gaps": {"type": "array"}, "message": {"type": "string"}}`

* **Code‑Interpreter download behavior (v2.0+)**

  * **Default**: File downloads are **disabled by default** for performance - use `--ci-download` flag to enable
  * **Rationale**: Most CI usage is computational (analysis, stats) where JSON response contains the value, not files
  * **Performance**: Single‑pass execution by default (~2‑3s faster), two‑pass only when downloads enabled
  * **Auto‑workaround**: When `--ci-download` used, automatically enables two‑pass sentinel strategy for structured output
  * **Migration**: `auto_download: true` config deprecated - use `--ci-download` flag instead
  * **Usage**: Add `--ci-download` for charts, reports, visualizations; skip for calculations, analysis results
  * **Schema requirement**: Include `"message": {"type": "string"}` in schema for reliable downloads when using `--ci-download`
  * **Download mechanics**: Raw HTTP via containers API with exponential backoff, 100MB size limit, progress reporting
  * **Error handling**: Container expiry detection, classified errors (DownloadError/ContainerExpiredError/EnhancedDownloadError)
  * **Model instructions**: Automatic model‑specific prompts injected to improve file annotation reliability when downloads enabled
  * **Model reliability**: gpt-4.1 most reliable for CI downloads; gpt-4o/gpt-4o-mini may require retries; o1-preview/o1-mini less reliable due to reasoning overhead affecting file annotations

* **Citation placement**: put numeric refs [n] inline *and* list full entries in the `sources` array when the schema defines it; otherwise include refs only inline.
* **Quick example**: `ostruct run tpl.j2 sch.json -V question='...' --enable-tool web-search --ws-context-size low --dry-run`

* **Upload‑cache** `--cache-uploads | --no-cache-uploads`, `--cache-path PATH`

* **Environment variables** `OSTRUCT_TEMPLATE_FILE_LIMIT`, `OSTRUCT_TEMPLATE_TOTAL_LIMIT`, `OSTRUCT_TEMPLATE_PREVIEW_LIMIT`, `OSTRUCT_CACHE_UPLOADS`, `OSTRUCT_DISABLE_REGISTRY_UPDATE_CHECKS`, `OSTRUCT_IGNORE_GITIGNORE`, `OSTRUCT_GITIGNORE_FILE`, `OSTRUCT_MAX_DOWNLOAD_SIZE` for size limits, caching, gitignore behaviour, and CI download limits.

* **Template functions**

  * Template function `get_embed_ref(alias)` creates structured file references: renders as `<alias>` in-prompt and automatically generates XML appendix at prompt end containing actual file contents. Use with `embed_text(alias)` to mark for inclusion. Only referenced aliases appear in appendix. Replaces deprecated `file_ref()` function.
  * Template function `safe_get(path, default)` prevents errors when accessing nested object properties: takes dot-separated string path (e.g., `"config.database.host"`) and returns value if exists/non-empty, otherwise returns default. Avoids UndefinedError/KeyError exceptions from risky direct access like `{{ config.database.host }}`.
  * Template filter `single` extracts single item from file collections: `{{ files | single }}` returns the file when exactly one file exists, raises error otherwise. Use for templates expecting one file but receiving collections.
  * Template filters `to_json` and `from_json` handle JSON conversion: `{{ data | to_json }}` serializes objects to JSON strings, `{{ json_string | from_json }}` parses JSON strings to objects. Provides safe fallbacks for non‑serializable objects.
  * File attachment helpers: `attach_file(path)`, `get_file_ref(path)`, `embed_text(alias)`, `get_embed_ref(alias)` for binary/text workflows.
  * Text processing filters: `word_count`, `char_count`, `extract_keywords`, `normalize`, `strip_markdown`.
  * Data processing filters: `sort_by(field)`, `group_by(field)`, `filter_by(field,value)`, `unique`, `frequency`, `aggregate`.
  * Table formatting: `table`, `auto_table`, `dict_to_table`, `list_to_table`, `align_table`.
  * Code processing: `format_code(lang)`, `strip_comments(lang)`, `escape_special`.
  * Utility functions: `estimate_tokens(text)`, `type_of(obj)`, `debug(var)`, `format_json(data)`, `summarize(data)`, `pivot_table(data,rows,cols)`.

* **Operational limits**

  * Template + attachments trimmed to fit model context (see `--max-output-tokens` and tool token overhead budgets).
  * Binary files have limited template access; content access raises errors but metadata (`.name`, `.path`, `.size`) works normally; pass binary files to Code‑Interpreter for analysis.

  * Auto‑routing file‑type detection upgraded with Magika if `pip install ostruct-cli[enhanced-detection]` is used.

* **Always‑remember heuristics**

  1. First run with `--dry-run` to see the fully‑rendered prompt and detect schema errors locally (syntax only).
  2. Use explicit `--tool-choice none` when structured data is the *only* goal.
  3. Keep schemas minimal but descriptive; model matches keys by name, descriptions improve coercion quality.
  4. Refresh models via `models update` to lock in newest OpenAI releases.
  5. CI file downloads require `message` field in schema; use `--help-debug` for troubleshooting guides.
  6. For CI file generation/download: prefer gpt-4.1 > gpt-4o > gpt-4o-mini > o1-models for reliability.

## Resources

* [GitHub repo](https://github.com/yaniv-golan/ostruct/) - includes this llms.txt for LLM use
* [Full docs (Read‑the‑Docs)](https://ostruct.readthedocs.io/en/latest/)
* [llms.txt format specification](https://llmstxt.org/#format)

[1]: https://jinja.palletsprojects.com/en/stable/sandbox/
[2]: https://platform.openai.com/docs/guides/structured-outputs
[3]: https://community.openai.com/t/structured-outputs-now-available-in-the-api/896796
[4]: https://ostruct.readthedocs.io/en/latest/security/overview.html
[5]: https://ostruct.readthedocs.io/en/latest/user-guide/security_and_path_control.html
[6]: https://ostruct.readthedocs.io/en/latest/user-guide/security_and_path_control.html
[7]: https://github.com/yaniv-golan/ostruct/
[8]: https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/structured-outputs
[9]: https://developer.mamezou-tech.com/en/blogs/2024/08/07/openai-structured-outputs/

* **Minimal end-to-end example**:

  ```jinja2
  # tpl.j2 (snippet)
  {% if web_search_enabled %}
  **IMPORTANT**: Use web search to answer the question and cite every source in the `sources` array.
  {% endif %}
  Question: {{ question }}
  ```

  ```jsonc
  // sch.json (snippet)
  {
    "type": "object",
    "properties": {
      "answer": {"type": "string"},
      "sources": {
        "type": "array",
        "items": {
          "type": "object",
          "properties": {
            "title": {"type": "string"},
            "url":   {"type": "string"}
          },
          "required": ["title", "url"]
        }
      }
    },
    "required": ["answer", "sources"]
  }
  ```

  ```bash
  ostruct run tpl.j2 sch.json -V question='Latest AI safety news' \
    --enable-tool web-search --ws-context-size low
  ```
